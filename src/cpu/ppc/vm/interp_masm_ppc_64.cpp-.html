<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/cpu/ppc/vm/interp_masm_ppc_64.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 2003, 2016, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2012, 2016 SAP SE. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 
  27 #include "precompiled.hpp"
  28 #include "asm/macroAssembler.inline.hpp"
  29 #include "interp_masm_ppc.hpp"
  30 #include "interpreter/interpreterRuntime.hpp"
  31 #include "prims/jvmtiThreadState.hpp"
  32 #include "runtime/sharedRuntime.hpp"
  33 
  34 #ifdef PRODUCT
  35 #define BLOCK_COMMENT(str) // nothing
  36 #else
  37 #define BLOCK_COMMENT(str) block_comment(str)
  38 #endif
  39 
  40 void InterpreterMacroAssembler::null_check_throw(Register a, int offset, Register temp_reg) {
  41   address exception_entry = Interpreter::throw_NullPointerException_entry();
  42   MacroAssembler::null_check_throw(a, offset, temp_reg, exception_entry);
  43 }
  44 
  45 void InterpreterMacroAssembler::jump_to_entry(address entry, Register Rscratch) {
  46   assert(entry, "Entry must have been generated by now");
  47   if (is_within_range_of_b(entry, pc())) {
  48     b(entry);
  49   } else {
  50     load_const_optimized(Rscratch, entry, R0);
  51     mtctr(Rscratch);
  52     bctr();
  53   }
  54 }
  55 
  56 void InterpreterMacroAssembler::dispatch_next(TosState state, int bcp_incr) {
  57   Register bytecode = R12_scratch2;
  58   if (bcp_incr != 0) {
  59     lbzu(bytecode, bcp_incr, R14_bcp);
  60   } else {
  61     lbz(bytecode, 0, R14_bcp);
  62   }
  63 
  64   dispatch_Lbyte_code(state, bytecode, Interpreter::dispatch_table(state));
  65 }
  66 
  67 void InterpreterMacroAssembler::dispatch_via(TosState state, address* table) {
  68   // Load current bytecode.
  69   Register bytecode = R12_scratch2;
  70   lbz(bytecode, 0, R14_bcp);
  71   dispatch_Lbyte_code(state, bytecode, table);
  72 }
  73 
  74 // Dispatch code executed in the prolog of a bytecode which does not do it's
  75 // own dispatch. The dispatch address is computed and placed in R24_dispatch_addr.
  76 void InterpreterMacroAssembler::dispatch_prolog(TosState state, int bcp_incr) {
  77   Register bytecode = R12_scratch2;
  78   lbz(bytecode, bcp_incr, R14_bcp);
  79 
  80   load_dispatch_table(R24_dispatch_addr, Interpreter::dispatch_table(state));
  81 
  82   sldi(bytecode, bytecode, LogBytesPerWord);
  83   ldx(R24_dispatch_addr, R24_dispatch_addr, bytecode);
  84 }
  85 
  86 // Dispatch code executed in the epilog of a bytecode which does not do it's
  87 // own dispatch. The dispatch address in R24_dispatch_addr is used for the
  88 // dispatch.
  89 void InterpreterMacroAssembler::dispatch_epilog(TosState state, int bcp_incr) {
  90   if (bcp_incr) { addi(R14_bcp, R14_bcp, bcp_incr); }
  91   mtctr(R24_dispatch_addr);
  92   bcctr(bcondAlways, 0, bhintbhBCCTRisNotPredictable);
  93 }
  94 
  95 void InterpreterMacroAssembler::check_and_handle_popframe(Register scratch_reg) {
  96   assert(scratch_reg != R0, "can't use R0 as scratch_reg here");
  97   if (JvmtiExport::can_pop_frame()) {
  98     Label L;
  99 
 100     // Check the "pending popframe condition" flag in the current thread.
 101     lwz(scratch_reg, in_bytes(JavaThread::popframe_condition_offset()), R16_thread);
 102 
 103     // Initiate popframe handling only if it is not already being
 104     // processed. If the flag has the popframe_processing bit set, it
 105     // means that this code is called *during* popframe handling - we
 106     // don't want to reenter.
 107     andi_(R0, scratch_reg, JavaThread::popframe_pending_bit);
 108     beq(CCR0, L);
 109 
 110     andi_(R0, scratch_reg, JavaThread::popframe_processing_bit);
 111     bne(CCR0, L);
 112 
 113     // Call the Interpreter::remove_activation_preserving_args_entry()
 114     // func to get the address of the same-named entrypoint in the
 115     // generated interpreter code.
 116 #if defined(ABI_ELFv2)
 117     call_c(CAST_FROM_FN_PTR(address,
 118                             Interpreter::remove_activation_preserving_args_entry),
 119            relocInfo::none);
 120 #else
 121     call_c(CAST_FROM_FN_PTR(FunctionDescriptor*,
 122                             Interpreter::remove_activation_preserving_args_entry),
 123            relocInfo::none);
 124 #endif
 125 
 126     // Jump to Interpreter::_remove_activation_preserving_args_entry.
 127     mtctr(R3_RET);
 128     bctr();
 129 
 130     align(32, 12);
 131     bind(L);
 132   }
 133 }
 134 
 135 void InterpreterMacroAssembler::check_and_handle_earlyret(Register scratch_reg) {
 136   const Register Rthr_state_addr = scratch_reg;
 137   if (JvmtiExport::can_force_early_return()) {
 138     Label Lno_early_ret;
 139     ld(Rthr_state_addr, in_bytes(JavaThread::jvmti_thread_state_offset()), R16_thread);
 140     cmpdi(CCR0, Rthr_state_addr, 0);
 141     beq(CCR0, Lno_early_ret);
 142 
 143     lwz(R0, in_bytes(JvmtiThreadState::earlyret_state_offset()), Rthr_state_addr);
 144     cmpwi(CCR0, R0, JvmtiThreadState::earlyret_pending);
 145     bne(CCR0, Lno_early_ret);
 146 
 147     // Jump to Interpreter::_earlyret_entry.
 148     lwz(R3_ARG1, in_bytes(JvmtiThreadState::earlyret_tos_offset()), Rthr_state_addr);
 149     call_VM_leaf(CAST_FROM_FN_PTR(address, Interpreter::remove_activation_early_entry));
 150     mtlr(R3_RET);
 151     blr();
 152 
 153     align(32, 12);
 154     bind(Lno_early_ret);
 155   }
 156 }
 157 
 158 void InterpreterMacroAssembler::load_earlyret_value(TosState state, Register Rscratch1) {
 159   const Register RjvmtiState = Rscratch1;
 160   const Register Rscratch2   = R0;
 161 
 162   ld(RjvmtiState, in_bytes(JavaThread::jvmti_thread_state_offset()), R16_thread);
 163   li(Rscratch2, 0);
 164 
 165   switch (state) {
 166     case atos: ld(R17_tos, in_bytes(JvmtiThreadState::earlyret_oop_offset()), RjvmtiState);
 167                std(Rscratch2, in_bytes(JvmtiThreadState::earlyret_oop_offset()), RjvmtiState);
 168                break;
 169     case ltos: ld(R17_tos, in_bytes(JvmtiThreadState::earlyret_value_offset()), RjvmtiState);
 170                break;
 171     case btos: // fall through
 172     case ztos: // fall through
 173     case ctos: // fall through
 174     case stos: // fall through
 175     case itos: lwz(R17_tos, in_bytes(JvmtiThreadState::earlyret_value_offset()), RjvmtiState);
 176                break;
 177     case ftos: lfs(F15_ftos, in_bytes(JvmtiThreadState::earlyret_value_offset()), RjvmtiState);
 178                break;
 179     case dtos: lfd(F15_ftos, in_bytes(JvmtiThreadState::earlyret_value_offset()), RjvmtiState);
 180                break;
 181     case vtos: break;
 182     default  : ShouldNotReachHere();
 183   }
 184 
 185   // Clean up tos value in the jvmti thread state.
 186   std(Rscratch2, in_bytes(JvmtiThreadState::earlyret_value_offset()), RjvmtiState);
 187   // Set tos state field to illegal value.
 188   li(Rscratch2, ilgl);
 189   stw(Rscratch2, in_bytes(JvmtiThreadState::earlyret_tos_offset()), RjvmtiState);
 190 }
 191 
 192 // Common code to dispatch and dispatch_only.
 193 // Dispatch value in Lbyte_code and increment Lbcp.
 194 
 195 void InterpreterMacroAssembler::load_dispatch_table(Register dst, address* table) {
 196   address table_base = (address)Interpreter::dispatch_table((TosState)0);
 197   intptr_t table_offs = (intptr_t)table - (intptr_t)table_base;
 198   if (is_simm16(table_offs)) {
 199     addi(dst, R25_templateTableBase, (int)table_offs);
 200   } else {
 201     load_const_optimized(dst, table, R0);
 202   }
 203 }
 204 
 205 void InterpreterMacroAssembler::dispatch_Lbyte_code(TosState state, Register bytecode,
 206                                                     address* table, bool verify) {
 207   if (verify) {
 208     unimplemented("dispatch_Lbyte_code: verify"); // See Sparc Implementation to implement this
 209   }
 210 
 211   assert_different_registers(bytecode, R11_scratch1);
 212 
 213   // Calc dispatch table address.
 214   load_dispatch_table(R11_scratch1, table);
 215 
 216   sldi(R12_scratch2, bytecode, LogBytesPerWord);
 217   ldx(R11_scratch1, R11_scratch1, R12_scratch2);
 218 
 219   // Jump off!
 220   mtctr(R11_scratch1);
 221   bcctr(bcondAlways, 0, bhintbhBCCTRisNotPredictable);
 222 }
 223 
 224 void InterpreterMacroAssembler::load_receiver(Register Rparam_count, Register Rrecv_dst) {
 225   sldi(Rrecv_dst, Rparam_count, Interpreter::logStackElementSize);
 226   ldx(Rrecv_dst, Rrecv_dst, R15_esp);
 227 }
 228 
 229 // helpers for expression stack
 230 
 231 void InterpreterMacroAssembler::pop_i(Register r) {
 232   lwzu(r, Interpreter::stackElementSize, R15_esp);
 233 }
 234 
 235 void InterpreterMacroAssembler::pop_ptr(Register r) {
 236   ldu(r, Interpreter::stackElementSize, R15_esp);
 237 }
 238 
 239 void InterpreterMacroAssembler::pop_l(Register r) {
 240   ld(r, Interpreter::stackElementSize, R15_esp);
 241   addi(R15_esp, R15_esp, 2 * Interpreter::stackElementSize);
 242 }
 243 
 244 void InterpreterMacroAssembler::pop_f(FloatRegister f) {
 245   lfsu(f, Interpreter::stackElementSize, R15_esp);
 246 }
 247 
 248 void InterpreterMacroAssembler::pop_d(FloatRegister f) {
 249   lfd(f, Interpreter::stackElementSize, R15_esp);
 250   addi(R15_esp, R15_esp, 2 * Interpreter::stackElementSize);
 251 }
 252 
 253 void InterpreterMacroAssembler::push_i(Register r) {
 254   stw(r, 0, R15_esp);
 255   addi(R15_esp, R15_esp, - Interpreter::stackElementSize );
 256 }
 257 
 258 void InterpreterMacroAssembler::push_ptr(Register r) {
 259   std(r, 0, R15_esp);
 260   addi(R15_esp, R15_esp, - Interpreter::stackElementSize );
 261 }
 262 
 263 void InterpreterMacroAssembler::push_l(Register r) {
 264   std(r, - Interpreter::stackElementSize, R15_esp);
 265   addi(R15_esp, R15_esp, - 2 * Interpreter::stackElementSize );
 266 }
 267 
 268 void InterpreterMacroAssembler::push_f(FloatRegister f) {
 269   stfs(f, 0, R15_esp);
 270   addi(R15_esp, R15_esp, - Interpreter::stackElementSize );
 271 }
 272 
 273 void InterpreterMacroAssembler::push_d(FloatRegister f)   {
 274   stfd(f, - Interpreter::stackElementSize, R15_esp);
 275   addi(R15_esp, R15_esp, - 2 * Interpreter::stackElementSize );
 276 }
 277 
 278 void InterpreterMacroAssembler::push_2ptrs(Register first, Register second) {
 279   std(first, 0, R15_esp);
 280   std(second, -Interpreter::stackElementSize, R15_esp);
 281   addi(R15_esp, R15_esp, - 2 * Interpreter::stackElementSize );
 282 }
 283 
 284 void InterpreterMacroAssembler::push_l_pop_d(Register l, FloatRegister d) {
 285   std(l, 0, R15_esp);
 286   lfd(d, 0, R15_esp);
 287 }
 288 
 289 void InterpreterMacroAssembler::push_d_pop_l(FloatRegister d, Register l) {
 290   stfd(d, 0, R15_esp);
 291   ld(l, 0, R15_esp);
 292 }
 293 
 294 void InterpreterMacroAssembler::push(TosState state) {
 295   switch (state) {
 296     case atos: push_ptr();                break;
 297     case btos:
 298     case ztos:
 299     case ctos:
 300     case stos:
 301     case itos: push_i();                  break;
 302     case ltos: push_l();                  break;
 303     case ftos: push_f();                  break;
 304     case dtos: push_d();                  break;
 305     case vtos: /* nothing to do */        break;
 306     default  : ShouldNotReachHere();
 307   }
 308 }
 309 
 310 void InterpreterMacroAssembler::pop(TosState state) {
 311   switch (state) {
 312     case atos: pop_ptr();            break;
 313     case btos:
 314     case ztos:
 315     case ctos:
 316     case stos:
 317     case itos: pop_i();              break;
 318     case ltos: pop_l();              break;
 319     case ftos: pop_f();              break;
 320     case dtos: pop_d();              break;
 321     case vtos: /* nothing to do */   break;
 322     default  : ShouldNotReachHere();
 323   }
 324   verify_oop(R17_tos, state);
 325 }
 326 
 327 void InterpreterMacroAssembler::empty_expression_stack() {
 328   addi(R15_esp, R26_monitor, - Interpreter::stackElementSize);
 329 }
 330 
 331 void InterpreterMacroAssembler::get_2_byte_integer_at_bcp(int         bcp_offset,
 332                                                           Register    Rdst,
 333                                                           signedOrNot is_signed) {
 334 #if defined(VM_LITTLE_ENDIAN)
 335   if (bcp_offset) {
 336     load_const_optimized(Rdst, bcp_offset);
 337     lhbrx(Rdst, R14_bcp, Rdst);
 338   } else {
 339     lhbrx(Rdst, R14_bcp);
 340   }
 341   if (is_signed == Signed) {
 342     extsh(Rdst, Rdst);
 343   }
 344 #else
 345   // Read Java big endian format.
 346   if (is_signed == Signed) {
 347     lha(Rdst, bcp_offset, R14_bcp);
 348   } else {
 349     lhz(Rdst, bcp_offset, R14_bcp);
 350   }
 351 #endif
 352 }
 353 
 354 void InterpreterMacroAssembler::get_4_byte_integer_at_bcp(int         bcp_offset,
 355                                                           Register    Rdst,
 356                                                           signedOrNot is_signed) {
 357 #if defined(VM_LITTLE_ENDIAN)
 358   if (bcp_offset) {
 359     load_const_optimized(Rdst, bcp_offset);
 360     lwbrx(Rdst, R14_bcp, Rdst);
 361   } else {
 362     lwbrx(Rdst, R14_bcp);
 363   }
 364   if (is_signed == Signed) {
 365     extsw(Rdst, Rdst);
 366   }
 367 #else
 368   // Read Java big endian format.
 369   if (bcp_offset &amp; 3) { // Offset unaligned?
 370     load_const_optimized(Rdst, bcp_offset);
 371     if (is_signed == Signed) {
 372       lwax(Rdst, R14_bcp, Rdst);
 373     } else {
 374       lwzx(Rdst, R14_bcp, Rdst);
 375     }
 376   } else {
 377     if (is_signed == Signed) {
 378       lwa(Rdst, bcp_offset, R14_bcp);
 379     } else {
 380       lwz(Rdst, bcp_offset, R14_bcp);
 381     }
 382   }
 383 #endif
 384 }
 385 
 386 
 387 // Load the constant pool cache index from the bytecode stream.
 388 //
 389 // Kills / writes:
 390 //   - Rdst, Rscratch
 391 void InterpreterMacroAssembler::get_cache_index_at_bcp(Register Rdst, int bcp_offset,
 392                                                        size_t index_size) {
 393   assert(bcp_offset &gt; 0, "bcp is still pointing to start of bytecode");
 394   // Cache index is always in the native format, courtesy of Rewriter.
 395   if (index_size == sizeof(u2)) {
 396     lhz(Rdst, bcp_offset, R14_bcp);
 397   } else if (index_size == sizeof(u4)) {
 398     if (bcp_offset &amp; 3) {
 399       load_const_optimized(Rdst, bcp_offset);
 400       lwax(Rdst, R14_bcp, Rdst);
 401     } else {
 402       lwa(Rdst, bcp_offset, R14_bcp);
 403     }
 404     assert(ConstantPool::decode_invokedynamic_index(~123) == 123, "else change next line");
 405     nand(Rdst, Rdst, Rdst); // convert to plain index
 406   } else if (index_size == sizeof(u1)) {
 407     lbz(Rdst, bcp_offset, R14_bcp);
 408   } else {
 409     ShouldNotReachHere();
 410   }
 411   // Rdst now contains cp cache index.
 412 }
 413 
 414 void InterpreterMacroAssembler::get_cache_and_index_at_bcp(Register cache, int bcp_offset,
 415                                                            size_t index_size) {
 416   get_cache_index_at_bcp(cache, bcp_offset, index_size);
 417   sldi(cache, cache, exact_log2(in_words(ConstantPoolCacheEntry::size()) * BytesPerWord));
 418   add(cache, R27_constPoolCache, cache);
 419 }
 420 
 421 // Load 4-byte signed or unsigned integer in Java format (that is, big-endian format)
 422 // from (Rsrc)+offset.
 423 void InterpreterMacroAssembler::get_u4(Register Rdst, Register Rsrc, int offset,
 424                                        signedOrNot is_signed) {
 425 #if defined(VM_LITTLE_ENDIAN)
 426   if (offset) {
 427     load_const_optimized(Rdst, offset);
 428     lwbrx(Rdst, Rdst, Rsrc);
 429   } else {
 430     lwbrx(Rdst, Rsrc);
 431   }
 432   if (is_signed == Signed) {
 433     extsw(Rdst, Rdst);
 434   }
 435 #else
 436   if (is_signed == Signed) {
 437     lwa(Rdst, offset, Rsrc);
 438   } else {
 439     lwz(Rdst, offset, Rsrc);
 440   }
 441 #endif
 442 }
 443 
 444 // Load object from cpool-&gt;resolved_references(index).
 445 void InterpreterMacroAssembler::load_resolved_reference_at_index(Register result, Register index, Label *is_null) {
 446   assert_different_registers(result, index);
 447   get_constant_pool(result);
 448 
 449   // Convert from field index to resolved_references() index and from
 450   // word index to byte offset. Since this is a java object, it can be compressed.
 451   Register tmp = index;  // reuse
 452   sldi(tmp, index, LogBytesPerHeapOop);
 453   // Load pointer for resolved_references[] objArray.
 454   ld(result, ConstantPool::resolved_references_offset_in_bytes(), result);
 455   // JNIHandles::resolve(result)
 456   ld(result, 0, result);
 457 #ifdef ASSERT
 458   Label index_ok;
 459   lwa(R0, arrayOopDesc::length_offset_in_bytes(), result);
 460   sldi(R0, R0, LogBytesPerHeapOop);
 461   cmpd(CCR0, tmp, R0);
 462   blt(CCR0, index_ok);
 463   stop("resolved reference index out of bounds", 0x09256);
 464   bind(index_ok);
 465 #endif
 466   // Add in the index.
 467   add(result, tmp, result);
 468   load_heap_oop(result, arrayOopDesc::base_offset_in_bytes(T_OBJECT), result, is_null);
 469 }
 470 
 471 // Generate a subtype check: branch to ok_is_subtype if sub_klass is
 472 // a subtype of super_klass. Blows registers Rsub_klass, tmp1, tmp2.
 473 void InterpreterMacroAssembler::gen_subtype_check(Register Rsub_klass, Register Rsuper_klass, Register Rtmp1,
 474                                                   Register Rtmp2, Register Rtmp3, Label &amp;ok_is_subtype) {
 475   // Profile the not-null value's klass.
 476   profile_typecheck(Rsub_klass, Rtmp1, Rtmp2);
 477   check_klass_subtype(Rsub_klass, Rsuper_klass, Rtmp1, Rtmp2, ok_is_subtype);
 478   profile_typecheck_failed(Rtmp1, Rtmp2);
 479 }
 480 
 481 // Separate these two to allow for delay slot in middle.
 482 // These are used to do a test and full jump to exception-throwing code.
 483 
 484 // Check that index is in range for array, then shift index by index_shift,
 485 // and put arrayOop + shifted_index into res.
 486 // Note: res is still shy of address by array offset into object.
 487 
 488 void InterpreterMacroAssembler::index_check_without_pop(Register Rarray, Register Rindex,
 489                                                         int index_shift, Register Rtmp, Register Rres) {
 490   // Check that index is in range for array, then shift index by index_shift,
 491   // and put arrayOop + shifted_index into res.
 492   // Note: res is still shy of address by array offset into object.
 493   // Kills:
 494   //   - Rindex
 495   // Writes:
 496   //   - Rres: Address that corresponds to the array index if check was successful.
 497   verify_oop(Rarray);
 498   const Register Rlength   = R0;
 499   const Register RsxtIndex = Rtmp;
 500   Label LisNull, LnotOOR;
 501 
 502   // Array nullcheck
 503   if (!ImplicitNullChecks) {
 504     cmpdi(CCR0, Rarray, 0);
 505     beq(CCR0, LisNull);
 506   } else {
 507     null_check_throw(Rarray, arrayOopDesc::length_offset_in_bytes(), /*temp*/RsxtIndex);
 508   }
 509 
 510   // Rindex might contain garbage in upper bits (remember that we don't sign extend
 511   // during integer arithmetic operations). So kill them and put value into same register
 512   // where ArrayIndexOutOfBounds would expect the index in.
 513   rldicl(RsxtIndex, Rindex, 0, 32); // zero extend 32 bit -&gt; 64 bit
 514 
 515   // Index check
 516   lwz(Rlength, arrayOopDesc::length_offset_in_bytes(), Rarray);
 517   cmplw(CCR0, Rindex, Rlength);
 518   sldi(RsxtIndex, RsxtIndex, index_shift);
 519   blt(CCR0, LnotOOR);
 520   // Index should be in R17_tos, array should be in R4_ARG2.
 521   mr_if_needed(R17_tos, Rindex);
 522   mr_if_needed(R4_ARG2, Rarray);
 523   load_dispatch_table(Rtmp, (address*)Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);
 524   mtctr(Rtmp);
 525   bctr();
 526 
 527   if (!ImplicitNullChecks) {
 528     bind(LisNull);
 529     load_dispatch_table(Rtmp, (address*)Interpreter::_throw_NullPointerException_entry);
 530     mtctr(Rtmp);
 531     bctr();
 532   }
 533 
 534   align(32, 16);
 535   bind(LnotOOR);
 536 
 537   // Calc address
 538   add(Rres, RsxtIndex, Rarray);
 539 }
 540 
 541 void InterpreterMacroAssembler::index_check(Register array, Register index,
 542                                             int index_shift, Register tmp, Register res) {
 543   // pop array
 544   pop_ptr(array);
 545 
 546   // check array
 547   index_check_without_pop(array, index, index_shift, tmp, res);
 548 }
 549 
 550 void InterpreterMacroAssembler::get_const(Register Rdst) {
 551   ld(Rdst, in_bytes(Method::const_offset()), R19_method);
 552 }
 553 
 554 void InterpreterMacroAssembler::get_constant_pool(Register Rdst) {
 555   get_const(Rdst);
 556   ld(Rdst, in_bytes(ConstMethod::constants_offset()), Rdst);
 557 }
 558 
 559 void InterpreterMacroAssembler::get_constant_pool_cache(Register Rdst) {
 560   get_constant_pool(Rdst);
 561   ld(Rdst, ConstantPool::cache_offset_in_bytes(), Rdst);
 562 }
 563 
 564 void InterpreterMacroAssembler::get_cpool_and_tags(Register Rcpool, Register Rtags) {
 565   get_constant_pool(Rcpool);
 566   ld(Rtags, ConstantPool::tags_offset_in_bytes(), Rcpool);
 567 }
 568 
 569 // Unlock if synchronized method.
 570 //
 571 // Unlock the receiver if this is a synchronized method.
 572 // Unlock any Java monitors from synchronized blocks.
 573 //
 574 // If there are locked Java monitors
 575 //   If throw_monitor_exception
 576 //     throws IllegalMonitorStateException
 577 //   Else if install_monitor_exception
 578 //     installs IllegalMonitorStateException
 579 //   Else
 580 //     no error processing
 581 void InterpreterMacroAssembler::unlock_if_synchronized_method(TosState state,
 582                                                               bool throw_monitor_exception,
 583                                                               bool install_monitor_exception) {
 584   Label Lunlocked, Lno_unlock;
 585   {
 586     Register Rdo_not_unlock_flag = R11_scratch1;
 587     Register Raccess_flags       = R12_scratch2;
 588 
 589     // Check if synchronized method or unlocking prevented by
 590     // JavaThread::do_not_unlock_if_synchronized flag.
 591     lbz(Rdo_not_unlock_flag, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread);
 592     lwz(Raccess_flags, in_bytes(Method::access_flags_offset()), R19_method);
 593     li(R0, 0);
 594     stb(R0, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread); // reset flag
 595 
 596     push(state);
 597 
 598     // Skip if we don't have to unlock.
 599     rldicl_(R0, Raccess_flags, 64-JVM_ACC_SYNCHRONIZED_BIT, 63); // Extract bit and compare to 0.
 600     beq(CCR0, Lunlocked);
 601 
 602     cmpwi(CCR0, Rdo_not_unlock_flag, 0);
 603     bne(CCR0, Lno_unlock);
 604   }
 605 
 606   // Unlock
 607   {
 608     Register Rmonitor_base = R11_scratch1;
 609 
 610     Label Lunlock;
 611     // If it's still locked, everything is ok, unlock it.
 612     ld(Rmonitor_base, 0, R1_SP);
 613     addi(Rmonitor_base, Rmonitor_base,
 614          -(frame::ijava_state_size + frame::interpreter_frame_monitor_size_in_bytes())); // Monitor base
 615 
 616     ld(R0, BasicObjectLock::obj_offset_in_bytes(), Rmonitor_base);
 617     cmpdi(CCR0, R0, 0);
 618     bne(CCR0, Lunlock);
 619 
 620     // If it's already unlocked, throw exception.
 621     if (throw_monitor_exception) {
 622       call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_illegal_monitor_state_exception));
 623       should_not_reach_here();
 624     } else {
 625       if (install_monitor_exception) {
 626         call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::new_illegal_monitor_state_exception));
 627         b(Lunlocked);
 628       }
 629     }
 630 
 631     bind(Lunlock);
 632     unlock_object(Rmonitor_base);
 633   }
 634 
 635   // Check that all other monitors are unlocked. Throw IllegelMonitorState exception if not.
 636   bind(Lunlocked);
 637   {
 638     Label Lexception, Lrestart;
 639     Register Rcurrent_obj_addr = R11_scratch1;
 640     const int delta = frame::interpreter_frame_monitor_size_in_bytes();
 641     assert((delta &amp; LongAlignmentMask) == 0, "sizeof BasicObjectLock must be even number of doublewords");
 642 
 643     bind(Lrestart);
 644     // Set up search loop: Calc num of iterations.
 645     {
 646       Register Riterations = R12_scratch2;
 647       Register Rmonitor_base = Rcurrent_obj_addr;
 648       ld(Rmonitor_base, 0, R1_SP);
 649       addi(Rmonitor_base, Rmonitor_base, - frame::ijava_state_size);  // Monitor base
 650 
 651       subf_(Riterations, R26_monitor, Rmonitor_base);
 652       ble(CCR0, Lno_unlock);
 653 
 654       addi(Rcurrent_obj_addr, Rmonitor_base,
 655            BasicObjectLock::obj_offset_in_bytes() - frame::interpreter_frame_monitor_size_in_bytes());
 656       // Check if any monitor is on stack, bail out if not
 657       srdi(Riterations, Riterations, exact_log2(delta));
 658       mtctr(Riterations);
 659     }
 660 
 661     // The search loop: Look for locked monitors.
 662     {
 663       const Register Rcurrent_obj = R0;
 664       Label Lloop;
 665 
 666       ld(Rcurrent_obj, 0, Rcurrent_obj_addr);
 667       addi(Rcurrent_obj_addr, Rcurrent_obj_addr, -delta);
 668       bind(Lloop);
 669 
 670       // Check if current entry is used.
 671       cmpdi(CCR0, Rcurrent_obj, 0);
 672       bne(CCR0, Lexception);
 673       // Preload next iteration's compare value.
 674       ld(Rcurrent_obj, 0, Rcurrent_obj_addr);
 675       addi(Rcurrent_obj_addr, Rcurrent_obj_addr, -delta);
 676       bdnz(Lloop);
 677     }
 678     // Fell through: Everything's unlocked =&gt; finish.
 679     b(Lno_unlock);
 680 
 681     // An object is still locked =&gt; need to throw exception.
 682     bind(Lexception);
 683     if (throw_monitor_exception) {
 684       call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_illegal_monitor_state_exception));
 685       should_not_reach_here();
 686     } else {
 687       // Stack unrolling. Unlock object and if requested, install illegal_monitor_exception.
 688       // Unlock does not block, so don't have to worry about the frame.
 689       Register Rmonitor_addr = R11_scratch1;
 690       addi(Rmonitor_addr, Rcurrent_obj_addr, -BasicObjectLock::obj_offset_in_bytes() + delta);
 691       unlock_object(Rmonitor_addr);
 692       if (install_monitor_exception) {
 693         call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::new_illegal_monitor_state_exception));
 694       }
 695       b(Lrestart);
 696     }
 697   }
 698 
 699   align(32, 12);
 700   bind(Lno_unlock);
 701   pop(state);
 702 }
 703 
 704 // Support function for remove_activation &amp; Co.
 705 void InterpreterMacroAssembler::merge_frames(Register Rsender_sp, Register return_pc,
 706                                              Register Rscratch1, Register Rscratch2) {
 707   // Pop interpreter frame.
 708   ld(Rscratch1, 0, R1_SP); // *SP
 709   ld(Rsender_sp, _ijava_state_neg(sender_sp), Rscratch1); // top_frame_sp
 710   ld(Rscratch2, 0, Rscratch1); // **SP
 711 #ifdef ASSERT
 712   {
 713     Label Lok;
 714     ld(R0, _ijava_state_neg(ijava_reserved), Rscratch1);
 715     cmpdi(CCR0, R0, 0x5afe);
 716     beq(CCR0, Lok);
 717     stop("frame corrupted (remove activation)", 0x5afe);
 718     bind(Lok);
 719   }
 720 #endif
 721   if (return_pc!=noreg) {
 722     ld(return_pc, _abi(lr), Rscratch1); // LR
 723   }
 724 
 725   // Merge top frames.
 726   subf(Rscratch1, R1_SP, Rsender_sp); // top_frame_sp - SP
 727   stdux(Rscratch2, R1_SP, Rscratch1); // atomically set *(SP = top_frame_sp) = **SP
 728 }
 729 
 730 void InterpreterMacroAssembler::narrow(Register result) {
 731   Register ret_type = R11_scratch1;
 732   ld(R11_scratch1, in_bytes(Method::const_offset()), R19_method);
 733   lbz(ret_type, in_bytes(ConstMethod::result_type_offset()), R11_scratch1);
 734 
 735   Label notBool, notByte, notChar, done;
 736 
 737   // common case first
 738   cmpwi(CCR0, ret_type, T_INT);
 739   beq(CCR0, done);
 740 
 741   cmpwi(CCR0, ret_type, T_BOOLEAN);
 742   bne(CCR0, notBool);
 743   andi(result, result, 0x1);
 744   b(done);
 745 
 746   bind(notBool);
 747   cmpwi(CCR0, ret_type, T_BYTE);
 748   bne(CCR0, notByte);
 749   extsb(result, result);
 750   b(done);
 751 
 752   bind(notByte);
 753   cmpwi(CCR0, ret_type, T_CHAR);
 754   bne(CCR0, notChar);
 755   andi(result, result, 0xffff);
 756   b(done);
 757 
 758   bind(notChar);
 759   // cmpwi(CCR0, ret_type, T_SHORT);  // all that's left
 760   // bne(CCR0, done);
 761   extsh(result, result);
 762 
 763   // Nothing to do for T_INT
 764   bind(done);
 765 }
 766 
 767 // Remove activation.
 768 //
 769 // Unlock the receiver if this is a synchronized method.
 770 // Unlock any Java monitors from synchronized blocks.
 771 // Remove the activation from the stack.
 772 //
 773 // If there are locked Java monitors
 774 //    If throw_monitor_exception
 775 //       throws IllegalMonitorStateException
 776 //    Else if install_monitor_exception
 777 //       installs IllegalMonitorStateException
 778 //    Else
 779 //       no error processing
 780 void InterpreterMacroAssembler::remove_activation(TosState state,
 781                                                   bool throw_monitor_exception,
 782                                                   bool install_monitor_exception) {
 783   BLOCK_COMMENT("remove_activation {");
 784   unlock_if_synchronized_method(state, throw_monitor_exception, install_monitor_exception);
 785 
 786   // Save result (push state before jvmti call and pop it afterwards) and notify jvmti.
 787   notify_method_exit(false, state, NotifyJVMTI, true);
 788 
 789   BLOCK_COMMENT("reserved_stack_check:");
 790   if (StackReservedPages &gt; 0) {
 791     // Test if reserved zone needs to be enabled.
 792     Label no_reserved_zone_enabling;
 793 
 794     // Compare frame pointers. There is no good stack pointer, as with stack
 795     // frame compression we can get different SPs when we do calls. A subsequent
 796     // call could have a smaller SP, so that this compare succeeds for an
 797     // inner call of the method annotated with ReservedStack.
 798     ld_ptr(R0, JavaThread::reserved_stack_activation_offset(), R16_thread);
 799     ld_ptr(R11_scratch1, _abi(callers_sp), R1_SP); // Load frame pointer.
 800     cmpld(CCR0, R11_scratch1, R0);
 801     blt_predict_taken(CCR0, no_reserved_zone_enabling);
 802 
 803     // Enable reserved zone again, throw stack overflow exception.
 804     call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), R16_thread);
 805     call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_delayed_StackOverflowError));
 806 
 807     should_not_reach_here();
 808 
 809     bind(no_reserved_zone_enabling);
 810   }
 811 
 812   verify_oop(R17_tos, state);
 813   verify_thread();
 814 
 815   merge_frames(/*top_frame_sp*/ R21_sender_SP, /*return_pc*/ R0, R11_scratch1, R12_scratch2);
 816   mtlr(R0);
 817   BLOCK_COMMENT("} remove_activation");
 818 }
 819 
 820 // Lock object
 821 //
 822 // Registers alive
 823 //   monitor - Address of the BasicObjectLock to be used for locking,
 824 //             which must be initialized with the object to lock.
 825 //   object  - Address of the object to be locked.
 826 //
 827 void InterpreterMacroAssembler::lock_object(Register monitor, Register object) {
 828   if (UseHeavyMonitors) {
 829     call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),
 830             monitor, /*check_for_exceptions=*/true);
 831   } else {
 832     // template code:
 833     //
 834     // markOop displaced_header = obj-&gt;mark().set_unlocked();
 835     // monitor-&gt;lock()-&gt;set_displaced_header(displaced_header);
 836     // if (Atomic::cmpxchg_ptr(/*ex=*/monitor, /*addr*/obj-&gt;mark_addr(), /*cmp*/displaced_header) == displaced_header) {
 837     //   // We stored the monitor address into the object's mark word.
 838     // } else if (THREAD-&gt;is_lock_owned((address)displaced_header))
 839     //   // Simple recursive case.
 840     //   monitor-&gt;lock()-&gt;set_displaced_header(NULL);
 841     // } else {
 842     //   // Slow path.
 843     //   InterpreterRuntime::monitorenter(THREAD, monitor);
 844     // }
 845 
 846     const Register displaced_header = R7_ARG5;
 847     const Register object_mark_addr = R8_ARG6;
 848     const Register current_header   = R9_ARG7;
 849     const Register tmp              = R10_ARG8;
 850 
 851     Label done;
 852     Label cas_failed, slow_case;
 853 
 854     assert_different_registers(displaced_header, object_mark_addr, current_header, tmp);
 855 
 856     // markOop displaced_header = obj-&gt;mark().set_unlocked();
 857 
 858     // Load markOop from object into displaced_header.
 859     ld(displaced_header, oopDesc::mark_offset_in_bytes(), object);
 860 
 861     if (UseBiasedLocking) {
 862       biased_locking_enter(CCR0, object, displaced_header, tmp, current_header, done, &amp;slow_case);
 863     }
 864 
 865     // Set displaced_header to be (markOop of object | UNLOCK_VALUE).
 866     ori(displaced_header, displaced_header, markOopDesc::unlocked_value);
 867 
 868     // monitor-&gt;lock()-&gt;set_displaced_header(displaced_header);
 869 
 870     // Initialize the box (Must happen before we update the object mark!).
 871     std(displaced_header, BasicObjectLock::lock_offset_in_bytes() +
 872         BasicLock::displaced_header_offset_in_bytes(), monitor);
 873 
 874     // if (Atomic::cmpxchg_ptr(/*ex=*/monitor, /*addr*/obj-&gt;mark_addr(), /*cmp*/displaced_header) == displaced_header) {
 875 
 876     // Store stack address of the BasicObjectLock (this is monitor) into object.
 877     addi(object_mark_addr, object, oopDesc::mark_offset_in_bytes());
 878 
 879     // Must fence, otherwise, preceding store(s) may float below cmpxchg.
 880     // CmpxchgX sets CCR0 to cmpX(current, displaced).
 881     cmpxchgd(/*flag=*/CCR0,
 882              /*current_value=*/current_header,
 883              /*compare_value=*/displaced_header, /*exchange_value=*/monitor,
 884              /*where=*/object_mark_addr,
 885              MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
 886              MacroAssembler::cmpxchgx_hint_acquire_lock(),
 887              noreg,
 888              &amp;cas_failed,
 889              /*check without membar and ldarx first*/true);
 890 
 891     // If the compare-and-exchange succeeded, then we found an unlocked
 892     // object and we have now locked it.
 893     b(done);
 894     bind(cas_failed);
 895 
 896     // } else if (THREAD-&gt;is_lock_owned((address)displaced_header))
 897     //   // Simple recursive case.
 898     //   monitor-&gt;lock()-&gt;set_displaced_header(NULL);
 899 
 900     // We did not see an unlocked object so try the fast recursive case.
 901 
 902     // Check if owner is self by comparing the value in the markOop of object
 903     // (current_header) with the stack pointer.
 904     sub(current_header, current_header, R1_SP);
 905 
 906     assert(os::vm_page_size() &gt; 0xfff, "page size too small - change the constant");
 907     load_const_optimized(tmp, ~(os::vm_page_size()-1) | markOopDesc::lock_mask_in_place);
 908 
 909     and_(R0/*==0?*/, current_header, tmp);
 910     // If condition is true we are done and hence we can store 0 in the displaced
 911     // header indicating it is a recursive lock.
 912     bne(CCR0, slow_case);
 913     std(R0/*==0!*/, BasicObjectLock::lock_offset_in_bytes() +
 914         BasicLock::displaced_header_offset_in_bytes(), monitor);
 915     b(done);
 916 
 917     // } else {
 918     //   // Slow path.
 919     //   InterpreterRuntime::monitorenter(THREAD, monitor);
 920 
 921     // None of the above fast optimizations worked so we have to get into the
 922     // slow case of monitor enter.
 923     bind(slow_case);
 924     call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),
 925             monitor, /*check_for_exceptions=*/true);
 926     // }
 927     align(32, 12);
 928     bind(done);
 929   }
 930 }
 931 
 932 // Unlocks an object. Used in monitorexit bytecode and remove_activation.
 933 //
 934 // Registers alive
 935 //   monitor - Address of the BasicObjectLock to be used for locking,
 936 //             which must be initialized with the object to lock.
 937 //
 938 // Throw IllegalMonitorException if object is not locked by current thread.
 939 void InterpreterMacroAssembler::unlock_object(Register monitor, bool check_for_exceptions) {
 940   if (UseHeavyMonitors) {
 941     call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit),
 942             monitor, check_for_exceptions);
 943   } else {
 944 
 945     // template code:
 946     //
 947     // if ((displaced_header = monitor-&gt;displaced_header()) == NULL) {
 948     //   // Recursive unlock. Mark the monitor unlocked by setting the object field to NULL.
 949     //   monitor-&gt;set_obj(NULL);
 950     // } else if (Atomic::cmpxchg_ptr(displaced_header, obj-&gt;mark_addr(), monitor) == monitor) {
 951     //   // We swapped the unlocked mark in displaced_header into the object's mark word.
 952     //   monitor-&gt;set_obj(NULL);
 953     // } else {
 954     //   // Slow path.
 955     //   InterpreterRuntime::monitorexit(THREAD, monitor);
 956     // }
 957 
 958     const Register object           = R7_ARG5;
 959     const Register displaced_header = R8_ARG6;
 960     const Register object_mark_addr = R9_ARG7;
 961     const Register current_header   = R10_ARG8;
 962 
 963     Label free_slot;
 964     Label slow_case;
 965 
 966     assert_different_registers(object, displaced_header, object_mark_addr, current_header);
 967 
 968     if (UseBiasedLocking) {
 969       // The object address from the monitor is in object.
 970       ld(object, BasicObjectLock::obj_offset_in_bytes(), monitor);
 971       assert(oopDesc::mark_offset_in_bytes() == 0, "offset of _mark is not 0");
 972       biased_locking_exit(CCR0, object, displaced_header, free_slot);
 973     }
 974 
 975     // Test first if we are in the fast recursive case.
 976     ld(displaced_header, BasicObjectLock::lock_offset_in_bytes() +
 977            BasicLock::displaced_header_offset_in_bytes(), monitor);
 978 
 979     // If the displaced header is zero, we have a recursive unlock.
 980     cmpdi(CCR0, displaced_header, 0);
 981     beq(CCR0, free_slot); // recursive unlock
 982 
 983     // } else if (Atomic::cmpxchg_ptr(displaced_header, obj-&gt;mark_addr(), monitor) == monitor) {
 984     //   // We swapped the unlocked mark in displaced_header into the object's mark word.
 985     //   monitor-&gt;set_obj(NULL);
 986 
 987     // If we still have a lightweight lock, unlock the object and be done.
 988 
 989     // The object address from the monitor is in object.
 990     if (!UseBiasedLocking) { ld(object, BasicObjectLock::obj_offset_in_bytes(), monitor); }
 991     addi(object_mark_addr, object, oopDesc::mark_offset_in_bytes());
 992 
 993     // We have the displaced header in displaced_header. If the lock is still
 994     // lightweight, it will contain the monitor address and we'll store the
 995     // displaced header back into the object's mark word.
 996     // CmpxchgX sets CCR0 to cmpX(current, monitor).
 997     cmpxchgd(/*flag=*/CCR0,
 998              /*current_value=*/current_header,
 999              /*compare_value=*/monitor, /*exchange_value=*/displaced_header,
1000              /*where=*/object_mark_addr,
1001              MacroAssembler::MemBarRel,
1002              MacroAssembler::cmpxchgx_hint_release_lock(),
1003              noreg,
1004              &amp;slow_case);
1005     b(free_slot);
1006 
1007     // } else {
1008     //   // Slow path.
1009     //   InterpreterRuntime::monitorexit(THREAD, monitor);
1010 
1011     // The lock has been converted into a heavy lock and hence
1012     // we need to get into the slow case.
1013     bind(slow_case);
1014     call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit),
1015             monitor, check_for_exceptions);
1016     // }
1017 
1018     Label done;
1019     b(done); // Monitor register may be overwritten! Runtime has already freed the slot.
1020 
1021     // Exchange worked, do monitor-&gt;set_obj(NULL);
1022     align(32, 12);
1023     bind(free_slot);
1024     li(R0, 0);
1025     std(R0, BasicObjectLock::obj_offset_in_bytes(), monitor);
1026     bind(done);
1027   }
1028 }
1029 
1030 // Load compiled (i2c) or interpreter entry when calling from interpreted and
1031 // do the call. Centralized so that all interpreter calls will do the same actions.
1032 // If jvmti single stepping is on for a thread we must not call compiled code.
1033 //
1034 // Input:
1035 //   - Rtarget_method: method to call
1036 //   - Rret_addr:      return address
1037 //   - 2 scratch regs
1038 //
1039 void InterpreterMacroAssembler::call_from_interpreter(Register Rtarget_method, Register Rret_addr,
1040                                                       Register Rscratch1, Register Rscratch2) {
1041   assert_different_registers(Rscratch1, Rscratch2, Rtarget_method, Rret_addr);
1042   // Assume we want to go compiled if available.
1043   const Register Rtarget_addr = Rscratch1;
1044   const Register Rinterp_only = Rscratch2;
1045 
1046   ld(Rtarget_addr, in_bytes(Method::from_interpreted_offset()), Rtarget_method);
1047 
1048   if (JvmtiExport::can_post_interpreter_events()) {
1049     lwz(Rinterp_only, in_bytes(JavaThread::interp_only_mode_offset()), R16_thread);
1050 
1051     // JVMTI events, such as single-stepping, are implemented partly by avoiding running
1052     // compiled code in threads for which the event is enabled. Check here for
1053     // interp_only_mode if these events CAN be enabled.
1054     Label done;
1055     verify_thread();
1056     cmpwi(CCR0, Rinterp_only, 0);
1057     beq(CCR0, done);
1058     ld(Rtarget_addr, in_bytes(Method::interpreter_entry_offset()), Rtarget_method);
1059     align(32, 12);
1060     bind(done);
1061   }
1062 
1063 #ifdef ASSERT
1064   {
1065     Label Lok;
1066     cmpdi(CCR0, Rtarget_addr, 0);
1067     bne(CCR0, Lok);
1068     stop("null entry point");
1069     bind(Lok);
1070   }
1071 #endif // ASSERT
1072 
1073   mr(R21_sender_SP, R1_SP);
1074 
1075   // Calc a precise SP for the call. The SP value we calculated in
1076   // generate_fixed_frame() is based on the max_stack() value, so we would waste stack space
1077   // if esp is not max. Also, the i2c adapter extends the stack space without restoring
1078   // our pre-calced value, so repeating calls via i2c would result in stack overflow.
1079   // Since esp already points to an empty slot, we just have to sub 1 additional slot
1080   // to meet the abi scratch requirements.
1081   // The max_stack pointer will get restored by means of the GR_Lmax_stack local in
1082   // the return entry of the interpreter.
1083   addi(Rscratch2, R15_esp, Interpreter::stackElementSize - frame::abi_reg_args_size);
1084   clrrdi(Rscratch2, Rscratch2, exact_log2(frame::alignment_in_bytes)); // round towards smaller address
1085   resize_frame_absolute(Rscratch2, Rscratch2, R0);
1086 
1087   mr_if_needed(R19_method, Rtarget_method);
1088   mtctr(Rtarget_addr);
1089   mtlr(Rret_addr);
1090 
1091   save_interpreter_state(Rscratch2);
1092 #ifdef ASSERT
1093   ld(Rscratch1, _ijava_state_neg(top_frame_sp), Rscratch2); // Rscratch2 contains fp
1094   cmpd(CCR0, R21_sender_SP, Rscratch1);
1095   asm_assert_eq("top_frame_sp incorrect", 0x951);
1096 #endif
1097 
1098   bctr();
1099 }
1100 
1101 // Set the method data pointer for the current bcp.
1102 void InterpreterMacroAssembler::set_method_data_pointer_for_bcp() {
1103   assert(ProfileInterpreter, "must be profiling interpreter");
1104   Label get_continue;
1105   ld(R28_mdx, in_bytes(Method::method_data_offset()), R19_method);
1106   test_method_data_pointer(get_continue);
1107   call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::bcp_to_di), R19_method, R14_bcp);
1108 
1109   addi(R28_mdx, R28_mdx, in_bytes(MethodData::data_offset()));
1110   add(R28_mdx, R28_mdx, R3_RET);
1111   bind(get_continue);
1112 }
1113 
1114 // Test ImethodDataPtr. If it is null, continue at the specified label.
1115 void InterpreterMacroAssembler::test_method_data_pointer(Label&amp; zero_continue) {
1116   assert(ProfileInterpreter, "must be profiling interpreter");
1117   cmpdi(CCR0, R28_mdx, 0);
1118   beq(CCR0, zero_continue);
1119 }
1120 
1121 void InterpreterMacroAssembler::verify_method_data_pointer() {
1122   assert(ProfileInterpreter, "must be profiling interpreter");
1123 #ifdef ASSERT
1124   Label verify_continue;
1125   test_method_data_pointer(verify_continue);
1126 
1127   // If the mdp is valid, it will point to a DataLayout header which is
1128   // consistent with the bcp. The converse is highly probable also.
1129   lhz(R11_scratch1, in_bytes(DataLayout::bci_offset()), R28_mdx);
1130   ld(R12_scratch2, in_bytes(Method::const_offset()), R19_method);
1131   addi(R11_scratch1, R11_scratch1, in_bytes(ConstMethod::codes_offset()));
1132   add(R11_scratch1, R12_scratch2, R12_scratch2);
1133   cmpd(CCR0, R11_scratch1, R14_bcp);
1134   beq(CCR0, verify_continue);
1135 
1136   call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::verify_mdp ), R19_method, R14_bcp, R28_mdx);
1137 
1138   bind(verify_continue);
1139 #endif
1140 }
1141 
1142 void InterpreterMacroAssembler::test_invocation_counter_for_mdp(Register invocation_count,
1143                                                                 Register method_counters,
1144                                                                 Register Rscratch,
1145                                                                 Label &amp;profile_continue) {
1146   assert(ProfileInterpreter, "must be profiling interpreter");
1147   // Control will flow to "profile_continue" if the counter is less than the
1148   // limit or if we call profile_method().
1149   Label done;
1150 
1151   // If no method data exists, and the counter is high enough, make one.
1152   lwz(Rscratch, in_bytes(MethodCounters::interpreter_profile_limit_offset()), method_counters);
1153 
1154   cmpdi(CCR0, R28_mdx, 0);
1155   // Test to see if we should create a method data oop.
1156   cmpd(CCR1, Rscratch, invocation_count);
1157   bne(CCR0, done);
1158   bge(CCR1, profile_continue);
1159 
1160   // Build it now.
1161   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));
1162   set_method_data_pointer_for_bcp();
1163   b(profile_continue);
1164 
1165   align(32, 12);
1166   bind(done);
1167 }
1168 
1169 void InterpreterMacroAssembler::test_backedge_count_for_osr(Register backedge_count, Register method_counters,
1170                                                             Register target_bcp, Register disp, Register Rtmp) {
1171   assert_different_registers(backedge_count, target_bcp, disp, Rtmp, R4_ARG2);
1172   assert(UseOnStackReplacement,"Must UseOnStackReplacement to test_backedge_count_for_osr");
1173 
1174   Label did_not_overflow;
1175   Label overflow_with_error;
1176 
1177   lwz(Rtmp, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset()), method_counters);
1178   cmpw(CCR0, backedge_count, Rtmp);
1179 
1180   blt(CCR0, did_not_overflow);
1181 
1182   // When ProfileInterpreter is on, the backedge_count comes from the
1183   // methodDataOop, which value does not get reset on the call to
1184   // frequency_counter_overflow(). To avoid excessive calls to the overflow
1185   // routine while the method is being compiled, add a second test to make sure
1186   // the overflow function is called only once every overflow_frequency.
1187   if (ProfileInterpreter) {
1188     const int overflow_frequency = 1024;
1189     andi_(Rtmp, backedge_count, overflow_frequency-1);
1190     bne(CCR0, did_not_overflow);
1191   }
1192 
1193   // Overflow in loop, pass branch bytecode.
1194   subf(R4_ARG2, disp, target_bcp); // Compute branch bytecode (previous bcp).
1195   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow), R4_ARG2, true);
1196 
1197   // Was an OSR adapter generated?
1198   cmpdi(CCR0, R3_RET, 0);
1199   beq(CCR0, overflow_with_error);
1200 
1201   // Has the nmethod been invalidated already?
1202   lbz(Rtmp, nmethod::state_offset(), R3_RET);
1203   cmpwi(CCR0, Rtmp, nmethod::in_use);
1204   bne(CCR0, overflow_with_error);
1205 
1206   // Migrate the interpreter frame off of the stack.
1207   // We can use all registers because we will not return to interpreter from this point.
1208 
1209   // Save nmethod.
1210   const Register osr_nmethod = R31;
1211   mr(osr_nmethod, R3_RET);
1212   set_top_ijava_frame_at_SP_as_last_Java_frame(R1_SP, R11_scratch1);
1213   call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin), R16_thread);
1214   reset_last_Java_frame();
1215   // OSR buffer is in ARG1
1216 
1217   // Remove the interpreter frame.
1218   merge_frames(/*top_frame_sp*/ R21_sender_SP, /*return_pc*/ R0, R11_scratch1, R12_scratch2);
1219 
1220   // Jump to the osr code.
1221   ld(R11_scratch1, nmethod::osr_entry_point_offset(), osr_nmethod);
1222   mtlr(R0);
1223   mtctr(R11_scratch1);
1224   bctr();
1225 
1226   align(32, 12);
1227   bind(overflow_with_error);
1228   bind(did_not_overflow);
1229 }
1230 
1231 // Store a value at some constant offset from the method data pointer.
1232 void InterpreterMacroAssembler::set_mdp_data_at(int constant, Register value) {
1233   assert(ProfileInterpreter, "must be profiling interpreter");
1234 
1235   std(value, constant, R28_mdx);
1236 }
1237 
1238 // Increment the value at some constant offset from the method data pointer.
1239 void InterpreterMacroAssembler::increment_mdp_data_at(int constant,
1240                                                       Register counter_addr,
1241                                                       Register Rbumped_count,
1242                                                       bool decrement) {
1243   // Locate the counter at a fixed offset from the mdp:
1244   addi(counter_addr, R28_mdx, constant);
1245   increment_mdp_data_at(counter_addr, Rbumped_count, decrement);
1246 }
1247 
1248 // Increment the value at some non-fixed (reg + constant) offset from
1249 // the method data pointer.
1250 void InterpreterMacroAssembler::increment_mdp_data_at(Register reg,
1251                                                       int constant,
1252                                                       Register scratch,
1253                                                       Register Rbumped_count,
1254                                                       bool decrement) {
1255   // Add the constant to reg to get the offset.
1256   add(scratch, R28_mdx, reg);
1257   // Then calculate the counter address.
1258   addi(scratch, scratch, constant);
1259   increment_mdp_data_at(scratch, Rbumped_count, decrement);
1260 }
1261 
1262 void InterpreterMacroAssembler::increment_mdp_data_at(Register counter_addr,
1263                                                       Register Rbumped_count,
1264                                                       bool decrement) {
1265   assert(ProfileInterpreter, "must be profiling interpreter");
1266 
1267   // Load the counter.
1268   ld(Rbumped_count, 0, counter_addr);
1269 
1270   if (decrement) {
1271     // Decrement the register. Set condition codes.
1272     addi(Rbumped_count, Rbumped_count, - DataLayout::counter_increment);
1273     // Store the decremented counter, if it is still negative.
1274     std(Rbumped_count, 0, counter_addr);
1275     // Note: add/sub overflow check are not ported, since 64 bit
1276     // calculation should never overflow.
1277   } else {
1278     // Increment the register. Set carry flag.
1279     addi(Rbumped_count, Rbumped_count, DataLayout::counter_increment);
1280     // Store the incremented counter.
1281     std(Rbumped_count, 0, counter_addr);
1282   }
1283 }
1284 
1285 // Set a flag value at the current method data pointer position.
1286 void InterpreterMacroAssembler::set_mdp_flag_at(int flag_constant,
1287                                                 Register scratch) {
1288   assert(ProfileInterpreter, "must be profiling interpreter");
1289   // Load the data header.
1290   lbz(scratch, in_bytes(DataLayout::flags_offset()), R28_mdx);
1291   // Set the flag.
1292   ori(scratch, scratch, flag_constant);
1293   // Store the modified header.
1294   stb(scratch, in_bytes(DataLayout::flags_offset()), R28_mdx);
1295 }
1296 
1297 // Test the location at some offset from the method data pointer.
1298 // If it is not equal to value, branch to the not_equal_continue Label.
1299 void InterpreterMacroAssembler::test_mdp_data_at(int offset,
1300                                                  Register value,
1301                                                  Label&amp; not_equal_continue,
1302                                                  Register test_out) {
1303   assert(ProfileInterpreter, "must be profiling interpreter");
1304 
1305   ld(test_out, offset, R28_mdx);
1306   cmpd(CCR0,  value, test_out);
1307   bne(CCR0, not_equal_continue);
1308 }
1309 
1310 // Update the method data pointer by the displacement located at some fixed
1311 // offset from the method data pointer.
1312 void InterpreterMacroAssembler::update_mdp_by_offset(int offset_of_disp,
1313                                                      Register scratch) {
1314   assert(ProfileInterpreter, "must be profiling interpreter");
1315 
1316   ld(scratch, offset_of_disp, R28_mdx);
1317   add(R28_mdx, scratch, R28_mdx);
1318 }
1319 
1320 // Update the method data pointer by the displacement located at the
1321 // offset (reg + offset_of_disp).
1322 void InterpreterMacroAssembler::update_mdp_by_offset(Register reg,
1323                                                      int offset_of_disp,
1324                                                      Register scratch) {
1325   assert(ProfileInterpreter, "must be profiling interpreter");
1326 
1327   add(scratch, reg, R28_mdx);
1328   ld(scratch, offset_of_disp, scratch);
1329   add(R28_mdx, scratch, R28_mdx);
1330 }
1331 
1332 // Update the method data pointer by a simple constant displacement.
1333 void InterpreterMacroAssembler::update_mdp_by_constant(int constant) {
1334   assert(ProfileInterpreter, "must be profiling interpreter");
1335   addi(R28_mdx, R28_mdx, constant);
1336 }
1337 
1338 // Update the method data pointer for a _ret bytecode whose target
1339 // was not among our cached targets.
1340 void InterpreterMacroAssembler::update_mdp_for_ret(TosState state,
1341                                                    Register return_bci) {
1342   assert(ProfileInterpreter, "must be profiling interpreter");
1343 
1344   push(state);
1345   assert(return_bci-&gt;is_nonvolatile(), "need to protect return_bci");
1346   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::update_mdp_for_ret), return_bci);
1347   pop(state);
1348 }
1349 
1350 // Increments the backedge counter.
1351 // Returns backedge counter + invocation counter in Rdst.
1352 void InterpreterMacroAssembler::increment_backedge_counter(const Register Rcounters, const Register Rdst,
1353                                                            const Register Rtmp1, Register Rscratch) {
1354   assert(UseCompiler, "incrementing must be useful");
1355   assert_different_registers(Rdst, Rtmp1);
1356   const Register invocation_counter = Rtmp1;
1357   const Register counter = Rdst;
1358   // TODO: PPC port: assert(4 == InvocationCounter::sz_counter(), "unexpected field size.");
1359 
1360   // Load backedge counter.
1361   lwz(counter, in_bytes(MethodCounters::backedge_counter_offset()) +
1362                in_bytes(InvocationCounter::counter_offset()), Rcounters);
1363   // Load invocation counter.
1364   lwz(invocation_counter, in_bytes(MethodCounters::invocation_counter_offset()) +
1365                           in_bytes(InvocationCounter::counter_offset()), Rcounters);
1366 
1367   // Add the delta to the backedge counter.
1368   addi(counter, counter, InvocationCounter::count_increment);
1369 
1370   // Mask the invocation counter.
1371   andi(invocation_counter, invocation_counter, InvocationCounter::count_mask_value);
1372 
1373   // Store new counter value.
1374   stw(counter, in_bytes(MethodCounters::backedge_counter_offset()) +
1375                in_bytes(InvocationCounter::counter_offset()), Rcounters);
1376   // Return invocation counter + backedge counter.
1377   add(counter, counter, invocation_counter);
1378 }
1379 
1380 // Count a taken branch in the bytecodes.
1381 void InterpreterMacroAssembler::profile_taken_branch(Register scratch, Register bumped_count) {
1382   if (ProfileInterpreter) {
1383     Label profile_continue;
1384 
1385     // If no method data exists, go to profile_continue.
1386     test_method_data_pointer(profile_continue);
1387 
1388     // We are taking a branch. Increment the taken count.
1389     increment_mdp_data_at(in_bytes(JumpData::taken_offset()), scratch, bumped_count);
1390 
1391     // The method data pointer needs to be updated to reflect the new target.
1392     update_mdp_by_offset(in_bytes(JumpData::displacement_offset()), scratch);
1393     bind (profile_continue);
1394   }
1395 }
1396 
1397 // Count a not-taken branch in the bytecodes.
1398 void InterpreterMacroAssembler::profile_not_taken_branch(Register scratch1, Register scratch2) {
1399   if (ProfileInterpreter) {
1400     Label profile_continue;
1401 
1402     // If no method data exists, go to profile_continue.
1403     test_method_data_pointer(profile_continue);
1404 
1405     // We are taking a branch. Increment the not taken count.
1406     increment_mdp_data_at(in_bytes(BranchData::not_taken_offset()), scratch1, scratch2);
1407 
1408     // The method data pointer needs to be updated to correspond to the
1409     // next bytecode.
1410     update_mdp_by_constant(in_bytes(BranchData::branch_data_size()));
1411     bind (profile_continue);
1412   }
1413 }
1414 
1415 // Count a non-virtual call in the bytecodes.
1416 void InterpreterMacroAssembler::profile_call(Register scratch1, Register scratch2) {
1417   if (ProfileInterpreter) {
1418     Label profile_continue;
1419 
1420     // If no method data exists, go to profile_continue.
1421     test_method_data_pointer(profile_continue);
1422 
1423     // We are making a call. Increment the count.
1424     increment_mdp_data_at(in_bytes(CounterData::count_offset()), scratch1, scratch2);
1425 
1426     // The method data pointer needs to be updated to reflect the new target.
1427     update_mdp_by_constant(in_bytes(CounterData::counter_data_size()));
1428     bind (profile_continue);
1429   }
1430 }
1431 
1432 // Count a final call in the bytecodes.
1433 void InterpreterMacroAssembler::profile_final_call(Register scratch1, Register scratch2) {
1434   if (ProfileInterpreter) {
1435     Label profile_continue;
1436 
1437     // If no method data exists, go to profile_continue.
1438     test_method_data_pointer(profile_continue);
1439 
1440     // We are making a call. Increment the count.
1441     increment_mdp_data_at(in_bytes(CounterData::count_offset()), scratch1, scratch2);
1442 
1443     // The method data pointer needs to be updated to reflect the new target.
1444     update_mdp_by_constant(in_bytes(VirtualCallData::virtual_call_data_size()));
1445     bind (profile_continue);
1446   }
1447 }
1448 
1449 // Count a virtual call in the bytecodes.
1450 void InterpreterMacroAssembler::profile_virtual_call(Register Rreceiver,
1451                                                      Register Rscratch1,
1452                                                      Register Rscratch2,
1453                                                      bool receiver_can_be_null) {
1454   if (!ProfileInterpreter) { return; }
1455   Label profile_continue;
1456 
1457   // If no method data exists, go to profile_continue.
1458   test_method_data_pointer(profile_continue);
1459 
1460   Label skip_receiver_profile;
1461   if (receiver_can_be_null) {
1462     Label not_null;
1463     cmpdi(CCR0, Rreceiver, 0);
1464     bne(CCR0, not_null);
1465     // We are making a call. Increment the count for null receiver.
1466     increment_mdp_data_at(in_bytes(CounterData::count_offset()), Rscratch1, Rscratch2);
1467     b(skip_receiver_profile);
1468     bind(not_null);
1469   }
1470 
1471   // Record the receiver type.
1472   record_klass_in_profile(Rreceiver, Rscratch1, Rscratch2, true);
1473   bind(skip_receiver_profile);
1474 
1475   // The method data pointer needs to be updated to reflect the new target.
1476   update_mdp_by_constant(in_bytes(VirtualCallData::virtual_call_data_size()));
1477   bind (profile_continue);
1478 }
1479 
1480 void InterpreterMacroAssembler::profile_typecheck(Register Rklass, Register Rscratch1, Register Rscratch2) {
1481   if (ProfileInterpreter) {
1482     Label profile_continue;
1483 
1484     // If no method data exists, go to profile_continue.
1485     test_method_data_pointer(profile_continue);
1486 
1487     int mdp_delta = in_bytes(BitData::bit_data_size());
1488     if (TypeProfileCasts) {
1489       mdp_delta = in_bytes(VirtualCallData::virtual_call_data_size());
1490 
1491       // Record the object type.
1492       record_klass_in_profile(Rklass, Rscratch1, Rscratch2, false);
1493     }
1494 
1495     // The method data pointer needs to be updated.
1496     update_mdp_by_constant(mdp_delta);
1497 
1498     bind (profile_continue);
1499   }
1500 }
1501 
1502 void InterpreterMacroAssembler::profile_typecheck_failed(Register Rscratch1, Register Rscratch2) {
1503   if (ProfileInterpreter &amp;&amp; TypeProfileCasts) {
1504     Label profile_continue;
1505 
1506     // If no method data exists, go to profile_continue.
1507     test_method_data_pointer(profile_continue);
1508 
1509     int count_offset = in_bytes(CounterData::count_offset());
1510     // Back up the address, since we have already bumped the mdp.
1511     count_offset -= in_bytes(VirtualCallData::virtual_call_data_size());
1512 
1513     // *Decrement* the counter. We expect to see zero or small negatives.
1514     increment_mdp_data_at(count_offset, Rscratch1, Rscratch2, true);
1515 
1516     bind (profile_continue);
1517   }
1518 }
1519 
1520 // Count a ret in the bytecodes.
1521 void InterpreterMacroAssembler::profile_ret(TosState state, Register return_bci,
1522                                             Register scratch1, Register scratch2) {
1523   if (ProfileInterpreter) {
1524     Label profile_continue;
1525     uint row;
1526 
1527     // If no method data exists, go to profile_continue.
1528     test_method_data_pointer(profile_continue);
1529 
1530     // Update the total ret count.
1531     increment_mdp_data_at(in_bytes(CounterData::count_offset()), scratch1, scratch2 );
1532 
1533     for (row = 0; row &lt; RetData::row_limit(); row++) {
1534       Label next_test;
1535 
1536       // See if return_bci is equal to bci[n]:
1537       test_mdp_data_at(in_bytes(RetData::bci_offset(row)), return_bci, next_test, scratch1);
1538 
1539       // return_bci is equal to bci[n]. Increment the count.
1540       increment_mdp_data_at(in_bytes(RetData::bci_count_offset(row)), scratch1, scratch2);
1541 
1542       // The method data pointer needs to be updated to reflect the new target.
1543       update_mdp_by_offset(in_bytes(RetData::bci_displacement_offset(row)), scratch1);
1544       b(profile_continue);
1545       bind(next_test);
1546     }
1547 
1548     update_mdp_for_ret(state, return_bci);
1549 
1550     bind (profile_continue);
1551   }
1552 }
1553 
1554 // Count the default case of a switch construct.
1555 void InterpreterMacroAssembler::profile_switch_default(Register scratch1,  Register scratch2) {
1556   if (ProfileInterpreter) {
1557     Label profile_continue;
1558 
1559     // If no method data exists, go to profile_continue.
1560     test_method_data_pointer(profile_continue);
1561 
1562     // Update the default case count
1563     increment_mdp_data_at(in_bytes(MultiBranchData::default_count_offset()),
1564                           scratch1, scratch2);
1565 
1566     // The method data pointer needs to be updated.
1567     update_mdp_by_offset(in_bytes(MultiBranchData::default_displacement_offset()),
1568                          scratch1);
1569 
1570     bind (profile_continue);
1571   }
1572 }
1573 
1574 // Count the index'th case of a switch construct.
1575 void InterpreterMacroAssembler::profile_switch_case(Register index,
1576                                                     Register scratch1,
1577                                                     Register scratch2,
1578                                                     Register scratch3) {
1579   if (ProfileInterpreter) {
1580     assert_different_registers(index, scratch1, scratch2, scratch3);
1581     Label profile_continue;
1582 
1583     // If no method data exists, go to profile_continue.
1584     test_method_data_pointer(profile_continue);
1585 
1586     // Build the base (index * per_case_size_in_bytes()) + case_array_offset_in_bytes().
1587     li(scratch3, in_bytes(MultiBranchData::case_array_offset()));
1588 
1589     assert (in_bytes(MultiBranchData::per_case_size()) == 16, "so that shladd works");
1590     sldi(scratch1, index, exact_log2(in_bytes(MultiBranchData::per_case_size())));
1591     add(scratch1, scratch1, scratch3);
1592 
1593     // Update the case count.
1594     increment_mdp_data_at(scratch1, in_bytes(MultiBranchData::relative_count_offset()), scratch2, scratch3);
1595 
1596     // The method data pointer needs to be updated.
1597     update_mdp_by_offset(scratch1, in_bytes(MultiBranchData::relative_displacement_offset()), scratch2);
1598 
1599     bind (profile_continue);
1600   }
1601 }
1602 
1603 void InterpreterMacroAssembler::profile_null_seen(Register Rscratch1, Register Rscratch2) {
1604   if (ProfileInterpreter) {
1605     assert_different_registers(Rscratch1, Rscratch2);
1606     Label profile_continue;
1607 
1608     // If no method data exists, go to profile_continue.
1609     test_method_data_pointer(profile_continue);
1610 
1611     set_mdp_flag_at(BitData::null_seen_byte_constant(), Rscratch1);
1612 
1613     // The method data pointer needs to be updated.
1614     int mdp_delta = in_bytes(BitData::bit_data_size());
1615     if (TypeProfileCasts) {
1616       mdp_delta = in_bytes(VirtualCallData::virtual_call_data_size());
1617     }
1618     update_mdp_by_constant(mdp_delta);
1619 
1620     bind (profile_continue);
1621   }
1622 }
1623 
1624 void InterpreterMacroAssembler::record_klass_in_profile(Register Rreceiver,
1625                                                         Register Rscratch1, Register Rscratch2,
1626                                                         bool is_virtual_call) {
1627   assert(ProfileInterpreter, "must be profiling");
1628   assert_different_registers(Rreceiver, Rscratch1, Rscratch2);
1629 
1630   Label done;
1631   record_klass_in_profile_helper(Rreceiver, Rscratch1, Rscratch2, 0, done, is_virtual_call);
1632   bind (done);
1633 }
1634 
1635 void InterpreterMacroAssembler::record_klass_in_profile_helper(
1636                                         Register receiver, Register scratch1, Register scratch2,
1637                                         int start_row, Label&amp; done, bool is_virtual_call) {
1638   if (TypeProfileWidth == 0) {
1639     if (is_virtual_call) {
1640       increment_mdp_data_at(in_bytes(CounterData::count_offset()), scratch1, scratch2);
1641     }
1642     return;
1643   }
1644 
1645   int last_row = VirtualCallData::row_limit() - 1;
1646   assert(start_row &lt;= last_row, "must be work left to do");
1647   // Test this row for both the receiver and for null.
1648   // Take any of three different outcomes:
1649   //   1. found receiver =&gt; increment count and goto done
1650   //   2. found null =&gt; keep looking for case 1, maybe allocate this cell
1651   //   3. found something else =&gt; keep looking for cases 1 and 2
1652   // Case 3 is handled by a recursive call.
1653   for (int row = start_row; row &lt;= last_row; row++) {
1654     Label next_test;
1655     bool test_for_null_also = (row == start_row);
1656 
1657     // See if the receiver is receiver[n].
1658     int recvr_offset = in_bytes(VirtualCallData::receiver_offset(row));
1659     test_mdp_data_at(recvr_offset, receiver, next_test, scratch1);
1660     // delayed()-&gt;tst(scratch);
1661 
1662     // The receiver is receiver[n]. Increment count[n].
1663     int count_offset = in_bytes(VirtualCallData::receiver_count_offset(row));
1664     increment_mdp_data_at(count_offset, scratch1, scratch2);
1665     b(done);
1666     bind(next_test);
1667 
1668     if (test_for_null_also) {
1669       Label found_null;
1670       // Failed the equality check on receiver[n]... Test for null.
1671       if (start_row == last_row) {
1672         // The only thing left to do is handle the null case.
1673         if (is_virtual_call) {
1674           // Scratch1 contains test_out from test_mdp_data_at.
1675           cmpdi(CCR0, scratch1, 0);
1676           beq(CCR0, found_null);
1677           // Receiver did not match any saved receiver and there is no empty row for it.
1678           // Increment total counter to indicate polymorphic case.
1679           increment_mdp_data_at(in_bytes(CounterData::count_offset()), scratch1, scratch2);
1680           b(done);
1681           bind(found_null);
1682         } else {
1683           cmpdi(CCR0, scratch1, 0);
1684           bne(CCR0, done);
1685         }
1686         break;
1687       }
1688       // Since null is rare, make it be the branch-taken case.
1689       cmpdi(CCR0, scratch1, 0);
1690       beq(CCR0, found_null);
1691 
1692       // Put all the "Case 3" tests here.
1693       record_klass_in_profile_helper(receiver, scratch1, scratch2, start_row + 1, done, is_virtual_call);
1694 
1695       // Found a null. Keep searching for a matching receiver,
1696       // but remember that this is an empty (unused) slot.
1697       bind(found_null);
1698     }
1699   }
1700 
1701   // In the fall-through case, we found no matching receiver, but we
1702   // observed the receiver[start_row] is NULL.
1703 
1704   // Fill in the receiver field and increment the count.
1705   int recvr_offset = in_bytes(VirtualCallData::receiver_offset(start_row));
1706   set_mdp_data_at(recvr_offset, receiver);
1707   int count_offset = in_bytes(VirtualCallData::receiver_count_offset(start_row));
1708   li(scratch1, DataLayout::counter_increment);
1709   set_mdp_data_at(count_offset, scratch1);
1710   if (start_row &gt; 0) {
1711     b(done);
1712   }
1713 }
1714 
1715 // Argument and return type profilig.
1716 // kills: tmp, tmp2, R0, CR0, CR1
1717 void InterpreterMacroAssembler::profile_obj_type(Register obj, Register mdo_addr_base,
1718                                                  RegisterOrConstant mdo_addr_offs,
1719                                                  Register tmp, Register tmp2) {
1720   Label do_nothing, do_update;
1721 
1722   // tmp2 = obj is allowed
1723   assert_different_registers(obj, mdo_addr_base, tmp, R0);
1724   assert_different_registers(tmp2, mdo_addr_base, tmp, R0);
1725   const Register klass = tmp2;
1726 
1727   verify_oop(obj);
1728 
1729   ld(tmp, mdo_addr_offs, mdo_addr_base);
1730 
1731   // Set null_seen if obj is 0.
1732   cmpdi(CCR0, obj, 0);
1733   ori(R0, tmp, TypeEntries::null_seen);
1734   beq(CCR0, do_update);
1735 
1736   load_klass(klass, obj);
1737 
1738   clrrdi(R0, tmp, exact_log2(-TypeEntries::type_klass_mask));
1739   // Basically same as andi(R0, tmp, TypeEntries::type_klass_mask);
1740   cmpd(CCR1, R0, klass);
1741   // Klass seen before, nothing to do (regardless of unknown bit).
1742   //beq(CCR1, do_nothing);
1743 
1744   andi_(R0, klass, TypeEntries::type_unknown);
1745   // Already unknown. Nothing to do anymore.
1746   //bne(CCR0, do_nothing);
1747   crorc(CCR0, Assembler::equal, CCR1, Assembler::equal); // cr0 eq = cr1 eq or cr0 ne
1748   beq(CCR0, do_nothing);
1749 
1750   clrrdi_(R0, tmp, exact_log2(-TypeEntries::type_mask));
1751   orr(R0, klass, tmp); // Combine klass and null_seen bit (only used if (tmp &amp; type_mask)==0).
1752   beq(CCR0, do_update); // First time here. Set profile type.
1753 
1754   // Different than before. Cannot keep accurate profile.
1755   ori(R0, tmp, TypeEntries::type_unknown);
1756 
1757   bind(do_update);
1758   // update profile
1759   std(R0, mdo_addr_offs, mdo_addr_base);
1760 
1761   align(32, 12);
1762   bind(do_nothing);
1763 }
1764 
1765 void InterpreterMacroAssembler::profile_arguments_type(Register callee,
1766                                                        Register tmp1, Register tmp2,
1767                                                        bool is_virtual) {
1768   if (!ProfileInterpreter) {
1769     return;
1770   }
1771 
1772   assert_different_registers(callee, tmp1, tmp2, R28_mdx);
1773 
1774   if (MethodData::profile_arguments() || MethodData::profile_return()) {
1775     Label profile_continue;
1776 
1777     test_method_data_pointer(profile_continue);
1778 
1779     int off_to_start = is_virtual ?
1780       in_bytes(VirtualCallData::virtual_call_data_size()) : in_bytes(CounterData::counter_data_size());
1781 
1782     lbz(tmp1, in_bytes(DataLayout::tag_offset()) - off_to_start, R28_mdx);
1783     cmpwi(CCR0, tmp1, is_virtual ? DataLayout::virtual_call_type_data_tag : DataLayout::call_type_data_tag);
1784     bne(CCR0, profile_continue);
1785 
1786     if (MethodData::profile_arguments()) {
1787       Label done;
1788       int off_to_args = in_bytes(TypeEntriesAtCall::args_data_offset());
1789       add(R28_mdx, off_to_args, R28_mdx);
1790 
1791       for (int i = 0; i &lt; TypeProfileArgsLimit; i++) {
1792         if (i &gt; 0 || MethodData::profile_return()) {
1793           // If return value type is profiled we may have no argument to profile.
1794           ld(tmp1, in_bytes(TypeEntriesAtCall::cell_count_offset())-off_to_args, R28_mdx);
1795           cmpdi(CCR0, tmp1, (i+1)*TypeStackSlotEntries::per_arg_count());
1796           addi(tmp1, tmp1, -i*TypeStackSlotEntries::per_arg_count());
1797           blt(CCR0, done);
1798         }
1799         ld(tmp1, in_bytes(Method::const_offset()), callee);
1800         lhz(tmp1, in_bytes(ConstMethod::size_of_parameters_offset()), tmp1);
1801         // Stack offset o (zero based) from the start of the argument
1802         // list, for n arguments translates into offset n - o - 1 from
1803         // the end of the argument list. But there's an extra slot at
1804         // the top of the stack. So the offset is n - o from Lesp.
1805         ld(tmp2, in_bytes(TypeEntriesAtCall::stack_slot_offset(i))-off_to_args, R28_mdx);
1806         subf(tmp1, tmp2, tmp1);
1807 
1808         sldi(tmp1, tmp1, Interpreter::logStackElementSize);
1809         ldx(tmp1, tmp1, R15_esp);
1810 
1811         profile_obj_type(tmp1, R28_mdx, in_bytes(TypeEntriesAtCall::argument_type_offset(i))-off_to_args, tmp2, tmp1);
1812 
1813         int to_add = in_bytes(TypeStackSlotEntries::per_arg_size());
1814         addi(R28_mdx, R28_mdx, to_add);
1815         off_to_args += to_add;
1816       }
1817 
1818       if (MethodData::profile_return()) {
1819         ld(tmp1, in_bytes(TypeEntriesAtCall::cell_count_offset())-off_to_args, R28_mdx);
1820         addi(tmp1, tmp1, -TypeProfileArgsLimit*TypeStackSlotEntries::per_arg_count());
1821       }
1822 
1823       bind(done);
1824 
1825       if (MethodData::profile_return()) {
1826         // We're right after the type profile for the last
1827         // argument. tmp1 is the number of cells left in the
1828         // CallTypeData/VirtualCallTypeData to reach its end. Non null
1829         // if there's a return to profile.
1830         assert(ReturnTypeEntry::static_cell_count() &lt; TypeStackSlotEntries::per_arg_count(),
1831                "can't move past ret type");
1832         sldi(tmp1, tmp1, exact_log2(DataLayout::cell_size));
1833         add(R28_mdx, tmp1, R28_mdx);
1834       }
1835     } else {
1836       assert(MethodData::profile_return(), "either profile call args or call ret");
1837       update_mdp_by_constant(in_bytes(TypeEntriesAtCall::return_only_size()));
1838     }
1839 
1840     // Mdp points right after the end of the
1841     // CallTypeData/VirtualCallTypeData, right after the cells for the
1842     // return value type if there's one.
1843     align(32, 12);
1844     bind(profile_continue);
1845   }
1846 }
1847 
1848 void InterpreterMacroAssembler::profile_return_type(Register ret, Register tmp1, Register tmp2) {
1849   assert_different_registers(ret, tmp1, tmp2);
1850   if (ProfileInterpreter &amp;&amp; MethodData::profile_return()) {
1851     Label profile_continue;
1852 
1853     test_method_data_pointer(profile_continue);
1854 
1855     if (MethodData::profile_return_jsr292_only()) {
1856       // If we don't profile all invoke bytecodes we must make sure
1857       // it's a bytecode we indeed profile. We can't go back to the
1858       // begining of the ProfileData we intend to update to check its
1859       // type because we're right after it and we don't known its
1860       // length.
1861       lbz(tmp1, 0, R14_bcp);
1862       lbz(tmp2, Method::intrinsic_id_offset_in_bytes(), R19_method);
1863       cmpwi(CCR0, tmp1, Bytecodes::_invokedynamic);
1864       cmpwi(CCR1, tmp1, Bytecodes::_invokehandle);
1865       cror(CCR0, Assembler::equal, CCR1, Assembler::equal);
1866       cmpwi(CCR1, tmp2, vmIntrinsics::_compiledLambdaForm);
1867       cror(CCR0, Assembler::equal, CCR1, Assembler::equal);
1868       bne(CCR0, profile_continue);
1869     }
1870 
1871     profile_obj_type(ret, R28_mdx, -in_bytes(ReturnTypeEntry::size()), tmp1, tmp2);
1872 
1873     align(32, 12);
1874     bind(profile_continue);
1875   }
1876 }
1877 
1878 void InterpreterMacroAssembler::profile_parameters_type(Register tmp1, Register tmp2,
1879                                                         Register tmp3, Register tmp4) {
1880   if (ProfileInterpreter &amp;&amp; MethodData::profile_parameters()) {
1881     Label profile_continue, done;
1882 
1883     test_method_data_pointer(profile_continue);
1884 
1885     // Load the offset of the area within the MDO used for
1886     // parameters. If it's negative we're not profiling any parameters.
1887     lwz(tmp1, in_bytes(MethodData::parameters_type_data_di_offset()) - in_bytes(MethodData::data_offset()), R28_mdx);
1888     cmpwi(CCR0, tmp1, 0);
1889     blt(CCR0, profile_continue);
1890 
1891     // Compute a pointer to the area for parameters from the offset
1892     // and move the pointer to the slot for the last
1893     // parameters. Collect profiling from last parameter down.
1894     // mdo start + parameters offset + array length - 1
1895 
1896     // Pointer to the parameter area in the MDO.
1897     const Register mdp = tmp1;
1898     add(mdp, tmp1, R28_mdx);
1899 
1900     // Offset of the current profile entry to update.
1901     const Register entry_offset = tmp2;
1902     // entry_offset = array len in number of cells
1903     ld(entry_offset, in_bytes(ArrayData::array_len_offset()), mdp);
1904 
1905     int off_base = in_bytes(ParametersTypeData::stack_slot_offset(0));
1906     assert(off_base % DataLayout::cell_size == 0, "should be a number of cells");
1907 
1908     // entry_offset (number of cells)  = array len - size of 1 entry + offset of the stack slot field
1909     addi(entry_offset, entry_offset, -TypeStackSlotEntries::per_arg_count() + (off_base / DataLayout::cell_size));
1910     // entry_offset in bytes
1911     sldi(entry_offset, entry_offset, exact_log2(DataLayout::cell_size));
1912 
1913     Label loop;
1914     align(32, 12);
1915     bind(loop);
1916 
1917     // Load offset on the stack from the slot for this parameter.
1918     ld(tmp3, entry_offset, mdp);
1919     sldi(tmp3, tmp3, Interpreter::logStackElementSize);
1920     neg(tmp3, tmp3);
1921     // Read the parameter from the local area.
1922     ldx(tmp3, tmp3, R18_locals);
1923 
1924     // Make entry_offset now point to the type field for this parameter.
1925     int type_base = in_bytes(ParametersTypeData::type_offset(0));
1926     assert(type_base &gt; off_base, "unexpected");
1927     addi(entry_offset, entry_offset, type_base - off_base);
1928 
1929     // Profile the parameter.
1930     profile_obj_type(tmp3, mdp, entry_offset, tmp4, tmp3);
1931 
1932     // Go to next parameter.
1933     int delta = TypeStackSlotEntries::per_arg_count() * DataLayout::cell_size + (type_base - off_base);
1934     cmpdi(CCR0, entry_offset, off_base + delta);
1935     addi(entry_offset, entry_offset, -delta);
1936     bge(CCR0, loop);
1937 
1938     align(32, 12);
1939     bind(profile_continue);
1940   }
1941 }
1942 
1943 // Add a InterpMonitorElem to stack (see frame_sparc.hpp).
1944 void InterpreterMacroAssembler::add_monitor_to_stack(bool stack_is_empty, Register Rtemp1, Register Rtemp2) {
1945 
1946   // Very-local scratch registers.
1947   const Register esp  = Rtemp1;
1948   const Register slot = Rtemp2;
1949 
1950   // Extracted monitor_size.
1951   int monitor_size = frame::interpreter_frame_monitor_size_in_bytes();
1952   assert(Assembler::is_aligned((unsigned int)monitor_size,
1953                                (unsigned int)frame::alignment_in_bytes),
1954          "size of a monitor must respect alignment of SP");
1955 
1956   resize_frame(-monitor_size, /*temp*/esp); // Allocate space for new monitor
1957   std(R1_SP, _ijava_state_neg(top_frame_sp), esp); // esp contains fp
1958 
1959   // Shuffle expression stack down. Recall that stack_base points
1960   // just above the new expression stack bottom. Old_tos and new_tos
1961   // are used to scan thru the old and new expression stacks.
1962   if (!stack_is_empty) {
1963     Label copy_slot, copy_slot_finished;
1964     const Register n_slots = slot;
1965 
1966     addi(esp, R15_esp, Interpreter::stackElementSize); // Point to first element (pre-pushed stack).
1967     subf(n_slots, esp, R26_monitor);
1968     srdi_(n_slots, n_slots, LogBytesPerWord);          // Compute number of slots to copy.
1969     assert(LogBytesPerWord == 3, "conflicts assembler instructions");
1970     beq(CCR0, copy_slot_finished);                     // Nothing to copy.
1971 
1972     mtctr(n_slots);
1973 
1974     // loop
1975     bind(copy_slot);
1976     ld(slot, 0, esp);              // Move expression stack down.
1977     std(slot, -monitor_size, esp); // distance = monitor_size
1978     addi(esp, esp, BytesPerWord);
1979     bdnz(copy_slot);
1980 
1981     bind(copy_slot_finished);
1982   }
1983 
1984   addi(R15_esp, R15_esp, -monitor_size);
1985   addi(R26_monitor, R26_monitor, -monitor_size);
1986 
1987   // Restart interpreter
1988 }
1989 
1990 // ============================================================================
1991 // Java locals access
1992 
1993 // Load a local variable at index in Rindex into register Rdst_value.
1994 // Also puts address of local into Rdst_address as a service.
1995 // Kills:
1996 //   - Rdst_value
1997 //   - Rdst_address
1998 void InterpreterMacroAssembler::load_local_int(Register Rdst_value, Register Rdst_address, Register Rindex) {
1999   sldi(Rdst_address, Rindex, Interpreter::logStackElementSize);
2000   subf(Rdst_address, Rdst_address, R18_locals);
2001   lwz(Rdst_value, 0, Rdst_address);
2002 }
2003 
2004 // Load a local variable at index in Rindex into register Rdst_value.
2005 // Also puts address of local into Rdst_address as a service.
2006 // Kills:
2007 //   - Rdst_value
2008 //   - Rdst_address
2009 void InterpreterMacroAssembler::load_local_long(Register Rdst_value, Register Rdst_address, Register Rindex) {
2010   sldi(Rdst_address, Rindex, Interpreter::logStackElementSize);
2011   subf(Rdst_address, Rdst_address, R18_locals);
2012   ld(Rdst_value, -8, Rdst_address);
2013 }
2014 
2015 // Load a local variable at index in Rindex into register Rdst_value.
2016 // Also puts address of local into Rdst_address as a service.
2017 // Input:
2018 //   - Rindex:      slot nr of local variable
2019 // Kills:
2020 //   - Rdst_value
2021 //   - Rdst_address
2022 void InterpreterMacroAssembler::load_local_ptr(Register Rdst_value,
2023                                                Register Rdst_address,
2024                                                Register Rindex) {
2025   sldi(Rdst_address, Rindex, Interpreter::logStackElementSize);
2026   subf(Rdst_address, Rdst_address, R18_locals);
2027   ld(Rdst_value, 0, Rdst_address);
2028 }
2029 
2030 // Load a local variable at index in Rindex into register Rdst_value.
2031 // Also puts address of local into Rdst_address as a service.
2032 // Kills:
2033 //   - Rdst_value
2034 //   - Rdst_address
2035 void InterpreterMacroAssembler::load_local_float(FloatRegister Rdst_value,
2036                                                  Register Rdst_address,
2037                                                  Register Rindex) {
2038   sldi(Rdst_address, Rindex, Interpreter::logStackElementSize);
2039   subf(Rdst_address, Rdst_address, R18_locals);
2040   lfs(Rdst_value, 0, Rdst_address);
2041 }
2042 
2043 // Load a local variable at index in Rindex into register Rdst_value.
2044 // Also puts address of local into Rdst_address as a service.
2045 // Kills:
2046 //   - Rdst_value
2047 //   - Rdst_address
2048 void InterpreterMacroAssembler::load_local_double(FloatRegister Rdst_value,
2049                                                   Register Rdst_address,
2050                                                   Register Rindex) {
2051   sldi(Rdst_address, Rindex, Interpreter::logStackElementSize);
2052   subf(Rdst_address, Rdst_address, R18_locals);
2053   lfd(Rdst_value, -8, Rdst_address);
2054 }
2055 
2056 // Store an int value at local variable slot Rindex.
2057 // Kills:
2058 //   - Rindex
2059 void InterpreterMacroAssembler::store_local_int(Register Rvalue, Register Rindex) {
2060   sldi(Rindex, Rindex, Interpreter::logStackElementSize);
2061   subf(Rindex, Rindex, R18_locals);
2062   stw(Rvalue, 0, Rindex);
2063 }
2064 
2065 // Store a long value at local variable slot Rindex.
2066 // Kills:
2067 //   - Rindex
2068 void InterpreterMacroAssembler::store_local_long(Register Rvalue, Register Rindex) {
2069   sldi(Rindex, Rindex, Interpreter::logStackElementSize);
2070   subf(Rindex, Rindex, R18_locals);
2071   std(Rvalue, -8, Rindex);
2072 }
2073 
2074 // Store an oop value at local variable slot Rindex.
2075 // Kills:
2076 //   - Rindex
2077 void InterpreterMacroAssembler::store_local_ptr(Register Rvalue, Register Rindex) {
2078   sldi(Rindex, Rindex, Interpreter::logStackElementSize);
2079   subf(Rindex, Rindex, R18_locals);
2080   std(Rvalue, 0, Rindex);
2081 }
2082 
2083 // Store an int value at local variable slot Rindex.
2084 // Kills:
2085 //   - Rindex
2086 void InterpreterMacroAssembler::store_local_float(FloatRegister Rvalue, Register Rindex) {
2087   sldi(Rindex, Rindex, Interpreter::logStackElementSize);
2088   subf(Rindex, Rindex, R18_locals);
2089   stfs(Rvalue, 0, Rindex);
2090 }
2091 
2092 // Store an int value at local variable slot Rindex.
2093 // Kills:
2094 //   - Rindex
2095 void InterpreterMacroAssembler::store_local_double(FloatRegister Rvalue, Register Rindex) {
2096   sldi(Rindex, Rindex, Interpreter::logStackElementSize);
2097   subf(Rindex, Rindex, R18_locals);
2098   stfd(Rvalue, -8, Rindex);
2099 }
2100 
2101 // Read pending exception from thread and jump to interpreter.
2102 // Throw exception entry if one if pending. Fall through otherwise.
2103 void InterpreterMacroAssembler::check_and_forward_exception(Register Rscratch1, Register Rscratch2) {
2104   assert_different_registers(Rscratch1, Rscratch2, R3);
2105   Register Rexception = Rscratch1;
2106   Register Rtmp       = Rscratch2;
2107   Label Ldone;
2108   // Get pending exception oop.
2109   ld(Rexception, thread_(pending_exception));
2110   cmpdi(CCR0, Rexception, 0);
2111   beq(CCR0, Ldone);
2112   li(Rtmp, 0);
2113   mr_if_needed(R3, Rexception);
2114   std(Rtmp, thread_(pending_exception)); // Clear exception in thread
2115   if (Interpreter::rethrow_exception_entry() != NULL) {
2116     // Already got entry address.
2117     load_dispatch_table(Rtmp, (address*)Interpreter::rethrow_exception_entry());
2118   } else {
2119     // Dynamically load entry address.
2120     int simm16_rest = load_const_optimized(Rtmp, &amp;Interpreter::_rethrow_exception_entry, R0, true);
2121     ld(Rtmp, simm16_rest, Rtmp);
2122   }
2123   mtctr(Rtmp);
2124   save_interpreter_state(Rtmp);
2125   bctr();
2126 
2127   align(32, 12);
2128   bind(Ldone);
2129 }
2130 
2131 void InterpreterMacroAssembler::call_VM(Register oop_result, address entry_point, bool check_exceptions) {
2132   save_interpreter_state(R11_scratch1);
2133 
2134   MacroAssembler::call_VM(oop_result, entry_point, false);
2135 
2136   restore_interpreter_state(R11_scratch1, /*bcp_and_mdx_only*/ true);
2137 
2138   check_and_handle_popframe(R11_scratch1);
2139   check_and_handle_earlyret(R11_scratch1);
2140   // Now check exceptions manually.
2141   if (check_exceptions) {
2142     check_and_forward_exception(R11_scratch1, R12_scratch2);
2143   }
2144 }
2145 
2146 void InterpreterMacroAssembler::call_VM(Register oop_result, address entry_point,
2147                                         Register arg_1, bool check_exceptions) {
2148   // ARG1 is reserved for the thread.
2149   mr_if_needed(R4_ARG2, arg_1);
2150   call_VM(oop_result, entry_point, check_exceptions);
2151 }
2152 
2153 void InterpreterMacroAssembler::call_VM(Register oop_result, address entry_point,
2154                                         Register arg_1, Register arg_2,
2155                                         bool check_exceptions) {
2156   // ARG1 is reserved for the thread.
2157   mr_if_needed(R4_ARG2, arg_1);
2158   assert(arg_2 != R4_ARG2, "smashed argument");
2159   mr_if_needed(R5_ARG3, arg_2);
2160   call_VM(oop_result, entry_point, check_exceptions);
2161 }
2162 
2163 void InterpreterMacroAssembler::call_VM(Register oop_result, address entry_point,
2164                                         Register arg_1, Register arg_2, Register arg_3,
2165                                         bool check_exceptions) {
2166   // ARG1 is reserved for the thread.
2167   mr_if_needed(R4_ARG2, arg_1);
2168   assert(arg_2 != R4_ARG2, "smashed argument");
2169   mr_if_needed(R5_ARG3, arg_2);
2170   assert(arg_3 != R4_ARG2 &amp;&amp; arg_3 != R5_ARG3, "smashed argument");
2171   mr_if_needed(R6_ARG4, arg_3);
2172   call_VM(oop_result, entry_point, check_exceptions);
2173 }
2174 
2175 void InterpreterMacroAssembler::save_interpreter_state(Register scratch) {
2176   ld(scratch, 0, R1_SP);
2177   std(R15_esp, _ijava_state_neg(esp), scratch);
2178   std(R14_bcp, _ijava_state_neg(bcp), scratch);
2179   std(R26_monitor, _ijava_state_neg(monitors), scratch);
2180   if (ProfileInterpreter) { std(R28_mdx, _ijava_state_neg(mdx), scratch); }
2181   // Other entries should be unchanged.
2182 }
2183 
2184 void InterpreterMacroAssembler::restore_interpreter_state(Register scratch, bool bcp_and_mdx_only) {
2185   ld(scratch, 0, R1_SP);
2186   ld(R14_bcp, _ijava_state_neg(bcp), scratch); // Changed by VM code (exception).
2187   if (ProfileInterpreter) { ld(R28_mdx, _ijava_state_neg(mdx), scratch); } // Changed by VM code.
2188   if (!bcp_and_mdx_only) {
2189     // Following ones are Metadata.
2190     ld(R19_method, _ijava_state_neg(method), scratch);
2191     ld(R27_constPoolCache, _ijava_state_neg(cpoolCache), scratch);
2192     // Following ones are stack addresses and don't require reload.
2193     ld(R15_esp, _ijava_state_neg(esp), scratch);
2194     ld(R18_locals, _ijava_state_neg(locals), scratch);
2195     ld(R26_monitor, _ijava_state_neg(monitors), scratch);
2196   }
2197 #ifdef ASSERT
2198   {
2199     Label Lok;
2200     subf(R0, R1_SP, scratch);
2201     cmpdi(CCR0, R0, frame::abi_reg_args_size + frame::ijava_state_size);
2202     bge(CCR0, Lok);
2203     stop("frame too small (restore istate)", 0x5432);
2204     bind(Lok);
2205   }
2206   {
2207     Label Lok;
2208     ld(R0, _ijava_state_neg(ijava_reserved), scratch);
2209     cmpdi(CCR0, R0, 0x5afe);
2210     beq(CCR0, Lok);
2211     stop("frame corrupted (restore istate)", 0x5afe);
2212     bind(Lok);
2213   }
2214 #endif
2215 }
2216 
2217 void InterpreterMacroAssembler::get_method_counters(Register method,
2218                                                     Register Rcounters,
2219                                                     Label&amp; skip) {
2220   BLOCK_COMMENT("Load and ev. allocate counter object {");
2221   Label has_counters;
2222   ld(Rcounters, in_bytes(Method::method_counters_offset()), method);
2223   cmpdi(CCR0, Rcounters, 0);
2224   bne(CCR0, has_counters);
2225   call_VM(noreg, CAST_FROM_FN_PTR(address,
2226                                   InterpreterRuntime::build_method_counters), method, false);
2227   ld(Rcounters, in_bytes(Method::method_counters_offset()), method);
2228   cmpdi(CCR0, Rcounters, 0);
2229   beq(CCR0, skip); // No MethodCounters, OutOfMemory.
2230   BLOCK_COMMENT("} Load and ev. allocate counter object");
2231 
2232   bind(has_counters);
2233 }
2234 
2235 void InterpreterMacroAssembler::increment_invocation_counter(Register Rcounters,
2236                                                              Register iv_be_count,
2237                                                              Register Rtmp_r0) {
2238   assert(UseCompiler || LogTouchedMethods, "incrementing must be useful");
2239   Register invocation_count = iv_be_count;
2240   Register backedge_count   = Rtmp_r0;
2241   int delta = InvocationCounter::count_increment;
2242 
2243   // Load each counter in a register.
2244   //  ld(inv_counter, Rtmp);
2245   //  ld(be_counter, Rtmp2);
2246   int inv_counter_offset = in_bytes(MethodCounters::invocation_counter_offset() +
2247                                     InvocationCounter::counter_offset());
2248   int be_counter_offset  = in_bytes(MethodCounters::backedge_counter_offset() +
2249                                     InvocationCounter::counter_offset());
2250 
2251   BLOCK_COMMENT("Increment profiling counters {");
2252 
2253   // Load the backedge counter.
2254   lwz(backedge_count, be_counter_offset, Rcounters); // is unsigned int
2255   // Mask the backedge counter.
2256   andi(backedge_count, backedge_count, InvocationCounter::count_mask_value);
2257 
2258   // Load the invocation counter.
2259   lwz(invocation_count, inv_counter_offset, Rcounters); // is unsigned int
2260   // Add the delta to the invocation counter and store the result.
2261   addi(invocation_count, invocation_count, delta);
2262   // Store value.
2263   stw(invocation_count, inv_counter_offset, Rcounters);
2264 
2265   // Add invocation counter + backedge counter.
2266   add(iv_be_count, backedge_count, invocation_count);
2267 
2268   // Note that this macro must leave the backedge_count + invocation_count in
2269   // register iv_be_count!
2270   BLOCK_COMMENT("} Increment profiling counters");
2271 }
2272 
2273 void InterpreterMacroAssembler::verify_oop(Register reg, TosState state) {
2274   if (state == atos) { MacroAssembler::verify_oop(reg); }
2275 }
2276 
2277 // Local helper function for the verify_oop_or_return_address macro.
2278 static bool verify_return_address(Method* m, int bci) {
2279 #ifndef PRODUCT
2280   address pc = (address)(m-&gt;constMethod()) + in_bytes(ConstMethod::codes_offset()) + bci;
2281   // Assume it is a valid return address if it is inside m and is preceded by a jsr.
2282   if (!m-&gt;contains(pc))                                            return false;
2283   address jsr_pc;
2284   jsr_pc = pc - Bytecodes::length_for(Bytecodes::_jsr);
2285   if (*jsr_pc == Bytecodes::_jsr   &amp;&amp; jsr_pc &gt;= m-&gt;code_base())    return true;
2286   jsr_pc = pc - Bytecodes::length_for(Bytecodes::_jsr_w);
2287   if (*jsr_pc == Bytecodes::_jsr_w &amp;&amp; jsr_pc &gt;= m-&gt;code_base())    return true;
2288 #endif // PRODUCT
2289   return false;
2290 }
2291 
2292 void InterpreterMacroAssembler::verify_FPU(int stack_depth, TosState state) {
2293   if (VerifyFPU) {
2294     unimplemented("verfiyFPU");
2295   }
2296 }
2297 
2298 void InterpreterMacroAssembler::verify_oop_or_return_address(Register reg, Register Rtmp) {
2299   if (!VerifyOops) return;
2300 
2301   // The VM documentation for the astore[_wide] bytecode allows
2302   // the TOS to be not only an oop but also a return address.
2303   Label test;
2304   Label skip;
2305   // See if it is an address (in the current method):
2306 
2307   const int log2_bytecode_size_limit = 16;
2308   srdi_(Rtmp, reg, log2_bytecode_size_limit);
2309   bne(CCR0, test);
2310 
2311   address fd = CAST_FROM_FN_PTR(address, verify_return_address);
2312   const int nbytes_save = MacroAssembler::num_volatile_regs * 8;
2313   save_volatile_gprs(R1_SP, -nbytes_save); // except R0
2314   save_LR_CR(Rtmp); // Save in old frame.
2315   push_frame_reg_args(nbytes_save, Rtmp);
2316 
2317   load_const_optimized(Rtmp, fd, R0);
2318   mr_if_needed(R4_ARG2, reg);
2319   mr(R3_ARG1, R19_method);
2320   call_c(Rtmp); // call C
2321 
2322   pop_frame();
2323   restore_LR_CR(Rtmp);
2324   restore_volatile_gprs(R1_SP, -nbytes_save); // except R0
2325   b(skip);
2326 
2327   // Perform a more elaborate out-of-line call.
2328   // Not an address; verify it:
2329   bind(test);
2330   verify_oop(reg);
2331   bind(skip);
2332 }
2333 
2334 // Inline assembly for:
2335 //
2336 // if (thread is in interp_only_mode) {
2337 //   InterpreterRuntime::post_method_entry();
2338 // }
2339 // if (*jvmpi::event_flags_array_at_addr(JVMPI_EVENT_METHOD_ENTRY ) ||
2340 //     *jvmpi::event_flags_array_at_addr(JVMPI_EVENT_METHOD_ENTRY2)   ) {
2341 //   SharedRuntime::jvmpi_method_entry(method, receiver);
2342 // }
2343 void InterpreterMacroAssembler::notify_method_entry() {
2344   // JVMTI
2345   // Whenever JVMTI puts a thread in interp_only_mode, method
2346   // entry/exit events are sent for that thread to track stack
2347   // depth. If it is possible to enter interp_only_mode we add
2348   // the code to check if the event should be sent.
2349   if (JvmtiExport::can_post_interpreter_events()) {
2350     Label jvmti_post_done;
2351 
2352     lwz(R0, in_bytes(JavaThread::interp_only_mode_offset()), R16_thread);
2353     cmpwi(CCR0, R0, 0);
2354     beq(CCR0, jvmti_post_done);
2355     call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_method_entry),
2356             /*check_exceptions=*/true);
2357 
2358     bind(jvmti_post_done);
2359   }
2360 }
2361 
2362 // Inline assembly for:
2363 //
2364 // if (thread is in interp_only_mode) {
2365 //   // save result
2366 //   InterpreterRuntime::post_method_exit();
2367 //   // restore result
2368 // }
2369 // if (*jvmpi::event_flags_array_at_addr(JVMPI_EVENT_METHOD_EXIT)) {
2370 //   // save result
2371 //   SharedRuntime::jvmpi_method_exit();
2372 //   // restore result
2373 // }
2374 //
2375 // Native methods have their result stored in d_tmp and l_tmp.
2376 // Java methods have their result stored in the expression stack.
2377 void InterpreterMacroAssembler::notify_method_exit(bool is_native_method, TosState state,
2378                                                    NotifyMethodExitMode mode, bool check_exceptions) {
2379   // JVMTI
2380   // Whenever JVMTI puts a thread in interp_only_mode, method
2381   // entry/exit events are sent for that thread to track stack
2382   // depth. If it is possible to enter interp_only_mode we add
2383   // the code to check if the event should be sent.
2384   if (mode == NotifyJVMTI &amp;&amp; JvmtiExport::can_post_interpreter_events()) {
2385     Label jvmti_post_done;
2386 
2387     lwz(R0, in_bytes(JavaThread::interp_only_mode_offset()), R16_thread);
2388     cmpwi(CCR0, R0, 0);
2389     beq(CCR0, jvmti_post_done);
2390     if (!is_native_method) { push(state); } // Expose tos to GC.
2391     call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_method_exit),
2392             /*check_exceptions=*/check_exceptions);
2393     if (!is_native_method) { pop(state); }
2394 
2395     align(32, 12);
2396     bind(jvmti_post_done);
2397   }
2398 
2399   // Dtrace support not implemented.
2400 }
2401 
</pre></body></html>
