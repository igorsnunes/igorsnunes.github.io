<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/share/vm/oops/oop.inline.hpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2016, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #ifndef SHARE_VM_OOPS_OOP_INLINE_HPP
  26 #define SHARE_VM_OOPS_OOP_INLINE_HPP
  27 
  28 #include "gc/shared/ageTable.hpp"
  29 #include "gc/shared/barrierSet.inline.hpp"
  30 #include "gc/shared/cardTableModRefBS.hpp"
  31 #include "gc/shared/collectedHeap.inline.hpp"
  32 #include "gc/shared/genCollectedHeap.hpp"
  33 #include "gc/shared/generation.hpp"
  34 #include "oops/arrayKlass.hpp"
  35 #include "oops/arrayOop.hpp"
  36 #include "oops/klass.inline.hpp"
  37 #include "oops/markOop.inline.hpp"
  38 #include "oops/oop.hpp"
  39 #include "runtime/atomic.hpp"
  40 #include "runtime/orderAccess.inline.hpp"
  41 #include "runtime/os.hpp"
  42 #include "utilities/macros.hpp"
  43 
  44 inline void update_barrier_set(void* p, oop v, bool release = false) {
  45   assert(oopDesc::bs() != NULL, "Uninitialized bs in oop!");
  46   oopDesc::bs()-&gt;write_ref_field(p, v, release);
  47 }
  48 
  49 template &lt;class T&gt; inline void update_barrier_set_pre(T* p, oop v) {
  50   oopDesc::bs()-&gt;write_ref_field_pre(p, v);
  51 }
  52 
  53 template &lt;class T&gt; void oop_store(T* p, oop v) {
  54   if (always_do_update_barrier) {
  55     oop_store((volatile T*)p, v);
  56   } else {
  57     update_barrier_set_pre(p, v);
  58     oopDesc::encode_store_heap_oop(p, v);
  59     // always_do_update_barrier == false =&gt;
  60     // Either we are at a safepoint (in GC) or CMS is not used. In both
  61     // cases it's unnecessary to mark the card as dirty with release sematics.
  62     update_barrier_set((void*)p, v, false /* release */);  // cast away type
  63   }
  64 }
  65 
  66 template &lt;class T&gt; void oop_store(volatile T* p, oop v) {
  67   update_barrier_set_pre((T*)p, v);   // cast away volatile
  68   // Used by release_obj_field_put, so use release_store_ptr.
  69   oopDesc::release_encode_store_heap_oop(p, v);
  70   // When using CMS we must mark the card corresponding to p as dirty
  71   // with release sematics to prevent that CMS sees the dirty card but
  72   // not the new value v at p due to reordering of the two
  73   // stores. Note that CMS has a concurrent precleaning phase, where
  74   // it reads the card table while the Java threads are running.
  75   update_barrier_set((void*)p, v, true /* release */);    // cast away type
  76 }
  77 
  78 // Should replace *addr = oop assignments where addr type depends on UseCompressedOops
  79 // (without having to remember the function name this calls).
  80 inline void oop_store_raw(HeapWord* addr, oop value) {
  81   if (UseCompressedOops) {
  82     oopDesc::encode_store_heap_oop((narrowOop*)addr, value);
  83   } else {
  84     oopDesc::encode_store_heap_oop((oop*)addr, value);
  85   }
  86 }
  87 
  88 // Implementation of all inlined member functions defined in oop.hpp
  89 // We need a separate file to avoid circular references
  90 
  91 void oopDesc::release_set_mark(markOop m) {
  92   OrderAccess::release_store_ptr(&amp;_mark, m);
  93 }
  94 
  95 markOop oopDesc::cas_set_mark(markOop new_mark, markOop old_mark) {
  96   return (markOop) Atomic::cmpxchg_ptr(new_mark, &amp;_mark, old_mark);
  97 }
  98 
  99 void oopDesc::init_mark() {
 100   set_mark(markOopDesc::prototype_for_object(this));
 101 }
 102 
 103 Klass* oopDesc::klass() const {
 104   if (UseCompressedClassPointers) {
 105     return Klass::decode_klass_not_null(_metadata._compressed_klass);
 106   } else {
 107     return _metadata._klass;
 108   }
 109 }
 110 
 111 Klass* oopDesc::klass_or_null() const volatile {
 112   if (UseCompressedClassPointers) {
 113     return Klass::decode_klass(_metadata._compressed_klass);
 114   } else {
 115     return _metadata._klass;
 116   }
 117 }
 118 
 119 Klass* oopDesc::klass_or_null_acquire() const volatile {
 120   if (UseCompressedClassPointers) {
 121     // Workaround for non-const load_acquire parameter.
 122     const volatile narrowKlass* addr = &amp;_metadata._compressed_klass;
 123     volatile narrowKlass* xaddr = const_cast&lt;volatile narrowKlass*&gt;(addr);
 124     return Klass::decode_klass(OrderAccess::load_acquire(xaddr));
 125   } else {
 126     return (Klass*)OrderAccess::load_ptr_acquire(&amp;_metadata._klass);
 127   }
 128 }
 129 
 130 Klass** oopDesc::klass_addr() {
 131   // Only used internally and with CMS and will not work with
 132   // UseCompressedOops
 133   assert(!UseCompressedClassPointers, "only supported with uncompressed klass pointers");
 134   return (Klass**) &amp;_metadata._klass;
 135 }
 136 
 137 narrowKlass* oopDesc::compressed_klass_addr() {
 138   assert(UseCompressedClassPointers, "only called by compressed klass pointers");
 139   return &amp;_metadata._compressed_klass;
 140 }
 141 
 142 #define CHECK_SET_KLASS(k)                                                \
 143   do {                                                                    \
 144     assert(Universe::is_bootstrapping() || k != NULL, "NULL Klass");      \
 145     assert(Universe::is_bootstrapping() || k-&gt;is_klass(), "not a Klass"); \
 146   } while (0)
 147 
 148 void oopDesc::set_klass(Klass* k) {
 149   CHECK_SET_KLASS(k);
 150   if (UseCompressedClassPointers) {
 151     *compressed_klass_addr() = Klass::encode_klass_not_null(k);
 152   } else {
 153     *klass_addr() = k;
 154   }
 155 }
 156 
 157 void oopDesc::release_set_klass(Klass* k) {
 158   CHECK_SET_KLASS(k);
 159   if (UseCompressedClassPointers) {
 160     OrderAccess::release_store(compressed_klass_addr(),
 161                                Klass::encode_klass_not_null(k));
 162   } else {
 163     OrderAccess::release_store_ptr(klass_addr(), k);
 164   }
 165 }
 166 
 167 #undef CHECK_SET_KLASS
 168 
 169 int oopDesc::klass_gap() const {
 170   return *(int*)(((intptr_t)this) + klass_gap_offset_in_bytes());
 171 }
 172 
 173 void oopDesc::set_klass_gap(int v) {
 174   if (UseCompressedClassPointers) {
 175     *(int*)(((intptr_t)this) + klass_gap_offset_in_bytes()) = v;
 176   }
 177 }
 178 
 179 void oopDesc::set_klass_to_list_ptr(oop k) {
 180   // This is only to be used during GC, for from-space objects, so no
 181   // barrier is needed.
 182   if (UseCompressedClassPointers) {
 183     _metadata._compressed_klass = (narrowKlass)encode_heap_oop(k);  // may be null (parnew overflow handling)
 184   } else {
 185     _metadata._klass = (Klass*)(address)k;
 186   }
 187 }
 188 
 189 oop oopDesc::list_ptr_from_klass() {
 190   // This is only to be used during GC, for from-space objects.
 191   if (UseCompressedClassPointers) {
 192     return decode_heap_oop((narrowOop)_metadata._compressed_klass);
 193   } else {
 194     // Special case for GC
 195     return (oop)(address)_metadata._klass;
 196   }
 197 }
 198 
 199 bool oopDesc::is_a(Klass* k) const {
 200   return klass()-&gt;is_subtype_of(k);
 201 }
 202 
 203 int oopDesc::size()  {
 204   return size_given_klass(klass());
 205 }
 206 
 207 int oopDesc::size_given_klass(Klass* klass)  {
 208   int lh = klass-&gt;layout_helper();
 209   int s;
 210 
 211   // lh is now a value computed at class initialization that may hint
 212   // at the size.  For instances, this is positive and equal to the
 213   // size.  For arrays, this is negative and provides log2 of the
 214   // array element size.  For other oops, it is zero and thus requires
 215   // a virtual call.
 216   //
 217   // We go to all this trouble because the size computation is at the
 218   // heart of phase 2 of mark-compaction, and called for every object,
 219   // alive or dead.  So the speed here is equal in importance to the
 220   // speed of allocation.
 221 
 222   if (lh &gt; Klass::_lh_neutral_value) {
 223     if (!Klass::layout_helper_needs_slow_path(lh)) {
 224       s = lh &gt;&gt; LogHeapWordSize;  // deliver size scaled by wordSize
 225     } else {
 226       s = klass-&gt;oop_size(this);
 227     }
 228   } else if (lh &lt;= Klass::_lh_neutral_value) {
 229     // The most common case is instances; fall through if so.
 230     if (lh &lt; Klass::_lh_neutral_value) {
 231       // Second most common case is arrays.  We have to fetch the
 232       // length of the array, shift (multiply) it appropriately,
 233       // up to wordSize, add the header, and align to object size.
 234       size_t size_in_bytes;
 235 #ifdef _M_IA64
 236       // The Windows Itanium Aug 2002 SDK hoists this load above
 237       // the check for s &lt; 0.  An oop at the end of the heap will
 238       // cause an access violation if this load is performed on a non
 239       // array oop.  Making the reference volatile prohibits this.
 240       // (%%% please explain by what magic the length is actually fetched!)
 241       volatile int *array_length;
 242       array_length = (volatile int *)( (intptr_t)this +
 243                           arrayOopDesc::length_offset_in_bytes() );
 244       assert(array_length &gt; 0, "Integer arithmetic problem somewhere");
 245       // Put into size_t to avoid overflow.
 246       size_in_bytes = (size_t) array_length;
 247       size_in_bytes = size_in_bytes &lt;&lt; Klass::layout_helper_log2_element_size(lh);
 248 #else
 249       size_t array_length = (size_t) ((arrayOop)this)-&gt;length();
 250       size_in_bytes = array_length &lt;&lt; Klass::layout_helper_log2_element_size(lh);
 251 #endif
 252       size_in_bytes += Klass::layout_helper_header_size(lh);
 253 
 254       // This code could be simplified, but by keeping array_header_in_bytes
 255       // in units of bytes and doing it this way we can round up just once,
 256       // skipping the intermediate round to HeapWordSize.  Cast the result
 257       // of round_to to size_t to guarantee unsigned division == right shift.
 258       s = (int)((size_t)round_to(size_in_bytes, MinObjAlignmentInBytes) /
 259         HeapWordSize);
 260 
 261       // ParNew (used by CMS), UseParallelGC and UseG1GC can change the length field
 262       // of an "old copy" of an object array in the young gen so it indicates
 263       // the grey portion of an already copied array. This will cause the first
 264       // disjunct below to fail if the two comparands are computed across such
 265       // a concurrent change.
 266       // ParNew also runs with promotion labs (which look like int
 267       // filler arrays) which are subject to changing their declared size
 268       // when finally retiring a PLAB; this also can cause the first disjunct
 269       // to fail for another worker thread that is concurrently walking the block
 270       // offset table. Both these invariant failures are benign for their
 271       // current uses; we relax the assertion checking to cover these two cases below:
 272       //     is_objArray() &amp;&amp; is_forwarded()   // covers first scenario above
 273       //  || is_typeArray()                    // covers second scenario above
 274       // If and when UseParallelGC uses the same obj array oop stealing/chunking
 275       // technique, we will need to suitably modify the assertion.
 276       assert((s == klass-&gt;oop_size(this)) ||
 277              (Universe::heap()-&gt;is_gc_active() &amp;&amp;
 278               ((is_typeArray() &amp;&amp; UseConcMarkSweepGC) ||
 279                (is_objArray()  &amp;&amp; is_forwarded() &amp;&amp; (UseConcMarkSweepGC || UseParallelGC || UseG1GC)))),
 280              "wrong array object size");
 281     } else {
 282       // Must be zero, so bite the bullet and take the virtual call.
 283       s = klass-&gt;oop_size(this);
 284     }
 285   }
 286 
 287   assert(s % MinObjAlignment == 0, "Oop size is not properly aligned: %d", s);
 288   assert(s &gt; 0, "Oop size must be greater than zero, not %d", s);
 289   return s;
 290 }
 291 
 292 bool oopDesc::is_instance()  const { return klass()-&gt;is_instance_klass();  }
 293 bool oopDesc::is_array()     const { return klass()-&gt;is_array_klass();     }
 294 bool oopDesc::is_objArray()  const { return klass()-&gt;is_objArray_klass();  }
 295 bool oopDesc::is_typeArray() const { return klass()-&gt;is_typeArray_klass(); }
 296 
 297 void*      oopDesc::field_base(int offset)          const { return (void*)&amp;((char*)this)[offset]; }
 298 
 299 jbyte*     oopDesc::byte_field_addr(int offset)     const { return (jbyte*)    field_base(offset); }
 300 jchar*     oopDesc::char_field_addr(int offset)     const { return (jchar*)    field_base(offset); }
 301 jboolean*  oopDesc::bool_field_addr(int offset)     const { return (jboolean*) field_base(offset); }
 302 jint*      oopDesc::int_field_addr(int offset)      const { return (jint*)     field_base(offset); }
 303 jshort*    oopDesc::short_field_addr(int offset)    const { return (jshort*)   field_base(offset); }
 304 jlong*     oopDesc::long_field_addr(int offset)     const { return (jlong*)    field_base(offset); }
 305 jfloat*    oopDesc::float_field_addr(int offset)    const { return (jfloat*)   field_base(offset); }
 306 jdouble*   oopDesc::double_field_addr(int offset)   const { return (jdouble*)  field_base(offset); }
 307 Metadata** oopDesc::metadata_field_addr(int offset) const { return (Metadata**)field_base(offset); }
 308 
 309 template &lt;class T&gt; T* oopDesc::obj_field_addr(int offset) const { return (T*)  field_base(offset); }
 310 address*   oopDesc::address_field_addr(int offset)  const { return (address*)  field_base(offset); }
 311 
 312 
 313 // Functions for getting and setting oops within instance objects.
 314 // If the oops are compressed, the type passed to these overloaded functions
 315 // is narrowOop.  All functions are overloaded so they can be called by
 316 // template functions without conditionals (the compiler instantiates via
 317 // the right type and inlines the appopriate code).
 318 
 319 // Algorithm for encoding and decoding oops from 64 bit pointers to 32 bit
 320 // offset from the heap base.  Saving the check for null can save instructions
 321 // in inner GC loops so these are separated.
 322 
 323 inline bool check_obj_alignment(oop obj) {
 324   return (cast_from_oop&lt;intptr_t&gt;(obj) &amp; MinObjAlignmentInBytesMask) == 0;
 325 }
 326 
 327 oop oopDesc::decode_heap_oop_not_null(narrowOop v) {
 328   assert(!is_null(v), "narrow oop value can never be zero");
 329   address base = Universe::narrow_oop_base();
 330   int    shift = Universe::narrow_oop_shift();
 331   oop result = (oop)(void*)((uintptr_t)base + ((uintptr_t)v &lt;&lt; shift));
 332   assert(check_obj_alignment(result), "address not aligned: " INTPTR_FORMAT, p2i((void*) result));
 333   return result;
 334 }
 335 
 336 oop oopDesc::decode_heap_oop(narrowOop v) {
 337   return is_null(v) ? (oop)NULL : decode_heap_oop_not_null(v);
 338 }
 339 
 340 narrowOop oopDesc::encode_heap_oop_not_null(oop v) {
 341   assert(!is_null(v), "oop value can never be zero");
 342   assert(check_obj_alignment(v), "Address not aligned");
 343   assert(Universe::heap()-&gt;is_in_reserved(v), "Address not in heap");
 344   address base = Universe::narrow_oop_base();
 345   int    shift = Universe::narrow_oop_shift();
 346   uint64_t  pd = (uint64_t)(pointer_delta((void*)v, (void*)base, 1));
 347   assert(OopEncodingHeapMax &gt; pd, "change encoding max if new encoding");
 348   uint64_t result = pd &gt;&gt; shift;
 349   assert((result &amp; CONST64(0xffffffff00000000)) == 0, "narrow oop overflow");
 350   assert(decode_heap_oop(result) == v, "reversibility");
 351   return (narrowOop)result;
 352 }
 353 
 354 narrowOop oopDesc::encode_heap_oop(oop v) {
 355   return (is_null(v)) ? (narrowOop)0 : encode_heap_oop_not_null(v);
 356 }
 357 
 358 // Load and decode an oop out of the Java heap into a wide oop.
 359 oop oopDesc::load_decode_heap_oop_not_null(narrowOop* p) {
 360   return decode_heap_oop_not_null(*p);
 361 }
 362 
 363 // Load and decode an oop out of the heap accepting null
 364 oop oopDesc::load_decode_heap_oop(narrowOop* p) {
 365   return decode_heap_oop(*p);
 366 }
 367 
 368 // Encode and store a heap oop.
 369 void oopDesc::encode_store_heap_oop_not_null(narrowOop* p, oop v) {
 370   *p = encode_heap_oop_not_null(v);
 371 }
 372 
 373 // Encode and store a heap oop allowing for null.
 374 void oopDesc::encode_store_heap_oop(narrowOop* p, oop v) {
 375   *p = encode_heap_oop(v);
 376 }
 377 
 378 // Store heap oop as is for volatile fields.
 379 void oopDesc::release_store_heap_oop(volatile oop* p, oop v) {
 380   OrderAccess::release_store_ptr(p, v);
 381 }
 382 void oopDesc::release_store_heap_oop(volatile narrowOop* p, narrowOop v) {
 383   OrderAccess::release_store(p, v);
 384 }
 385 
 386 void oopDesc::release_encode_store_heap_oop_not_null(volatile narrowOop* p, oop v) {
 387   // heap oop is not pointer sized.
 388   OrderAccess::release_store(p, encode_heap_oop_not_null(v));
 389 }
 390 void oopDesc::release_encode_store_heap_oop_not_null(volatile oop* p, oop v) {
 391   OrderAccess::release_store_ptr(p, v);
 392 }
 393 
 394 void oopDesc::release_encode_store_heap_oop(volatile oop* p, oop v) {
 395   OrderAccess::release_store_ptr(p, v);
 396 }
 397 void oopDesc::release_encode_store_heap_oop(volatile narrowOop* p, oop v) {
 398   OrderAccess::release_store(p, encode_heap_oop(v));
 399 }
 400 
 401 // These functions are only used to exchange oop fields in instances,
 402 // not headers.
 403 oop oopDesc::atomic_exchange_oop(oop exchange_value, volatile HeapWord *dest) {
 404   if (UseCompressedOops) {
 405     // encode exchange value from oop to T
 406     narrowOop val = encode_heap_oop(exchange_value);
 407     narrowOop old = (narrowOop)Atomic::xchg(val, (narrowOop*)dest);
 408     // decode old from T to oop
 409     return decode_heap_oop(old);
 410   } else {
 411     return (oop)Atomic::xchg_ptr(exchange_value, (oop*)dest);
 412   }
 413 }
 414 
 415 oop oopDesc::atomic_compare_exchange_oop(oop exchange_value,
 416                                          volatile HeapWord *dest,
 417                                          oop compare_value,
 418                                          bool prebarrier) {
 419   if (UseCompressedOops) {
 420     if (prebarrier) {
 421       update_barrier_set_pre((narrowOop*)dest, exchange_value);
 422     }
 423     // encode exchange and compare value from oop to T
 424     narrowOop val = encode_heap_oop(exchange_value);
 425     narrowOop cmp = encode_heap_oop(compare_value);
 426 
 427     narrowOop old = (narrowOop) Atomic::cmpxchg(val, (narrowOop*)dest, cmp);
 428     // decode old from T to oop
 429     return decode_heap_oop(old);
 430   } else {
 431     if (prebarrier) {
 432       update_barrier_set_pre((oop*)dest, exchange_value);
 433     }
 434     return (oop)Atomic::cmpxchg_ptr(exchange_value, (oop*)dest, compare_value);
 435   }
 436 }
 437 
 438 // In order to put or get a field out of an instance, must first check
 439 // if the field has been compressed and uncompress it.
 440 oop oopDesc::obj_field(int offset) const {
 441   return UseCompressedOops ?
 442     load_decode_heap_oop(obj_field_addr&lt;narrowOop&gt;(offset)) :
 443     load_decode_heap_oop(obj_field_addr&lt;oop&gt;(offset));
 444 }
 445 
 446 void oopDesc::obj_field_put(int offset, oop value) {
 447   UseCompressedOops ? oop_store(obj_field_addr&lt;narrowOop&gt;(offset), value) :
 448                       oop_store(obj_field_addr&lt;oop&gt;(offset),       value);
 449 }
 450 
 451 void oopDesc::obj_field_put_raw(int offset, oop value) {
 452   UseCompressedOops ?
 453     encode_store_heap_oop(obj_field_addr&lt;narrowOop&gt;(offset), value) :
 454     encode_store_heap_oop(obj_field_addr&lt;oop&gt;(offset),       value);
 455 }
 456 void oopDesc::obj_field_put_volatile(int offset, oop value) {
 457   OrderAccess::release();
 458   obj_field_put(offset, value);
 459   OrderAccess::fence();
 460 }
 461 
 462 Metadata* oopDesc::metadata_field(int offset) const           { return *metadata_field_addr(offset);   }
 463 void oopDesc::metadata_field_put(int offset, Metadata* value) { *metadata_field_addr(offset) = value;  }
 464 
 465 jbyte oopDesc::byte_field(int offset) const                   { return (jbyte) *byte_field_addr(offset);    }
 466 void oopDesc::byte_field_put(int offset, jbyte contents)      { *byte_field_addr(offset) = (jint) contents; }
 467 
 468 jchar oopDesc::char_field(int offset) const                   { return (jchar) *char_field_addr(offset);    }
 469 void oopDesc::char_field_put(int offset, jchar contents)      { *char_field_addr(offset) = (jint) contents; }
 470 
 471 jboolean oopDesc::bool_field(int offset) const                { return (jboolean) *bool_field_addr(offset); }
 472 void oopDesc::bool_field_put(int offset, jboolean contents)   { *bool_field_addr(offset) = (((jint) contents) &amp; 1); }
 473 
 474 jint oopDesc::int_field(int offset) const                     { return *int_field_addr(offset);        }
 475 void oopDesc::int_field_put(int offset, jint contents)        { *int_field_addr(offset) = contents;    }
 476 
 477 jshort oopDesc::short_field(int offset) const                 { return (jshort) *short_field_addr(offset);  }
 478 void oopDesc::short_field_put(int offset, jshort contents)    { *short_field_addr(offset) = (jint) contents;}
 479 
 480 jlong oopDesc::long_field(int offset) const                   { return *long_field_addr(offset);       }
 481 void oopDesc::long_field_put(int offset, jlong contents)      { *long_field_addr(offset) = contents;   }
 482 
 483 jfloat oopDesc::float_field(int offset) const                 { return *float_field_addr(offset);      }
 484 void oopDesc::float_field_put(int offset, jfloat contents)    { *float_field_addr(offset) = contents;  }
 485 
 486 jdouble oopDesc::double_field(int offset) const               { return *double_field_addr(offset);     }
 487 void oopDesc::double_field_put(int offset, jdouble contents)  { *double_field_addr(offset) = contents; }
 488 
 489 address oopDesc::address_field(int offset) const              { return *address_field_addr(offset);     }
 490 void oopDesc::address_field_put(int offset, address contents) { *address_field_addr(offset) = contents; }
 491 
 492 oop oopDesc::obj_field_acquire(int offset) const {
 493   return UseCompressedOops ?
 494              decode_heap_oop((narrowOop)
 495                OrderAccess::load_acquire(obj_field_addr&lt;narrowOop&gt;(offset)))
 496            : decode_heap_oop((oop)
 497                OrderAccess::load_ptr_acquire(obj_field_addr&lt;oop&gt;(offset)));
 498 }
 499 void oopDesc::release_obj_field_put(int offset, oop value) {
 500   UseCompressedOops ?
 501     oop_store((volatile narrowOop*)obj_field_addr&lt;narrowOop&gt;(offset), value) :
 502     oop_store((volatile oop*)      obj_field_addr&lt;oop&gt;(offset),       value);
 503 }
 504 
 505 jbyte oopDesc::byte_field_acquire(int offset) const                   { return OrderAccess::load_acquire(byte_field_addr(offset));     }
 506 void oopDesc::release_byte_field_put(int offset, jbyte contents)      { OrderAccess::release_store(byte_field_addr(offset), contents); }
 507 
 508 jchar oopDesc::char_field_acquire(int offset) const                   { return OrderAccess::load_acquire(char_field_addr(offset));     }
 509 void oopDesc::release_char_field_put(int offset, jchar contents)      { OrderAccess::release_store(char_field_addr(offset), contents); }
 510 
 511 jboolean oopDesc::bool_field_acquire(int offset) const                { return OrderAccess::load_acquire(bool_field_addr(offset));     }
 512 void oopDesc::release_bool_field_put(int offset, jboolean contents)   { OrderAccess::release_store(bool_field_addr(offset), (contents &amp; 1)); }
 513 
 514 jint oopDesc::int_field_acquire(int offset) const                     { return OrderAccess::load_acquire(int_field_addr(offset));      }
 515 void oopDesc::release_int_field_put(int offset, jint contents)        { OrderAccess::release_store(int_field_addr(offset), contents);  }
 516 
 517 jshort oopDesc::short_field_acquire(int offset) const                 { return (jshort)OrderAccess::load_acquire(short_field_addr(offset)); }
 518 void oopDesc::release_short_field_put(int offset, jshort contents)    { OrderAccess::release_store(short_field_addr(offset), contents);     }
 519 
 520 jlong oopDesc::long_field_acquire(int offset) const                   { return OrderAccess::load_acquire(long_field_addr(offset));       }
 521 void oopDesc::release_long_field_put(int offset, jlong contents)      { OrderAccess::release_store(long_field_addr(offset), contents);   }
 522 
 523 jfloat oopDesc::float_field_acquire(int offset) const                 { return OrderAccess::load_acquire(float_field_addr(offset));      }
 524 void oopDesc::release_float_field_put(int offset, jfloat contents)    { OrderAccess::release_store(float_field_addr(offset), contents);  }
 525 
 526 jdouble oopDesc::double_field_acquire(int offset) const               { return OrderAccess::load_acquire(double_field_addr(offset));     }
 527 void oopDesc::release_double_field_put(int offset, jdouble contents)  { OrderAccess::release_store(double_field_addr(offset), contents); }
 528 
 529 address oopDesc::address_field_acquire(int offset) const              { return (address) OrderAccess::load_ptr_acquire(address_field_addr(offset)); }
 530 void oopDesc::release_address_field_put(int offset, address contents) { OrderAccess::release_store_ptr(address_field_addr(offset), contents); }
 531 
 532 bool oopDesc::is_locked() const {
 533   return mark()-&gt;is_locked();
 534 }
 535 
 536 bool oopDesc::is_unlocked() const {
 537   return mark()-&gt;is_unlocked();
 538 }
 539 
 540 bool oopDesc::has_bias_pattern() const {
 541   return mark()-&gt;has_bias_pattern();
 542 }
 543 
 544 // used only for asserts
 545 bool oopDesc::is_oop(bool ignore_mark_word) const {
 546   oop obj = (oop) this;
 547   if (!check_obj_alignment(obj)) return false;
 548   if (!Universe::heap()-&gt;is_in_reserved(obj)) return false;
 549   // obj is aligned and accessible in heap
 550   if (Universe::heap()-&gt;is_in_reserved(obj-&gt;klass_or_null())) return false;
 551 
 552   // Header verification: the mark is typically non-NULL. If we're
 553   // at a safepoint, it must not be null.
 554   // Outside of a safepoint, the header could be changing (for example,
 555   // another thread could be inflating a lock on this object).
 556   if (ignore_mark_word) {
 557     return true;
 558   }
 559   if (mark() != NULL) {
 560     return true;
 561   }
 562   return !SafepointSynchronize::is_at_safepoint();
 563 }
 564 
 565 
 566 // used only for asserts
 567 bool oopDesc::is_oop_or_null(bool ignore_mark_word) const {
 568   return this == NULL ? true : is_oop(ignore_mark_word);
 569 }
 570 
 571 #ifndef PRODUCT
 572 // used only for asserts
 573 bool oopDesc::is_unlocked_oop() const {
 574   if (!Universe::heap()-&gt;is_in_reserved(this)) return false;
 575   return mark()-&gt;is_unlocked();
 576 }
 577 #endif // PRODUCT
 578 
 579 // Used only for markSweep, scavenging
 580 bool oopDesc::is_gc_marked() const {
 581   return mark()-&gt;is_marked();
 582 }
 583 
 584 bool oopDesc::is_scavengable() const {
 585   return Universe::heap()-&gt;is_scavengable(this);
 586 }
 587 
 588 // Used by scavengers
 589 bool oopDesc::is_forwarded() const {
 590   // The extra heap check is needed since the obj might be locked, in which case the
 591   // mark would point to a stack location and have the sentinel bit cleared
 592   return mark()-&gt;is_marked();
 593 }
 594 
 595 // Used by scavengers
 596 void oopDesc::forward_to(oop p) {
 597   assert(check_obj_alignment(p),
 598          "forwarding to something not aligned");
 599   assert(Universe::heap()-&gt;is_in_reserved(p),
 600          "forwarding to something not in heap");
 601   markOop m = markOopDesc::encode_pointer_as_mark(p);
 602   assert(m-&gt;decode_pointer() == p, "encoding must be reversable");
 603   set_mark(m);
 604 }
 605 
 606 // Used by parallel scavengers
 607 bool oopDesc::cas_forward_to(oop p, markOop compare) {
 608   assert(check_obj_alignment(p),
 609          "forwarding to something not aligned");
 610   assert(Universe::heap()-&gt;is_in_reserved(p),
 611          "forwarding to something not in heap");
 612   markOop m = markOopDesc::encode_pointer_as_mark(p);
 613   assert(m-&gt;decode_pointer() == p, "encoding must be reversable");
 614   return cas_set_mark(m, compare) == compare;
 615 }
 616 
 617 #if INCLUDE_ALL_GCS
 618 oop oopDesc::forward_to_atomic(oop p) {
 619   markOop oldMark = mark();
 620   markOop forwardPtrMark = markOopDesc::encode_pointer_as_mark(p);
 621   markOop curMark;
 622 
 623   assert(forwardPtrMark-&gt;decode_pointer() == p, "encoding must be reversable");
 624   assert(sizeof(markOop) == sizeof(intptr_t), "CAS below requires this.");
 625 
 626   while (!oldMark-&gt;is_marked()) {
 627     curMark = (markOop)Atomic::cmpxchg_ptr(forwardPtrMark, &amp;_mark, oldMark);
 628     assert(is_forwarded(), "object should have been forwarded");
 629     if (curMark == oldMark) {
 630       return NULL;
 631     }
 632     // If the CAS was unsuccessful then curMark-&gt;is_marked()
 633     // should return true as another thread has CAS'd in another
 634     // forwarding pointer.
 635     oldMark = curMark;
 636   }
 637   return forwardee();
 638 }
 639 #endif
 640 
 641 // Note that the forwardee is not the same thing as the displaced_mark.
 642 // The forwardee is used when copying during scavenge and mark-sweep.
 643 // It does need to clear the low two locking- and GC-related bits.
 644 oop oopDesc::forwardee() const {
 645   return (oop) mark()-&gt;decode_pointer();
 646 }
 647 
 648 // The following method needs to be MT safe.
 649 uint oopDesc::age() const {
 650   assert(!is_forwarded(), "Attempt to read age from forwarded mark");
 651   if (has_displaced_mark()) {
 652     return displaced_mark()-&gt;age();
 653   } else {
 654     return mark()-&gt;age();
 655   }
 656 }
 657 
 658 void oopDesc::incr_age() {
 659   assert(!is_forwarded(), "Attempt to increment age of forwarded mark");
 660   if (has_displaced_mark()) {
 661     set_displaced_mark(displaced_mark()-&gt;incr_age());
 662   } else {
 663     set_mark(mark()-&gt;incr_age());
 664   }
 665 }
 666 
 667 int oopDesc::ms_adjust_pointers() {
 668   debug_only(int check_size = size());
 669   int s = klass()-&gt;oop_ms_adjust_pointers(this);
 670   assert(s == check_size, "should be the same");
 671   return s;
 672 }
 673 
 674 #if INCLUDE_ALL_GCS
 675 void oopDesc::pc_follow_contents(ParCompactionManager* cm) {
 676   klass()-&gt;oop_pc_follow_contents(this, cm);
 677 }
 678 
 679 void oopDesc::pc_update_contents(ParCompactionManager* cm) {
 680   Klass* k = klass();
 681   if (!k-&gt;is_typeArray_klass()) {
 682     // It might contain oops beyond the header, so take the virtual call.
 683     k-&gt;oop_pc_update_pointers(this, cm);
 684   }
 685   // Else skip it.  The TypeArrayKlass in the header never needs scavenging.
 686 }
 687 
 688 void oopDesc::ps_push_contents(PSPromotionManager* pm) {
 689   Klass* k = klass();
 690   if (!k-&gt;is_typeArray_klass()) {
 691     // It might contain oops beyond the header, so take the virtual call.
 692     k-&gt;oop_ps_push_contents(this, pm);
 693   }
 694   // Else skip it.  The TypeArrayKlass in the header never needs scavenging.
 695 }
 696 #endif // INCLUDE_ALL_GCS
 697 
 698 #define OOP_ITERATE_DEFN(OopClosureType, nv_suffix)                 \
 699                                                                     \
 700 void oopDesc::oop_iterate(OopClosureType* blk) {                    \
 701   klass()-&gt;oop_oop_iterate##nv_suffix(this, blk);                   \
 702 }                                                                   \
 703                                                                     \
 704 void oopDesc::oop_iterate(OopClosureType* blk, MemRegion mr) {      \
 705   klass()-&gt;oop_oop_iterate_bounded##nv_suffix(this, blk, mr);       \
 706 }
 707 
 708 #define OOP_ITERATE_SIZE_DEFN(OopClosureType, nv_suffix)            \
 709                                                                     \
 710 int oopDesc::oop_iterate_size(OopClosureType* blk) {                \
 711   Klass* k = klass();                                               \
 712   int size = size_given_klass(k);                                   \
 713   k-&gt;oop_oop_iterate##nv_suffix(this, blk);                         \
 714   return size;                                                      \
 715 }                                                                   \
 716                                                                     \
 717 int oopDesc::oop_iterate_size(OopClosureType* blk, MemRegion mr) {  \
 718   Klass* k = klass();                                               \
 719   int size = size_given_klass(k);                                   \
 720   k-&gt;oop_oop_iterate_bounded##nv_suffix(this, blk, mr);             \
 721   return size;                                                      \
 722 }
 723 
 724 int oopDesc::oop_iterate_no_header(OopClosure* blk) {
 725   // The NoHeaderExtendedOopClosure wraps the OopClosure and proxies all
 726   // the do_oop calls, but turns off all other features in ExtendedOopClosure.
 727   NoHeaderExtendedOopClosure cl(blk);
 728   return oop_iterate_size(&amp;cl);
 729 }
 730 
 731 int oopDesc::oop_iterate_no_header(OopClosure* blk, MemRegion mr) {
 732   NoHeaderExtendedOopClosure cl(blk);
 733   return oop_iterate_size(&amp;cl, mr);
 734 }
 735 
 736 #if INCLUDE_ALL_GCS
 737 #define OOP_ITERATE_BACKWARDS_DEFN(OopClosureType, nv_suffix)       \
 738                                                                     \
 739 inline void oopDesc::oop_iterate_backwards(OopClosureType* blk) {   \
 740   klass()-&gt;oop_oop_iterate_backwards##nv_suffix(this, blk);         \
 741 }
 742 #else
 743 #define OOP_ITERATE_BACKWARDS_DEFN(OopClosureType, nv_suffix)
 744 #endif // INCLUDE_ALL_GCS
 745 
 746 #define ALL_OOPDESC_OOP_ITERATE(OopClosureType, nv_suffix)  \
 747   OOP_ITERATE_DEFN(OopClosureType, nv_suffix)               \
 748   OOP_ITERATE_SIZE_DEFN(OopClosureType, nv_suffix)          \
 749   OOP_ITERATE_BACKWARDS_DEFN(OopClosureType, nv_suffix)
 750 
 751 ALL_OOP_OOP_ITERATE_CLOSURES_1(ALL_OOPDESC_OOP_ITERATE)
 752 ALL_OOP_OOP_ITERATE_CLOSURES_2(ALL_OOPDESC_OOP_ITERATE)
 753 
 754 intptr_t oopDesc::identity_hash() {
 755   // Fast case; if the object is unlocked and the hash value is set, no locking is needed
 756   // Note: The mark must be read into local variable to avoid concurrent updates.
 757   markOop mrk = mark();
 758   if (mrk-&gt;is_unlocked() &amp;&amp; !mrk-&gt;has_no_hash()) {
 759     return mrk-&gt;hash();
 760   } else if (mrk-&gt;is_marked()) {
 761     return mrk-&gt;hash();
 762   } else {
 763     return slow_identity_hash();
 764   }
 765 }
 766 
 767 bool oopDesc::has_displaced_mark() const {
 768   return mark()-&gt;has_displaced_mark_helper();
 769 }
 770 
 771 markOop oopDesc::displaced_mark() const {
 772   return mark()-&gt;displaced_mark_helper();
 773 }
 774 
 775 void oopDesc::set_displaced_mark(markOop m) {
 776   mark()-&gt;set_displaced_mark_helper(m);
 777 }
 778 
 779 #endif // SHARE_VM_OOPS_OOP_INLINE_HPP
</pre></body></html>
