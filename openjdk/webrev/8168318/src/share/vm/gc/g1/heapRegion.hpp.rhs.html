<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre></pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 2001, 2016, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #ifndef SHARE_VM_GC_G1_HEAPREGION_HPP
  26 #define SHARE_VM_GC_G1_HEAPREGION_HPP
  27 
  28 #include "gc/g1/g1AllocationContext.hpp"
  29 #include "gc/g1/g1BlockOffsetTable.hpp"
  30 #include "gc/g1/g1HeapRegionTraceType.hpp"
  31 #include "gc/g1/heapRegionTracer.hpp"
  32 #include "gc/g1/heapRegionType.hpp"
  33 #include "gc/g1/survRateGroup.hpp"
  34 #include "gc/shared/ageTable.hpp"
  35 #include "gc/shared/spaceDecorator.hpp"
  36 #include "utilities/macros.hpp"
  37 
  38 // A HeapRegion is the smallest piece of a G1CollectedHeap that
  39 // can be collected independently.
  40 
  41 // NOTE: Although a HeapRegion is a Space, its
  42 // Space::initDirtyCardClosure method must not be called.
  43 // The problem is that the existence of this method breaks
  44 // the independence of barrier sets from remembered sets.
  45 // The solution is to remove this method from the definition
  46 // of a Space.
  47 
  48 // Each heap region is self contained. top() and end() can never
  49 // be set beyond the end of the region. For humongous objects,
  50 // the first region is a StartsHumongous region. If the humongous
  51 // object is larger than a heap region, the following regions will
  52 // be of type ContinuesHumongous. In this case the top() of the
  53 // StartHumongous region and all ContinuesHumongous regions except
  54 // the last will point to their own end. For the last ContinuesHumongous
  55 // region, top() will equal the object's top.
  56 
  57 class G1CollectedHeap;
  58 class HeapRegionRemSet;
  59 class HeapRegionRemSetIterator;
  60 class HeapRegion;
  61 class HeapRegionSetBase;
  62 class nmethod;
  63 
  64 #define HR_FORMAT "%u:(%s)[" PTR_FORMAT "," PTR_FORMAT "," PTR_FORMAT "]"
  65 #define HR_FORMAT_PARAMS(_hr_) \
  66                 (_hr_)-&gt;hrm_index(), \
  67                 (_hr_)-&gt;get_short_type_str(), \
  68                 p2i((_hr_)-&gt;bottom()), p2i((_hr_)-&gt;top()), p2i((_hr_)-&gt;end())
  69 
  70 // sentinel value for hrm_index
  71 #define G1_NO_HRM_INDEX ((uint) -1)
  72 
  73 // A dirty card to oop closure for heap regions. It
  74 // knows how to get the G1 heap and how to use the bitmap
  75 // in the concurrent marker used by G1 to filter remembered
  76 // sets.
  77 
  78 class HeapRegionDCTOC : public DirtyCardToOopClosure {
  79 private:
  80   HeapRegion* _hr;
  81   G1ParPushHeapRSClosure* _rs_scan;
  82   G1CollectedHeap* _g1;
  83 
  84   // Walk the given memory region from bottom to (actual) top
  85   // looking for objects and applying the oop closure (_cl) to
  86   // them. The base implementation of this treats the area as
  87   // blocks, where a block may or may not be an object. Sub-
  88   // classes should override this to provide more accurate
  89   // or possibly more efficient walking.
  90   void walk_mem_region(MemRegion mr, HeapWord* bottom, HeapWord* top);
  91 
  92 public:
  93   HeapRegionDCTOC(G1CollectedHeap* g1,
  94                   HeapRegion* hr,
  95                   G1ParPushHeapRSClosure* cl,
  96                   CardTableModRefBS::PrecisionStyle precision);
  97 };
  98 
  99 // The complicating factor is that BlockOffsetTable diverged
 100 // significantly, and we need functionality that is only in the G1 version.
 101 // So I copied that code, which led to an alternate G1 version of
 102 // OffsetTableContigSpace.  If the two versions of BlockOffsetTable could
 103 // be reconciled, then G1OffsetTableContigSpace could go away.
 104 
 105 // The idea behind time stamps is the following. We want to keep track of
 106 // the highest address where it's safe to scan objects for each region.
 107 // This is only relevant for current GC alloc regions so we keep a time stamp
 108 // per region to determine if the region has been allocated during the current
 109 // GC or not. If the time stamp is current we report a scan_top value which
 110 // was saved at the end of the previous GC for retained alloc regions and which is
 111 // equal to the bottom for all other regions.
 112 // There is a race between card scanners and allocating gc workers where we must ensure
 113 // that card scanners do not read the memory allocated by the gc workers.
 114 // In order to enforce that, we must not return a value of _top which is more recent than the
 115 // time stamp. This is due to the fact that a region may become a gc alloc region at
 116 // some point after we've read the timestamp value as being &lt; the current time stamp.
 117 // The time stamps are re-initialized to zero at cleanup and at Full GCs.
 118 // The current scheme that uses sequential unsigned ints will fail only if we have 4b
 119 // evacuation pauses between two cleanups, which is _highly_ unlikely.
 120 class G1ContiguousSpace: public CompactibleSpace {
 121   friend class VMStructs;
 122   HeapWord* volatile _top;
 123   HeapWord* volatile _scan_top;
 124  protected:
 125   G1BlockOffsetTablePart _bot_part;
 126   Mutex _par_alloc_lock;
 127   volatile uint _gc_time_stamp;
 128   // When we need to retire an allocation region, while other threads
 129   // are also concurrently trying to allocate into it, we typically
 130   // allocate a dummy object at the end of the region to ensure that
 131   // no more allocations can take place in it. However, sometimes we
 132   // want to know where the end of the last "real" object we allocated
 133   // into the region was and this is what this keeps track.
 134   HeapWord* _pre_dummy_top;
 135 
 136  public:
 137   G1ContiguousSpace(G1BlockOffsetTable* bot);
 138 
 139   void set_top(HeapWord* value) { _top = value; }
 140   HeapWord* top() const { return _top; }
 141 
 142  protected:
 143   // Reset the G1ContiguousSpace.
 144   virtual void initialize(MemRegion mr, bool clear_space, bool mangle_space);
 145 
 146   HeapWord* volatile* top_addr() { return &amp;_top; }
 147   // Try to allocate at least min_word_size and up to desired_size from this Space.
 148   // Returns NULL if not possible, otherwise sets actual_word_size to the amount of
 149   // space allocated.
 150   // This version assumes that all allocation requests to this Space are properly
 151   // synchronized.
 152   inline HeapWord* allocate_impl(size_t min_word_size, size_t desired_word_size, size_t* actual_word_size);
 153   // Try to allocate at least min_word_size and up to desired_size from this Space.
 154   // Returns NULL if not possible, otherwise sets actual_word_size to the amount of
 155   // space allocated.
 156   // This version synchronizes with other calls to par_allocate_impl().
 157   inline HeapWord* par_allocate_impl(size_t min_word_size, size_t desired_word_size, size_t* actual_word_size);
 158 
 159  public:
 160   void reset_after_compaction() { set_top(compaction_top()); }
 161 
 162   size_t used() const { return byte_size(bottom(), top()); }
 163   size_t free() const { return byte_size(top(), end()); }
 164   bool is_free_block(const HeapWord* p) const { return p &gt;= top(); }
 165 
 166   MemRegion used_region() const { return MemRegion(bottom(), top()); }
 167 
 168   void object_iterate(ObjectClosure* blk);
 169   void safe_object_iterate(ObjectClosure* blk);
 170 
 171   void mangle_unused_area() PRODUCT_RETURN;
 172   void mangle_unused_area_complete() PRODUCT_RETURN;
 173 
 174   HeapWord* scan_top() const;
 175   void record_timestamp();
 176   void reset_gc_time_stamp() { _gc_time_stamp = 0; }
 177   uint get_gc_time_stamp() { return _gc_time_stamp; }
 178   void record_retained_region();
 179 
 180   // See the comment above in the declaration of _pre_dummy_top for an
 181   // explanation of what it is.
 182   void set_pre_dummy_top(HeapWord* pre_dummy_top) {
 183     assert(is_in(pre_dummy_top) &amp;&amp; pre_dummy_top &lt;= top(), "pre-condition");
 184     _pre_dummy_top = pre_dummy_top;
 185   }
 186   HeapWord* pre_dummy_top() {
 187     return (_pre_dummy_top == NULL) ? top() : _pre_dummy_top;
 188   }
 189   void reset_pre_dummy_top() { _pre_dummy_top = NULL; }
 190 
 191   virtual void clear(bool mangle_space);
 192 
 193   HeapWord* block_start(const void* p);
 194   HeapWord* block_start_const(const void* p) const;
 195 
 196   // Allocation (return NULL if full).  Assumes the caller has established
 197   // mutually exclusive access to the space.
 198   HeapWord* allocate(size_t min_word_size, size_t desired_word_size, size_t* actual_word_size);
 199   // Allocation (return NULL if full).  Enforces mutual exclusion internally.
 200   HeapWord* par_allocate(size_t min_word_size, size_t desired_word_size, size_t* actual_word_size);
 201 
 202   virtual HeapWord* allocate(size_t word_size);
 203   virtual HeapWord* par_allocate(size_t word_size);
 204 
 205   HeapWord* saved_mark_word() const { ShouldNotReachHere(); return NULL; }
 206 
 207   // MarkSweep support phase3
 208   virtual HeapWord* initialize_threshold();
 209   virtual HeapWord* cross_threshold(HeapWord* start, HeapWord* end);
 210 
 211   virtual void print() const;
 212 
 213   void reset_bot() {
 214     _bot_part.reset_bot();
 215   }
 216 
 217   void print_bot_on(outputStream* out) {
 218     _bot_part.print_on(out);
 219   }
 220 };
 221 
 222 class HeapRegion: public G1ContiguousSpace {
 223   friend class VMStructs;
 224   // Allow scan_and_forward to call (private) overrides for auxiliary functions on this class
 225   template &lt;typename SpaceType&gt;
 226   friend void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp);
 227  private:
 228 
 229   // The remembered set for this region.
 230   // (Might want to make this "inline" later, to avoid some alloc failure
 231   // issues.)
 232   HeapRegionRemSet* _rem_set;
 233 
 234   // Auxiliary functions for scan_and_forward support.
 235   // See comments for CompactibleSpace for more information.
 236   inline HeapWord* scan_limit() const {
 237     return top();
 238   }
 239 
 240   inline bool scanned_block_is_obj(const HeapWord* addr) const {
 241     return true; // Always true, since scan_limit is top
 242   }
 243 
 244   inline size_t scanned_block_size(const HeapWord* addr) const {
 245     return HeapRegion::block_size(addr); // Avoid virtual call
 246   }
 247 
 248   void report_region_type_change(G1HeapRegionTraceType::Type to);
 249 
 250  protected:
 251   // The index of this region in the heap region sequence.
 252   uint  _hrm_index;
 253 
 254   AllocationContext_t _allocation_context;
 255 
 256   HeapRegionType _type;
 257 
 258   // For a humongous region, region in which it starts.
 259   HeapRegion* _humongous_start_region;
 260 
 261   // True iff an attempt to evacuate an object in the region failed.
 262   bool _evacuation_failed;
 263 
 264   // Fields used by the HeapRegionSetBase class and subclasses.
 265   HeapRegion* _next;
 266   HeapRegion* _prev;
 267 #ifdef ASSERT
 268   HeapRegionSetBase* _containing_set;
 269 #endif // ASSERT
 270 
 271   // We use concurrent marking to determine the amount of live data
 272   // in each heap region.
 273   size_t _prev_marked_bytes;    // Bytes known to be live via last completed marking.
 274   size_t _next_marked_bytes;    // Bytes known to be live via in-progress marking.
 275 
 276   // The calculated GC efficiency of the region.
 277   double _gc_efficiency;
 278 
 279   int  _young_index_in_cset;
 280   SurvRateGroup* _surv_rate_group;
 281   int  _age_index;
 282 
 283   // The start of the unmarked area. The unmarked area extends from this
 284   // word until the top and/or end of the region, and is the part
 285   // of the region for which no marking was done, i.e. objects may
 286   // have been allocated in this part since the last mark phase.
 287   // "prev" is the top at the start of the last completed marking.
 288   // "next" is the top at the start of the in-progress marking (if any.)
 289   HeapWord* _prev_top_at_mark_start;
 290   HeapWord* _next_top_at_mark_start;
 291   // If a collection pause is in progress, this is the top at the start
 292   // of that pause.
 293 
 294   void init_top_at_mark_start() {
 295     assert(_prev_marked_bytes == 0 &amp;&amp;
 296            _next_marked_bytes == 0,
 297            "Must be called after zero_marked_bytes.");
 298     HeapWord* bot = bottom();
 299     _prev_top_at_mark_start = bot;
 300     _next_top_at_mark_start = bot;
 301   }
 302 
 303   // Cached attributes used in the collection set policy information
 304 
 305   // The RSet length that was added to the total value
 306   // for the collection set.
 307   size_t _recorded_rs_length;
 308 
 309   // The predicted elapsed time that was added to total value
 310   // for the collection set.
 311   double _predicted_elapsed_time_ms;
 312 
 313   // The predicted number of bytes to copy that was added to
 314   // the total value for the collection set.
 315   size_t _predicted_bytes_to_copy;
 316 
 317  public:
 318   HeapRegion(uint hrm_index,
 319              G1BlockOffsetTable* bot,
 320              MemRegion mr);
 321 
 322   // Initializing the HeapRegion not only resets the data structure, but also
 323   // resets the BOT for that heap region.
 324   // The default values for clear_space means that we will do the clearing if
 325   // there's clearing to be done ourselves. We also always mangle the space.
 326   virtual void initialize(MemRegion mr, bool clear_space = false, bool mangle_space = SpaceDecorator::Mangle);
 327 
 328   static int    LogOfHRGrainBytes;
 329   static int    LogOfHRGrainWords;
 330 
 331   static size_t GrainBytes;
 332   static size_t GrainWords;
 333   static size_t CardsPerRegion;
 334 
 335   static size_t align_up_to_region_byte_size(size_t sz) {
 336     return (sz + (size_t) GrainBytes - 1) &amp;
 337                                       ~((1 &lt;&lt; (size_t) LogOfHRGrainBytes) - 1);
 338   }
 339 
 340 
 341   // Returns whether a field is in the same region as the obj it points to.
 342   template &lt;typename T&gt;
 343   static bool is_in_same_region(T* p, oop obj) {
 344     assert(p != NULL, "p can't be NULL");
 345     assert(obj != NULL, "obj can't be NULL");
 346     return (((uintptr_t) p ^ cast_from_oop&lt;uintptr_t&gt;(obj)) &gt;&gt; LogOfHRGrainBytes) == 0;
 347   }
 348 
 349   static size_t max_region_size();
 350   static size_t min_region_size_in_words();
 351 
 352   // It sets up the heap region size (GrainBytes / GrainWords), as
 353   // well as other related fields that are based on the heap region
 354   // size (LogOfHRGrainBytes / LogOfHRGrainWords /
 355   // CardsPerRegion). All those fields are considered constant
 356   // throughout the JVM's execution, therefore they should only be set
 357   // up once during initialization time.
 358   static void setup_heap_region_size(size_t initial_heap_size, size_t max_heap_size);
 359 
 360   // All allocated blocks are occupied by objects in a HeapRegion
 361   bool block_is_obj(const HeapWord* p) const;
 362 
 363   // Returns the object size for all valid block starts
 364   // and the amount of unallocated words if called on top()
 365   size_t block_size(const HeapWord* p) const;
 366 
 367   // Override for scan_and_forward support.
 368   void prepare_for_compaction(CompactPoint* cp);
 369 
 370   inline HeapWord* par_allocate_no_bot_updates(size_t min_word_size, size_t desired_word_size, size_t* word_size);
 371   inline HeapWord* allocate_no_bot_updates(size_t word_size);
 372   inline HeapWord* allocate_no_bot_updates(size_t min_word_size, size_t desired_word_size, size_t* actual_size);
 373 
 374   // If this region is a member of a HeapRegionManager, the index in that
 375   // sequence, otherwise -1.
 376   uint hrm_index() const { return _hrm_index; }
 377 
 378   // The number of bytes marked live in the region in the last marking phase.
 379   size_t marked_bytes()    { return _prev_marked_bytes; }
 380   size_t live_bytes() {
 381     return (top() - prev_top_at_mark_start()) * HeapWordSize + marked_bytes();
 382   }
 383 
 384   // The number of bytes counted in the next marking.
 385   size_t next_marked_bytes() { return _next_marked_bytes; }
 386   // The number of bytes live wrt the next marking.
 387   size_t next_live_bytes() {
 388     return
 389       (top() - next_top_at_mark_start()) * HeapWordSize + next_marked_bytes();
 390   }
 391 
 392   // A lower bound on the amount of garbage bytes in the region.
 393   size_t garbage_bytes() {
 394     size_t used_at_mark_start_bytes =
 395       (prev_top_at_mark_start() - bottom()) * HeapWordSize;
 396     return used_at_mark_start_bytes - marked_bytes();
 397   }
 398 
 399   // Return the amount of bytes we'll reclaim if we collect this
 400   // region. This includes not only the known garbage bytes in the
 401   // region but also any unallocated space in it, i.e., [top, end),
 402   // since it will also be reclaimed if we collect the region.
 403   size_t reclaimable_bytes() {
 404     size_t known_live_bytes = live_bytes();
 405     assert(known_live_bytes &lt;= capacity(), "sanity");
 406     return capacity() - known_live_bytes;
 407   }
 408 
 409   // An upper bound on the number of live bytes in the region.
 410   size_t max_live_bytes() { return used() - garbage_bytes(); }
 411 
 412   void add_to_marked_bytes(size_t incr_bytes) {
 413     _next_marked_bytes = _next_marked_bytes + incr_bytes;
 414   }
 415 
 416   void zero_marked_bytes()      {
 417     _prev_marked_bytes = _next_marked_bytes = 0;
 418   }
 419 
 420   const char* get_type_str() const { return _type.get_str(); }
 421   const char* get_short_type_str() const { return _type.get_short_str(); }
 422   G1HeapRegionTraceType::Type get_trace_type() { return _type.get_trace_type(); }
 423 
 424   bool is_free() const { return _type.is_free(); }
 425 
 426   bool is_young()    const { return _type.is_young();    }
 427   bool is_eden()     const { return _type.is_eden();     }
 428   bool is_survivor() const { return _type.is_survivor(); }
 429 
 430   bool is_humongous() const { return _type.is_humongous(); }
 431   bool is_starts_humongous() const { return _type.is_starts_humongous(); }
 432   bool is_continues_humongous() const { return _type.is_continues_humongous();   }
 433 
 434   bool is_old() const { return _type.is_old(); }
 435 
 436   // A pinned region contains objects which are not moved by garbage collections.
 437   // Humongous regions and archive regions are pinned.
 438   bool is_pinned() const { return _type.is_pinned(); }
 439 
 440   // An archive region is a pinned region, also tagged as old, which
 441   // should not be marked during mark/sweep. This allows the address
 442   // space to be shared by JVM instances.
 443   bool is_archive() const { return _type.is_archive(); }
 444 
 445   // For a humongous region, region in which it starts.
 446   HeapRegion* humongous_start_region() const {
 447     return _humongous_start_region;
 448   }
 449 
 450   // Makes the current region be a "starts humongous" region, i.e.,
 451   // the first region in a series of one or more contiguous regions
 452   // that will contain a single "humongous" object.
 453   //
 454   // obj_top : points to the top of the humongous object.
 455   // fill_size : size of the filler object at the end of the region series.
 456   void set_starts_humongous(HeapWord* obj_top, size_t fill_size);
 457 
 458   // Makes the current region be a "continues humongous'
 459   // region. first_hr is the "start humongous" region of the series
 460   // which this region will be part of.
 461   void set_continues_humongous(HeapRegion* first_hr);
 462 
 463   // Unsets the humongous-related fields on the region.
 464   void clear_humongous();
 465 
 466   // If the region has a remembered set, return a pointer to it.
 467   HeapRegionRemSet* rem_set() const {
 468     return _rem_set;
 469   }
 470 
 471   inline bool in_collection_set() const;
 472 
 473   void set_allocation_context(AllocationContext_t context) {
 474     _allocation_context = context;
 475   }
 476 
 477   AllocationContext_t  allocation_context() const {
 478     return _allocation_context;
 479   }
 480 
 481   // Methods used by the HeapRegionSetBase class and subclasses.
 482 
 483   // Getter and setter for the next and prev fields used to link regions into
 484   // linked lists.
 485   HeapRegion* next()              { return _next; }
 486   HeapRegion* prev()              { return _prev; }
 487 
 488   void set_next(HeapRegion* next) { _next = next; }
 489   void set_prev(HeapRegion* prev) { _prev = prev; }
 490 
 491   // Every region added to a set is tagged with a reference to that
 492   // set. This is used for doing consistency checking to make sure that
 493   // the contents of a set are as they should be and it's only
 494   // available in non-product builds.
 495 #ifdef ASSERT
 496   void set_containing_set(HeapRegionSetBase* containing_set) {
 497     assert((containing_set == NULL &amp;&amp; _containing_set != NULL) ||
 498            (containing_set != NULL &amp;&amp; _containing_set == NULL),
 499            "containing_set: " PTR_FORMAT " "
 500            "_containing_set: " PTR_FORMAT,
 501            p2i(containing_set), p2i(_containing_set));
 502 
 503     _containing_set = containing_set;
 504   }
 505 
 506   HeapRegionSetBase* containing_set() { return _containing_set; }
 507 #else // ASSERT
 508   void set_containing_set(HeapRegionSetBase* containing_set) { }
 509 
 510   // containing_set() is only used in asserts so there's no reason
 511   // to provide a dummy version of it.
 512 #endif // ASSERT
 513 
 514 
 515   // Reset the HeapRegion to default values.
 516   // If skip_remset is true, do not clear the remembered set.
 517   void hr_clear(bool skip_remset, bool clear_space, bool locked = false);
 518   // Clear the parts skipped by skip_remset in hr_clear() in the HeapRegion during
 519   // a concurrent phase.
 520   void par_clear();
 521 
 522   // Get the start of the unmarked area in this region.
 523   HeapWord* prev_top_at_mark_start() const { return _prev_top_at_mark_start; }
 524   HeapWord* next_top_at_mark_start() const { return _next_top_at_mark_start; }
 525 
 526   // Note the start or end of marking. This tells the heap region
 527   // that the collector is about to start or has finished (concurrently)
 528   // marking the heap.
 529 
 530   // Notify the region that concurrent marking is starting. Initialize
 531   // all fields related to the next marking info.
 532   inline void note_start_of_marking();
 533 
 534   // Notify the region that concurrent marking has finished. Copy the
 535   // (now finalized) next marking info fields into the prev marking
 536   // info fields.
 537   inline void note_end_of_marking();
 538 
 539   // Notify the region that it will be used as to-space during a GC
 540   // and we are about to start copying objects into it.
 541   inline void note_start_of_copying(bool during_initial_mark);
 542 
 543   // Notify the region that it ceases being to-space during a GC and
 544   // we will not copy objects into it any more.
 545   inline void note_end_of_copying(bool during_initial_mark);
 546 
 547   // Notify the region that we are about to start processing
 548   // self-forwarded objects during evac failure handling.
 549   void note_self_forwarding_removal_start(bool during_initial_mark,
 550                                           bool during_conc_mark);
 551 
 552   // Notify the region that we have finished processing self-forwarded
 553   // objects during evac failure handling.
 554   void note_self_forwarding_removal_end(bool during_initial_mark,
 555                                         bool during_conc_mark,
 556                                         size_t marked_bytes);
 557 
 558   // Returns "false" iff no object in the region was allocated when the
 559   // last mark phase ended.
 560   bool is_marked() { return _prev_top_at_mark_start != bottom(); }
 561 
 562   void reset_during_compaction() {
 563     assert(is_humongous(),
 564            "should only be called for humongous regions");
 565 
 566     zero_marked_bytes();
 567     init_top_at_mark_start();
 568   }
 569 
 570   void calc_gc_efficiency(void);
 571   double gc_efficiency() { return _gc_efficiency;}
 572 
 573   int  young_index_in_cset() const { return _young_index_in_cset; }
 574   void set_young_index_in_cset(int index) {
 575     assert( (index == -1) || is_young(), "pre-condition" );
 576     _young_index_in_cset = index;
 577   }
 578 
 579   int age_in_surv_rate_group() {
 580     assert( _surv_rate_group != NULL, "pre-condition" );
 581     assert( _age_index &gt; -1, "pre-condition" );
 582     return _surv_rate_group-&gt;age_in_group(_age_index);
 583   }
 584 
 585   void record_surv_words_in_group(size_t words_survived) {
 586     assert( _surv_rate_group != NULL, "pre-condition" );
 587     assert( _age_index &gt; -1, "pre-condition" );
 588     int age_in_group = age_in_surv_rate_group();
 589     _surv_rate_group-&gt;record_surviving_words(age_in_group, words_survived);
 590   }
 591 
 592   int age_in_surv_rate_group_cond() {
 593     if (_surv_rate_group != NULL)
 594       return age_in_surv_rate_group();
 595     else
 596       return -1;
 597   }
 598 
 599   SurvRateGroup* surv_rate_group() {
 600     return _surv_rate_group;
 601   }
 602 
 603   void install_surv_rate_group(SurvRateGroup* surv_rate_group) {
 604     assert( surv_rate_group != NULL, "pre-condition" );
 605     assert( _surv_rate_group == NULL, "pre-condition" );
 606     assert( is_young(), "pre-condition" );
 607 
 608     _surv_rate_group = surv_rate_group;
 609     _age_index = surv_rate_group-&gt;next_age_index();
 610   }
 611 
 612   void uninstall_surv_rate_group() {
 613     if (_surv_rate_group != NULL) {
 614       assert( _age_index &gt; -1, "pre-condition" );
 615       assert( is_young(), "pre-condition" );
 616 
 617       _surv_rate_group = NULL;
 618       _age_index = -1;
 619     } else {
 620       assert( _age_index == -1, "pre-condition" );
 621     }
 622   }
 623 
 624   void set_free();
 625 
 626   void set_eden();
 627   void set_eden_pre_gc();
 628   void set_survivor();
 629 
 630   void set_old();
 631 
 632   void set_archive();
 633 
 634   // Determine if an object has been allocated since the last
 635   // mark performed by the collector. This returns true iff the object
 636   // is within the unmarked area of the region.
 637   bool obj_allocated_since_prev_marking(oop obj) const {
 638     return (HeapWord *) obj &gt;= prev_top_at_mark_start();
 639   }
 640   bool obj_allocated_since_next_marking(oop obj) const {
 641     return (HeapWord *) obj &gt;= next_top_at_mark_start();
 642   }
 643 
 644   // Returns the "evacuation_failed" property of the region.
 645   bool evacuation_failed() { return _evacuation_failed; }
 646 
 647   // Sets the "evacuation_failed" property of the region.
 648   void set_evacuation_failed(bool b) {
 649     _evacuation_failed = b;
 650 
 651     if (b) {
 652       _next_marked_bytes = 0;
 653     }
 654   }
 655 
<a name="1" id="anc1"></a><span class="changed"> 656   // Iterate over the card in the card designated by card_ptr,</span>
<span class="changed"> 657   // applying cl to all references in the region.</span>
<span class="changed"> 658   // mr: the memory region covered by the card.</span>
<span class="changed"> 659   // card_ptr: if we decide that the card is not young and we iterate</span>
<span class="changed"> 660   // over it, we'll clean the card before we start the iteration.</span>
<span class="changed"> 661   // Returns true if card was successfully processed, false if an</span>
<span class="changed"> 662   // unparsable part of the heap was encountered, which should only</span>
<span class="changed"> 663   // happen when invoked concurrently with the mutator.</span>
<span class="changed"> 664   bool oops_on_card_seq_iterate_careful(MemRegion mr,</span>
 665                                         FilterOutOfRegionClosure* cl,
<a name="2" id="anc2"></a>
 666                                         jbyte* card_ptr);
 667 
 668   size_t recorded_rs_length() const        { return _recorded_rs_length; }
 669   double predicted_elapsed_time_ms() const { return _predicted_elapsed_time_ms; }
 670   size_t predicted_bytes_to_copy() const   { return _predicted_bytes_to_copy; }
 671 
 672   void set_recorded_rs_length(size_t rs_length) {
 673     _recorded_rs_length = rs_length;
 674   }
 675 
 676   void set_predicted_elapsed_time_ms(double ms) {
 677     _predicted_elapsed_time_ms = ms;
 678   }
 679 
 680   void set_predicted_bytes_to_copy(size_t bytes) {
 681     _predicted_bytes_to_copy = bytes;
 682   }
 683 
 684   virtual CompactibleSpace* next_compaction_space() const;
 685 
 686   virtual void reset_after_compaction();
 687 
 688   // Routines for managing a list of code roots (attached to the
 689   // this region's RSet) that point into this heap region.
 690   void add_strong_code_root(nmethod* nm);
 691   void add_strong_code_root_locked(nmethod* nm);
 692   void remove_strong_code_root(nmethod* nm);
 693 
 694   // Applies blk-&gt;do_code_blob() to each of the entries in
 695   // the strong code roots list for this region
 696   void strong_code_roots_do(CodeBlobClosure* blk) const;
 697 
 698   // Verify that the entries on the strong code root list for this
 699   // region are live and include at least one pointer into this region.
 700   void verify_strong_code_roots(VerifyOption vo, bool* failures) const;
 701 
 702   void print() const;
 703   void print_on(outputStream* st) const;
 704 
 705   // vo == UsePrevMarking  -&gt; use "prev" marking information,
 706   // vo == UseNextMarking -&gt; use "next" marking information
 707   // vo == UseMarkWord    -&gt; use the mark word in the object header
 708   //
 709   // NOTE: Only the "prev" marking information is guaranteed to be
 710   // consistent most of the time, so most calls to this should use
 711   // vo == UsePrevMarking.
 712   // Currently, there is only one case where this is called with
 713   // vo == UseNextMarking, which is to verify the "next" marking
 714   // information at the end of remark.
 715   // Currently there is only one place where this is called with
 716   // vo == UseMarkWord, which is to verify the marking during a
 717   // full GC.
 718   void verify(VerifyOption vo, bool *failures) const;
 719 
 720   // Override; it uses the "prev" marking information
 721   virtual void verify() const;
 722 
 723   void verify_rem_set(VerifyOption vo, bool *failures) const;
 724   void verify_rem_set() const;
 725 };
 726 
 727 // HeapRegionClosure is used for iterating over regions.
 728 // Terminates the iteration when the "doHeapRegion" method returns "true".
 729 class HeapRegionClosure : public StackObj {
 730   friend class HeapRegionManager;
 731   friend class G1CollectionSet;
 732 
 733   bool _complete;
 734   void incomplete() { _complete = false; }
 735 
 736  public:
 737   HeapRegionClosure(): _complete(true) {}
 738 
 739   // Typically called on each region until it returns true.
 740   virtual bool doHeapRegion(HeapRegion* r) = 0;
 741 
 742   // True after iteration if the closure was applied to all heap regions
 743   // and returned "false" in all cases.
 744   bool complete() { return _complete; }
 745 };
 746 
 747 #endif // SHARE_VM_GC_G1_HEAPREGION_HPP
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="3" type="hidden" /></form></body></html>
