<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/cpu/ppc/vm/templateInterpreterGenerator_ppc.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2015, 2016 SAP SE. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include "precompiled.hpp"
  27 #include "asm/macroAssembler.inline.hpp"
  28 #include "interpreter/bytecodeHistogram.hpp"
  29 #include "interpreter/interpreter.hpp"
  30 #include "interpreter/interpreterRuntime.hpp"
  31 #include "interpreter/interp_masm.hpp"
  32 #include "interpreter/templateInterpreterGenerator.hpp"
  33 #include "interpreter/templateTable.hpp"
  34 #include "oops/arrayOop.hpp"
  35 #include "oops/methodData.hpp"
  36 #include "oops/method.hpp"
  37 #include "oops/oop.inline.hpp"
  38 #include "prims/jvmtiExport.hpp"
  39 #include "prims/jvmtiThreadState.hpp"
  40 #include "runtime/arguments.hpp"
  41 #include "runtime/deoptimization.hpp"
  42 #include "runtime/frame.inline.hpp"
  43 #include "runtime/sharedRuntime.hpp"
  44 #include "runtime/stubRoutines.hpp"
  45 #include "runtime/synchronizer.hpp"
  46 #include "runtime/timer.hpp"
  47 #include "runtime/vframeArray.hpp"
  48 #include "utilities/debug.hpp"
  49 #include "utilities/macros.hpp"
  50 
  51 #undef __
  52 #define __ _masm-&gt;
  53 
  54 // Size of interpreter code.  Increase if too small.  Interpreter will
  55 // fail with a guarantee ("not enough space for interpreter generation");
  56 // if too small.
  57 // Run with +PrintInterpreter to get the VM to print out the size.
  58 // Max size with JVMTI
  59 int TemplateInterpreter::InterpreterCodeSize = 230*K;
  60 
  61 #ifdef PRODUCT
  62 #define BLOCK_COMMENT(str) /* nothing */
  63 #else
  64 #define BLOCK_COMMENT(str) __ block_comment(str)
  65 #endif
  66 
  67 #define BIND(label)        __ bind(label); BLOCK_COMMENT(#label ":")
  68 
  69 //-----------------------------------------------------------------------------
  70 
  71 address TemplateInterpreterGenerator::generate_slow_signature_handler() {
  72   // Slow_signature handler that respects the PPC C calling conventions.
  73   //
  74   // We get called by the native entry code with our output register
  75   // area == 8. First we call InterpreterRuntime::get_result_handler
  76   // to copy the pointer to the signature string temporarily to the
  77   // first C-argument and to return the result_handler in
  78   // R3_RET. Since native_entry will copy the jni-pointer to the
  79   // first C-argument slot later on, it is OK to occupy this slot
  80   // temporarilly. Then we copy the argument list on the java
  81   // expression stack into native varargs format on the native stack
  82   // and load arguments into argument registers. Integer arguments in
  83   // the varargs vector will be sign-extended to 8 bytes.
  84   //
  85   // On entry:
  86   //   R3_ARG1        - intptr_t*     Address of java argument list in memory.
  87   //   R15_prev_state - BytecodeInterpreter* Address of interpreter state for
  88   //     this method
  89   //   R19_method
  90   //
  91   // On exit (just before return instruction):
  92   //   R3_RET            - contains the address of the result_handler.
  93   //   R4_ARG2           - is not updated for static methods and contains "this" otherwise.
  94   //   R5_ARG3-R10_ARG8: - When the (i-2)th Java argument is not of type float or double,
  95   //                       ARGi contains this argument. Otherwise, ARGi is not updated.
  96   //   F1_ARG1-F13_ARG13 - contain the first 13 arguments of type float or double.
  97 
  98   const int LogSizeOfTwoInstructions = 3;
  99 
 100   // FIXME: use Argument:: GL: Argument names different numbers!
 101   const int max_fp_register_arguments  = 13;
 102   const int max_int_register_arguments = 6;  // first 2 are reserved
 103 
 104   const Register arg_java       = R21_tmp1;
 105   const Register arg_c          = R22_tmp2;
 106   const Register signature      = R23_tmp3;  // is string
 107   const Register sig_byte       = R24_tmp4;
 108   const Register fpcnt          = R25_tmp5;
 109   const Register argcnt         = R26_tmp6;
 110   const Register intSlot        = R27_tmp7;
 111   const Register target_sp      = R28_tmp8;
 112   const FloatRegister floatSlot = F0;
 113 
 114   address entry = __ function_entry();
 115 
 116   __ save_LR_CR(R0);
 117   __ save_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));
 118   // We use target_sp for storing arguments in the C frame.
 119   __ mr(target_sp, R1_SP);
 120   __ push_frame_reg_args_nonvolatiles(0, R11_scratch1);
 121 
 122   __ mr(arg_java, R3_ARG1);
 123 
 124   __ call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::get_signature), R16_thread, R19_method);
 125 
 126   // Signature is in R3_RET. Signature is callee saved.
 127   __ mr(signature, R3_RET);
 128 
 129   // Get the result handler.
 130   __ call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::get_result_handler), R16_thread, R19_method);
 131 
 132   {
 133     Label L;
 134     // test if static
 135     // _access_flags._flags must be at offset 0.
 136     // TODO PPC port: requires change in shared code.
 137     //assert(in_bytes(AccessFlags::flags_offset()) == 0,
 138     //       "MethodDesc._access_flags == MethodDesc._access_flags._flags");
 139     // _access_flags must be a 32 bit value.
 140     assert(sizeof(AccessFlags) == 4, "wrong size");
 141     __ lwa(R11_scratch1/*access_flags*/, method_(access_flags));
 142     // testbit with condition register.
 143     __ testbitdi(CCR0, R0, R11_scratch1/*access_flags*/, JVM_ACC_STATIC_BIT);
 144     __ btrue(CCR0, L);
 145     // For non-static functions, pass "this" in R4_ARG2 and copy it
 146     // to 2nd C-arg slot.
 147     // We need to box the Java object here, so we use arg_java
 148     // (address of current Java stack slot) as argument and don't
 149     // dereference it as in case of ints, floats, etc.
 150     __ mr(R4_ARG2, arg_java);
 151     __ addi(arg_java, arg_java, -BytesPerWord);
 152     __ std(R4_ARG2, _abi(carg_2), target_sp);
 153     __ bind(L);
 154   }
 155 
 156   // Will be incremented directly after loop_start. argcnt=0
 157   // corresponds to 3rd C argument.
 158   __ li(argcnt, -1);
 159   // arg_c points to 3rd C argument
 160   __ addi(arg_c, target_sp, _abi(carg_3));
 161   // no floating-point args parsed so far
 162   __ li(fpcnt, 0);
 163 
 164   Label move_intSlot_to_ARG, move_floatSlot_to_FARG;
 165   Label loop_start, loop_end;
 166   Label do_int, do_long, do_float, do_double, do_dontreachhere, do_object, do_array, do_boxed;
 167 
 168   // signature points to '(' at entry
 169 #ifdef ASSERT
 170   __ lbz(sig_byte, 0, signature);
 171   __ cmplwi(CCR0, sig_byte, '(');
 172   __ bne(CCR0, do_dontreachhere);
 173 #endif
 174 
 175   __ bind(loop_start);
 176 
 177   __ addi(argcnt, argcnt, 1);
 178   __ lbzu(sig_byte, 1, signature);
 179 
 180   __ cmplwi(CCR0, sig_byte, ')'); // end of signature
 181   __ beq(CCR0, loop_end);
 182 
 183   __ cmplwi(CCR0, sig_byte, 'B'); // byte
 184   __ beq(CCR0, do_int);
 185 
 186   __ cmplwi(CCR0, sig_byte, 'C'); // char
 187   __ beq(CCR0, do_int);
 188 
 189   __ cmplwi(CCR0, sig_byte, 'D'); // double
 190   __ beq(CCR0, do_double);
 191 
 192   __ cmplwi(CCR0, sig_byte, 'F'); // float
 193   __ beq(CCR0, do_float);
 194 
 195   __ cmplwi(CCR0, sig_byte, 'I'); // int
 196   __ beq(CCR0, do_int);
 197 
 198   __ cmplwi(CCR0, sig_byte, 'J'); // long
 199   __ beq(CCR0, do_long);
 200 
 201   __ cmplwi(CCR0, sig_byte, 'S'); // short
 202   __ beq(CCR0, do_int);
 203 
 204   __ cmplwi(CCR0, sig_byte, 'Z'); // boolean
 205   __ beq(CCR0, do_int);
 206 
 207   __ cmplwi(CCR0, sig_byte, 'L'); // object
 208   __ beq(CCR0, do_object);
 209 
 210   __ cmplwi(CCR0, sig_byte, '['); // array
 211   __ beq(CCR0, do_array);
 212 
 213   //  __ cmplwi(CCR0, sig_byte, 'V'); // void cannot appear since we do not parse the return type
 214   //  __ beq(CCR0, do_void);
 215 
 216   __ bind(do_dontreachhere);
 217 
 218   __ unimplemented("ShouldNotReachHere in slow_signature_handler", 120);
 219 
 220   __ bind(do_array);
 221 
 222   {
 223     Label start_skip, end_skip;
 224 
 225     __ bind(start_skip);
 226     __ lbzu(sig_byte, 1, signature);
 227     __ cmplwi(CCR0, sig_byte, '[');
 228     __ beq(CCR0, start_skip); // skip further brackets
 229     __ cmplwi(CCR0, sig_byte, '9');
 230     __ bgt(CCR0, end_skip);   // no optional size
 231     __ cmplwi(CCR0, sig_byte, '0');
 232     __ bge(CCR0, start_skip); // skip optional size
 233     __ bind(end_skip);
 234 
 235     __ cmplwi(CCR0, sig_byte, 'L');
 236     __ beq(CCR0, do_object);  // for arrays of objects, the name of the object must be skipped
 237     __ b(do_boxed);          // otherwise, go directly to do_boxed
 238   }
 239 
 240   __ bind(do_object);
 241   {
 242     Label L;
 243     __ bind(L);
 244     __ lbzu(sig_byte, 1, signature);
 245     __ cmplwi(CCR0, sig_byte, ';');
 246     __ bne(CCR0, L);
 247    }
 248   // Need to box the Java object here, so we use arg_java (address of
 249   // current Java stack slot) as argument and don't dereference it as
 250   // in case of ints, floats, etc.
 251   Label do_null;
 252   __ bind(do_boxed);
 253   __ ld(R0,0, arg_java);
 254   __ cmpdi(CCR0, R0, 0);
 255   __ li(intSlot,0);
 256   __ beq(CCR0, do_null);
 257   __ mr(intSlot, arg_java);
 258   __ bind(do_null);
 259   __ std(intSlot, 0, arg_c);
 260   __ addi(arg_java, arg_java, -BytesPerWord);
 261   __ addi(arg_c, arg_c, BytesPerWord);
 262   __ cmplwi(CCR0, argcnt, max_int_register_arguments);
 263   __ blt(CCR0, move_intSlot_to_ARG);
 264   __ b(loop_start);
 265 
 266   __ bind(do_int);
 267   __ lwa(intSlot, 0, arg_java);
 268   __ std(intSlot, 0, arg_c);
 269   __ addi(arg_java, arg_java, -BytesPerWord);
 270   __ addi(arg_c, arg_c, BytesPerWord);
 271   __ cmplwi(CCR0, argcnt, max_int_register_arguments);
 272   __ blt(CCR0, move_intSlot_to_ARG);
 273   __ b(loop_start);
 274 
 275   __ bind(do_long);
 276   __ ld(intSlot, -BytesPerWord, arg_java);
 277   __ std(intSlot, 0, arg_c);
 278   __ addi(arg_java, arg_java, - 2 * BytesPerWord);
 279   __ addi(arg_c, arg_c, BytesPerWord);
 280   __ cmplwi(CCR0, argcnt, max_int_register_arguments);
 281   __ blt(CCR0, move_intSlot_to_ARG);
 282   __ b(loop_start);
 283 
 284   __ bind(do_float);
 285   __ lfs(floatSlot, 0, arg_java);
 286 #if defined(LINUX)
 287   // Linux uses ELF ABI. Both original ELF and ELFv2 ABIs have float
 288   // in the least significant word of an argument slot.
 289 #if defined(VM_LITTLE_ENDIAN)
 290   __ stfs(floatSlot, 0, arg_c);
 291 #else
 292   __ stfs(floatSlot, 4, arg_c);
 293 #endif
 294 #elif defined(AIX)
 295   // Although AIX runs on big endian CPU, float is in most significant
 296   // word of an argument slot.
 297   __ stfs(floatSlot, 0, arg_c);
 298 #else
 299 #error "unknown OS"
 300 #endif
 301   __ addi(arg_java, arg_java, -BytesPerWord);
 302   __ addi(arg_c, arg_c, BytesPerWord);
 303   __ cmplwi(CCR0, fpcnt, max_fp_register_arguments);
 304   __ blt(CCR0, move_floatSlot_to_FARG);
 305   __ b(loop_start);
 306 
 307   __ bind(do_double);
 308   __ lfd(floatSlot, - BytesPerWord, arg_java);
 309   __ stfd(floatSlot, 0, arg_c);
 310   __ addi(arg_java, arg_java, - 2 * BytesPerWord);
 311   __ addi(arg_c, arg_c, BytesPerWord);
 312   __ cmplwi(CCR0, fpcnt, max_fp_register_arguments);
 313   __ blt(CCR0, move_floatSlot_to_FARG);
 314   __ b(loop_start);
 315 
 316   __ bind(loop_end);
 317 
 318   __ pop_frame();
 319   __ restore_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));
 320   __ restore_LR_CR(R0);
 321 
 322   __ blr();
 323 
 324   Label move_int_arg, move_float_arg;
 325   __ bind(move_int_arg); // each case must consist of 2 instructions (otherwise adapt LogSizeOfTwoInstructions)
 326   __ mr(R5_ARG3, intSlot);  __ b(loop_start);
 327   __ mr(R6_ARG4, intSlot);  __ b(loop_start);
 328   __ mr(R7_ARG5, intSlot);  __ b(loop_start);
 329   __ mr(R8_ARG6, intSlot);  __ b(loop_start);
 330   __ mr(R9_ARG7, intSlot);  __ b(loop_start);
 331   __ mr(R10_ARG8, intSlot); __ b(loop_start);
 332 
 333   __ bind(move_float_arg); // each case must consist of 2 instructions (otherwise adapt LogSizeOfTwoInstructions)
 334   __ fmr(F1_ARG1, floatSlot);   __ b(loop_start);
 335   __ fmr(F2_ARG2, floatSlot);   __ b(loop_start);
 336   __ fmr(F3_ARG3, floatSlot);   __ b(loop_start);
 337   __ fmr(F4_ARG4, floatSlot);   __ b(loop_start);
 338   __ fmr(F5_ARG5, floatSlot);   __ b(loop_start);
 339   __ fmr(F6_ARG6, floatSlot);   __ b(loop_start);
 340   __ fmr(F7_ARG7, floatSlot);   __ b(loop_start);
 341   __ fmr(F8_ARG8, floatSlot);   __ b(loop_start);
 342   __ fmr(F9_ARG9, floatSlot);   __ b(loop_start);
 343   __ fmr(F10_ARG10, floatSlot); __ b(loop_start);
 344   __ fmr(F11_ARG11, floatSlot); __ b(loop_start);
 345   __ fmr(F12_ARG12, floatSlot); __ b(loop_start);
 346   __ fmr(F13_ARG13, floatSlot); __ b(loop_start);
 347 
 348   __ bind(move_intSlot_to_ARG);
 349   __ sldi(R0, argcnt, LogSizeOfTwoInstructions);
 350   __ load_const(R11_scratch1, move_int_arg); // Label must be bound here.
 351   __ add(R11_scratch1, R0, R11_scratch1);
 352   __ mtctr(R11_scratch1/*branch_target*/);
 353   __ bctr();
 354   __ bind(move_floatSlot_to_FARG);
 355   __ sldi(R0, fpcnt, LogSizeOfTwoInstructions);
 356   __ addi(fpcnt, fpcnt, 1);
 357   __ load_const(R11_scratch1, move_float_arg); // Label must be bound here.
 358   __ add(R11_scratch1, R0, R11_scratch1);
 359   __ mtctr(R11_scratch1/*branch_target*/);
 360   __ bctr();
 361 
 362   return entry;
 363 }
 364 
 365 address TemplateInterpreterGenerator::generate_result_handler_for(BasicType type) {
 366   //
 367   // Registers alive
 368   //   R3_RET
 369   //   LR
 370   //
 371   // Registers updated
 372   //   R3_RET
 373   //
 374 
 375   Label done;
 376   address entry = __ pc();
 377 
 378   switch (type) {
 379   case T_BOOLEAN:
 380     // convert !=0 to 1
 381     __ neg(R0, R3_RET);
 382     __ orr(R0, R3_RET, R0);
 383     __ srwi(R3_RET, R0, 31);
 384     break;
 385   case T_BYTE:
 386      // sign extend 8 bits
 387      __ extsb(R3_RET, R3_RET);
 388      break;
 389   case T_CHAR:
 390      // zero extend 16 bits
 391      __ clrldi(R3_RET, R3_RET, 48);
 392      break;
 393   case T_SHORT:
 394      // sign extend 16 bits
 395      __ extsh(R3_RET, R3_RET);
 396      break;
 397   case T_INT:
 398      // sign extend 32 bits
 399      __ extsw(R3_RET, R3_RET);
 400      break;
 401   case T_LONG:
 402      break;
 403   case T_OBJECT:
 404     // unbox result if not null
 405     __ cmpdi(CCR0, R3_RET, 0);
 406     __ beq(CCR0, done);
 407     __ ld(R3_RET, 0, R3_RET);
 408     __ verify_oop(R3_RET);
 409     break;
 410   case T_FLOAT:
 411      break;
 412   case T_DOUBLE:
 413      break;
 414   case T_VOID:
 415      break;
 416   default: ShouldNotReachHere();
 417   }
 418 
 419   BIND(done);
 420   __ blr();
 421 
 422   return entry;
 423 }
 424 
 425 // Abstract method entry.
 426 //
 427 address TemplateInterpreterGenerator::generate_abstract_entry(void) {
 428   address entry = __ pc();
 429 
 430   //
 431   // Registers alive
 432   //   R16_thread     - JavaThread*
 433   //   R19_method     - callee's method (method to be invoked)
 434   //   R1_SP          - SP prepared such that caller's outgoing args are near top
 435   //   LR             - return address to caller
 436   //
 437   // Stack layout at this point:
 438   //
 439   //   0       [TOP_IJAVA_FRAME_ABI]         &lt;-- R1_SP
 440   //           alignment (optional)
 441   //           [outgoing Java arguments]
 442   //           ...
 443   //   PARENT  [PARENT_IJAVA_FRAME_ABI]
 444   //            ...
 445   //
 446 
 447   // Can't use call_VM here because we have not set up a new
 448   // interpreter state. Make the call to the vm and make it look like
 449   // our caller set up the JavaFrameAnchor.
 450   __ set_top_ijava_frame_at_SP_as_last_Java_frame(R1_SP, R12_scratch2/*tmp*/);
 451 
 452   // Push a new C frame and save LR.
 453   __ save_LR_CR(R0);
 454   __ push_frame_reg_args(0, R11_scratch1);
 455 
 456   // This is not a leaf but we have a JavaFrameAnchor now and we will
 457   // check (create) exceptions afterward so this is ok.
 458   __ call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_AbstractMethodError),
 459                   R16_thread);
 460 
 461   // Pop the C frame and restore LR.
 462   __ pop_frame();
 463   __ restore_LR_CR(R0);
 464 
 465   // Reset JavaFrameAnchor from call_VM_leaf above.
 466   __ reset_last_Java_frame();
 467 
 468   // We don't know our caller, so jump to the general forward exception stub,
 469   // which will also pop our full frame off. Satisfy the interface of
 470   // SharedRuntime::generate_forward_exception()
 471   __ load_const_optimized(R11_scratch1, StubRoutines::forward_exception_entry(), R0);
 472   __ mtctr(R11_scratch1);
 473   __ bctr();
 474 
 475   return entry;
 476 }
 477 
 478 // Interpreter intrinsic for WeakReference.get().
 479 // 1. Don't push a full blown frame and go on dispatching, but fetch the value
 480 //    into R8 and return quickly
 481 // 2. If G1 is active we *must* execute this intrinsic for corrrectness:
 482 //    It contains a GC barrier which puts the reference into the satb buffer
 483 //    to indicate that someone holds a strong reference to the object the
 484 //    weak ref points to!
 485 address TemplateInterpreterGenerator::generate_Reference_get_entry(void) {
 486   // Code: _aload_0, _getfield, _areturn
 487   // parameter size = 1
 488   //
 489   // The code that gets generated by this routine is split into 2 parts:
 490   //    1. the "intrinsified" code for G1 (or any SATB based GC),
 491   //    2. the slow path - which is an expansion of the regular method entry.
 492   //
 493   // Notes:
 494   // * In the G1 code we do not check whether we need to block for
 495   //   a safepoint. If G1 is enabled then we must execute the specialized
 496   //   code for Reference.get (except when the Reference object is null)
 497   //   so that we can log the value in the referent field with an SATB
 498   //   update buffer.
 499   //   If the code for the getfield template is modified so that the
 500   //   G1 pre-barrier code is executed when the current method is
 501   //   Reference.get() then going through the normal method entry
 502   //   will be fine.
 503   // * The G1 code can, however, check the receiver object (the instance
 504   //   of java.lang.Reference) and jump to the slow path if null. If the
 505   //   Reference object is null then we obviously cannot fetch the referent
 506   //   and so we don't need to call the G1 pre-barrier. Thus we can use the
 507   //   regular method entry code to generate the NPE.
 508   //
 509 
 510   if (UseG1GC) {
 511     address entry = __ pc();
 512 
 513     const int referent_offset = java_lang_ref_Reference::referent_offset;
 514     guarantee(referent_offset &gt; 0, "referent offset not initialized");
 515 
 516     Label slow_path;
 517 
 518     // Debugging not possible, so can't use __ skip_if_jvmti_mode(slow_path, GR31_SCRATCH);
 519 
 520     // In the G1 code we don't check if we need to reach a safepoint. We
 521     // continue and the thread will safepoint at the next bytecode dispatch.
 522 
 523     // If the receiver is null then it is OK to jump to the slow path.
 524     __ ld(R3_RET, Interpreter::stackElementSize, R15_esp); // get receiver
 525 
 526     // Check if receiver == NULL and go the slow path.
 527     __ cmpdi(CCR0, R3_RET, 0);
 528     __ beq(CCR0, slow_path);
 529 
 530     // Load the value of the referent field.
 531     __ load_heap_oop(R3_RET, referent_offset, R3_RET);
 532 
 533     // Generate the G1 pre-barrier code to log the value of
 534     // the referent field in an SATB buffer. Note with
 535     // these parameters the pre-barrier does not generate
 536     // the load of the previous value.
 537 
 538     // Restore caller sp for c2i case.
 539 #ifdef ASSERT
 540       __ ld(R9_ARG7, 0, R1_SP);
 541       __ ld(R10_ARG8, 0, R21_sender_SP);
 542       __ cmpd(CCR0, R9_ARG7, R10_ARG8);
 543       __ asm_assert_eq("backlink", 0x544);
 544 #endif // ASSERT
 545     __ mr(R1_SP, R21_sender_SP); // Cut the stack back to where the caller started.
 546 
 547     __ g1_write_barrier_pre(noreg,         // obj
 548                             noreg,         // offset
 549                             R3_RET,        // pre_val
 550                             R11_scratch1,  // tmp
 551                             R12_scratch2,  // tmp
 552                             true);         // needs_frame
 553 
 554     __ blr();
 555 
 556     // Generate regular method entry.
 557     __ bind(slow_path);
 558     __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::zerolocals), R11_scratch1);
 559     return entry;
 560   }
 561 
 562   return NULL;
 563 }
 564 
 565 address TemplateInterpreterGenerator::generate_StackOverflowError_handler() {
 566   address entry = __ pc();
 567 
 568   // Expression stack must be empty before entering the VM if an
 569   // exception happened.
 570   __ empty_expression_stack();
 571   // Throw exception.
 572   __ call_VM(noreg,
 573              CAST_FROM_FN_PTR(address,
 574                               InterpreterRuntime::throw_StackOverflowError));
 575   return entry;
 576 }
 577 
 578 address TemplateInterpreterGenerator::generate_ArrayIndexOutOfBounds_handler(const char* name) {
 579   address entry = __ pc();
 580   __ empty_expression_stack();
 581   __ load_const_optimized(R4_ARG2, (address) name);
 582   // Index is in R17_tos.
 583   __ mr(R5_ARG3, R17_tos);
 584   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_ArrayIndexOutOfBoundsException));
 585   return entry;
 586 }
 587 
 588 #if 0
 589 // Call special ClassCastException constructor taking object to cast
 590 // and target class as arguments.
 591 address TemplateInterpreterGenerator::generate_ClassCastException_verbose_handler() {
 592   address entry = __ pc();
 593 
 594   // Expression stack must be empty before entering the VM if an
 595   // exception happened.
 596   __ empty_expression_stack();
 597 
 598   // Thread will be loaded to R3_ARG1.
 599   // Target class oop is in register R5_ARG3 by convention!
 600   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_ClassCastException_verbose), R17_tos, R5_ARG3);
 601   // Above call must not return here since exception pending.
 602   DEBUG_ONLY(__ should_not_reach_here();)
 603   return entry;
 604 }
 605 #endif
 606 
 607 address TemplateInterpreterGenerator::generate_ClassCastException_handler() {
 608   address entry = __ pc();
 609   // Expression stack must be empty before entering the VM if an
 610   // exception happened.
 611   __ empty_expression_stack();
 612 
 613   // Load exception object.
 614   // Thread will be loaded to R3_ARG1.
 615   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_ClassCastException), R17_tos);
 616 #ifdef ASSERT
 617   // Above call must not return here since exception pending.
 618   __ should_not_reach_here();
 619 #endif
 620   return entry;
 621 }
 622 
 623 address TemplateInterpreterGenerator::generate_exception_handler_common(const char* name, const char* message, bool pass_oop) {
 624   address entry = __ pc();
 625   //__ untested("generate_exception_handler_common");
 626   Register Rexception = R17_tos;
 627 
 628   // Expression stack must be empty before entering the VM if an exception happened.
 629   __ empty_expression_stack();
 630 
 631   __ load_const_optimized(R4_ARG2, (address) name, R11_scratch1);
 632   if (pass_oop) {
 633     __ mr(R5_ARG3, Rexception);
 634     __ call_VM(Rexception, CAST_FROM_FN_PTR(address, InterpreterRuntime::create_klass_exception), false);
 635   } else {
 636     __ load_const_optimized(R5_ARG3, (address) message, R11_scratch1);
 637     __ call_VM(Rexception, CAST_FROM_FN_PTR(address, InterpreterRuntime::create_exception), false);
 638   }
 639 
 640   // Throw exception.
 641   __ mr(R3_ARG1, Rexception);
 642   __ load_const_optimized(R11_scratch1, Interpreter::throw_exception_entry(), R12_scratch2);
 643   __ mtctr(R11_scratch1);
 644   __ bctr();
 645 
 646   return entry;
 647 }
 648 
 649 address TemplateInterpreterGenerator::generate_continuation_for(TosState state) {
 650   address entry = __ pc();
 651   __ unimplemented("generate_continuation_for");
 652   return entry;
 653 }
 654 
 655 // This entry is returned to when a call returns to the interpreter.
 656 // When we arrive here, we expect that the callee stack frame is already popped.
 657 address TemplateInterpreterGenerator::generate_return_entry_for(TosState state, int step, size_t index_size) {
 658   address entry = __ pc();
 659 
 660   // Move the value out of the return register back to the TOS cache of current frame.
 661   switch (state) {
 662     case ltos:
 663     case btos:
 664     case ztos:
 665     case ctos:
 666     case stos:
 667     case atos:
 668     case itos: __ mr(R17_tos, R3_RET); break;   // RET -&gt; TOS cache
 669     case ftos:
 670     case dtos: __ fmr(F15_ftos, F1_RET); break; // TOS cache -&gt; GR_FRET
 671     case vtos: break;                           // Nothing to do, this was a void return.
 672     default  : ShouldNotReachHere();
 673   }
 674 
 675   __ restore_interpreter_state(R11_scratch1); // Sets R11_scratch1 = fp.
 676   __ ld(R12_scratch2, _ijava_state_neg(top_frame_sp), R11_scratch1);
 677   __ resize_frame_absolute(R12_scratch2, R11_scratch1, R0);
 678 
 679   // Compiled code destroys templateTableBase, reload.
 680   __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R12_scratch2);
 681 
 682   if (state == atos) {
 683     __ profile_return_type(R3_RET, R11_scratch1, R12_scratch2);
 684   }
 685 
 686   const Register cache = R11_scratch1;
 687   const Register size  = R12_scratch2;
 688   __ get_cache_and_index_at_bcp(cache, 1, index_size);
 689 
 690   // Get least significant byte of 64 bit value:
 691 #if defined(VM_LITTLE_ENDIAN)
 692   __ lbz(size, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()), cache);
 693 #else
 694   __ lbz(size, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()) + 7, cache);
 695 #endif
 696   __ sldi(size, size, Interpreter::logStackElementSize);
 697   __ add(R15_esp, R15_esp, size);
 698   __ dispatch_next(state, step);
 699   return entry;
 700 }
 701 
 702 address TemplateInterpreterGenerator::generate_deopt_entry_for(TosState state, int step) {
 703   address entry = __ pc();
 704   // If state != vtos, we're returning from a native method, which put it's result
 705   // into the result register. So move the value out of the return register back
 706   // to the TOS cache of current frame.
 707 
 708   switch (state) {
 709     case ltos:
 710     case btos:
 711     case ztos:
 712     case ctos:
 713     case stos:
 714     case atos:
 715     case itos: __ mr(R17_tos, R3_RET); break;   // GR_RET -&gt; TOS cache
 716     case ftos:
 717     case dtos: __ fmr(F15_ftos, F1_RET); break; // TOS cache -&gt; GR_FRET
 718     case vtos: break;                           // Nothing to do, this was a void return.
 719     default  : ShouldNotReachHere();
 720   }
 721 
 722   // Load LcpoolCache @@@ should be already set!
 723   __ get_constant_pool_cache(R27_constPoolCache);
 724 
 725   // Handle a pending exception, fall through if none.
 726   __ check_and_forward_exception(R11_scratch1, R12_scratch2);
 727 
 728   // Start executing bytecodes.
 729   __ dispatch_next(state, step);
 730 
 731   return entry;
 732 }
 733 
 734 address TemplateInterpreterGenerator::generate_safept_entry_for(TosState state, address runtime_entry) {
 735   address entry = __ pc();
 736 
 737   __ push(state);
 738   __ call_VM(noreg, runtime_entry);
 739   __ dispatch_via(vtos, Interpreter::_normal_table.table_for(vtos));
 740 
 741   return entry;
 742 }
 743 
 744 // Helpers for commoning out cases in the various type of method entries.
 745 
 746 // Increment invocation count &amp; check for overflow.
 747 //
 748 // Note: checking for negative value instead of overflow
 749 //       so we have a 'sticky' overflow test.
 750 //
 751 void TemplateInterpreterGenerator::generate_counter_incr(Label* overflow, Label* profile_method, Label* profile_method_continue) {
 752   // Note: In tiered we increment either counters in method or in MDO depending if we're profiling or not.
 753   Register Rscratch1   = R11_scratch1;
 754   Register Rscratch2   = R12_scratch2;
 755   Register R3_counters = R3_ARG1;
 756   Label done;
 757 
 758   if (TieredCompilation) {
 759     const int increment = InvocationCounter::count_increment;
 760     Label no_mdo;
 761     if (ProfileInterpreter) {
 762       const Register Rmdo = R3_counters;
 763       // If no method data exists, go to profile_continue.
 764       __ ld(Rmdo, in_bytes(Method::method_data_offset()), R19_method);
 765       __ cmpdi(CCR0, Rmdo, 0);
 766       __ beq(CCR0, no_mdo);
 767 
 768       // Increment invocation counter in the MDO.
 769       const int mdo_ic_offs = in_bytes(MethodData::invocation_counter_offset()) + in_bytes(InvocationCounter::counter_offset());
 770       __ lwz(Rscratch2, mdo_ic_offs, Rmdo);
 771       __ lwz(Rscratch1, in_bytes(MethodData::invoke_mask_offset()), Rmdo);
 772       __ addi(Rscratch2, Rscratch2, increment);
 773       __ stw(Rscratch2, mdo_ic_offs, Rmdo);
 774       __ and_(Rscratch1, Rscratch2, Rscratch1);
 775       __ bne(CCR0, done);
 776       __ b(*overflow);
 777     }
 778 
 779     // Increment counter in MethodCounters*.
 780     const int mo_ic_offs = in_bytes(MethodCounters::invocation_counter_offset()) + in_bytes(InvocationCounter::counter_offset());
 781     __ bind(no_mdo);
 782     __ get_method_counters(R19_method, R3_counters, done);
 783     __ lwz(Rscratch2, mo_ic_offs, R3_counters);
 784     __ lwz(Rscratch1, in_bytes(MethodCounters::invoke_mask_offset()), R3_counters);
 785     __ addi(Rscratch2, Rscratch2, increment);
 786     __ stw(Rscratch2, mo_ic_offs, R3_counters);
 787     __ and_(Rscratch1, Rscratch2, Rscratch1);
 788     __ beq(CCR0, *overflow);
 789 
 790     __ bind(done);
 791 
 792   } else {
 793 
 794     // Update standard invocation counters.
 795     Register Rsum_ivc_bec = R4_ARG2;
 796     __ get_method_counters(R19_method, R3_counters, done);
 797     __ increment_invocation_counter(R3_counters, Rsum_ivc_bec, R12_scratch2);
 798     // Increment interpreter invocation counter.
 799     if (ProfileInterpreter) {  // %%% Merge this into methodDataOop.
 800       __ lwz(R12_scratch2, in_bytes(MethodCounters::interpreter_invocation_counter_offset()), R3_counters);
 801       __ addi(R12_scratch2, R12_scratch2, 1);
 802       __ stw(R12_scratch2, in_bytes(MethodCounters::interpreter_invocation_counter_offset()), R3_counters);
 803     }
 804     // Check if we must create a method data obj.
 805     if (ProfileInterpreter &amp;&amp; profile_method != NULL) {
 806       const Register profile_limit = Rscratch1;
 807       __ lwz(profile_limit, in_bytes(MethodCounters::interpreter_profile_limit_offset()), R3_counters);
 808       // Test to see if we should create a method data oop.
 809       __ cmpw(CCR0, Rsum_ivc_bec, profile_limit);
 810       __ blt(CCR0, *profile_method_continue);
 811       // If no method data exists, go to profile_method.
 812       __ test_method_data_pointer(*profile_method);
 813     }
 814     // Finally check for counter overflow.
 815     if (overflow) {
 816       const Register invocation_limit = Rscratch1;
 817       __ lwz(invocation_limit, in_bytes(MethodCounters::interpreter_invocation_limit_offset()), R3_counters);
 818       __ cmpw(CCR0, Rsum_ivc_bec, invocation_limit);
 819       __ bge(CCR0, *overflow);
 820     }
 821 
 822     __ bind(done);
 823   }
 824 }
 825 
 826 // Generate code to initiate compilation on invocation counter overflow.
 827 void TemplateInterpreterGenerator::generate_counter_overflow(Label&amp; continue_entry) {
 828   // Generate code to initiate compilation on the counter overflow.
 829 
 830   // InterpreterRuntime::frequency_counter_overflow takes one arguments,
 831   // which indicates if the counter overflow occurs at a backwards branch (NULL bcp)
 832   // We pass zero in.
 833   // The call returns the address of the verified entry point for the method or NULL
 834   // if the compilation did not complete (either went background or bailed out).
 835   //
 836   // Unlike the C++ interpreter above: Check exceptions!
 837   // Assumption: Caller must set the flag "do_not_unlock_if_sychronized" if the monitor of a sync'ed
 838   // method has not yet been created. Thus, no unlocking of a non-existing monitor can occur.
 839 
 840   __ li(R4_ARG2, 0);
 841   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow), R4_ARG2, true);
 842 
 843   // Returns verified_entry_point or NULL.
 844   // We ignore it in any case.
 845   __ b(continue_entry);
 846 }
 847 
 848 // See if we've got enough room on the stack for locals plus overhead below
 849 // JavaThread::stack_overflow_limit(). If not, throw a StackOverflowError
 850 // without going through the signal handler, i.e., reserved and yellow zones
 851 // will not be made usable. The shadow zone must suffice to handle the
 852 // overflow.
 853 //
 854 // Kills Rmem_frame_size, Rscratch1.
 855 void TemplateInterpreterGenerator::generate_stack_overflow_check(Register Rmem_frame_size, Register Rscratch1) {
 856   Label done;
 857   assert_different_registers(Rmem_frame_size, Rscratch1);
 858 
 859   BLOCK_COMMENT("stack_overflow_check_with_compare {");
 860   __ sub(Rmem_frame_size, R1_SP, Rmem_frame_size);
 861   __ ld(Rscratch1, thread_(stack_overflow_limit));
 862   __ cmpld(CCR0/*is_stack_overflow*/, Rmem_frame_size, Rscratch1);
 863   __ bgt(CCR0/*is_stack_overflow*/, done);
 864 
 865   // The stack overflows. Load target address of the runtime stub and call it.
 866   assert(StubRoutines::throw_StackOverflowError_entry() != NULL, "generated in wrong order");
 867   __ load_const_optimized(Rscratch1, (StubRoutines::throw_StackOverflowError_entry()), R0);
 868   __ mtctr(Rscratch1);
 869   // Restore caller_sp.
 870 #ifdef ASSERT
 871   __ ld(Rscratch1, 0, R1_SP);
 872   __ ld(R0, 0, R21_sender_SP);
 873   __ cmpd(CCR0, R0, Rscratch1);
 874   __ asm_assert_eq("backlink", 0x547);
 875 #endif // ASSERT
 876   __ mr(R1_SP, R21_sender_SP);
 877   __ bctr();
 878 
 879   __ align(32, 12);
 880   __ bind(done);
 881   BLOCK_COMMENT("} stack_overflow_check_with_compare");
 882 }
 883 
 884 // Lock the current method, interpreter register window must be set up!
 885 void TemplateInterpreterGenerator::lock_method(Register Rflags, Register Rscratch1, Register Rscratch2, bool flags_preloaded) {
 886   const Register Robj_to_lock = Rscratch2;
 887 
 888   {
 889     if (!flags_preloaded) {
 890       __ lwz(Rflags, method_(access_flags));
 891     }
 892 
 893 #ifdef ASSERT
 894     // Check if methods needs synchronization.
 895     {
 896       Label Lok;
 897       __ testbitdi(CCR0, R0, Rflags, JVM_ACC_SYNCHRONIZED_BIT);
 898       __ btrue(CCR0,Lok);
 899       __ stop("method doesn't need synchronization");
 900       __ bind(Lok);
 901     }
 902 #endif // ASSERT
 903   }
 904 
 905   // Get synchronization object to Rscratch2.
 906   {
 907     Label Lstatic;
 908     Label Ldone;
 909 
 910     __ testbitdi(CCR0, R0, Rflags, JVM_ACC_STATIC_BIT);
 911     __ btrue(CCR0, Lstatic);
 912 
 913     // Non-static case: load receiver obj from stack and we're done.
 914     __ ld(Robj_to_lock, R18_locals);
 915     __ b(Ldone);
 916 
 917     __ bind(Lstatic); // Static case: Lock the java mirror
 918     __ load_mirror(Robj_to_lock, R19_method);
 919 
 920     __ bind(Ldone);
 921     __ verify_oop(Robj_to_lock);
 922   }
 923 
 924   // Got the oop to lock =&gt; execute!
 925   __ add_monitor_to_stack(true, Rscratch1, R0);
 926 
 927   __ std(Robj_to_lock, BasicObjectLock::obj_offset_in_bytes(), R26_monitor);
 928   __ lock_object(R26_monitor, Robj_to_lock);
 929 }
 930 
 931 // Generate a fixed interpreter frame for pure interpreter
 932 // and I2N native transition frames.
 933 //
 934 // Before (stack grows downwards):
 935 //
 936 //         |  ...         |
 937 //         |------------- |
 938 //         |  java arg0   |
 939 //         |  ...         |
 940 //         |  java argn   |
 941 //         |              |   &lt;-   R15_esp
 942 //         |              |
 943 //         |--------------|
 944 //         | abi_112      |
 945 //         |              |   &lt;-   R1_SP
 946 //         |==============|
 947 //
 948 //
 949 // After:
 950 //
 951 //         |  ...         |
 952 //         |  java arg0   |&lt;-   R18_locals
 953 //         |  ...         |
 954 //         |  java argn   |
 955 //         |--------------|
 956 //         |              |
 957 //         |  java locals |
 958 //         |              |
 959 //         |--------------|
 960 //         |  abi_48      |
 961 //         |==============|
 962 //         |              |
 963 //         |   istate     |
 964 //         |              |
 965 //         |--------------|
 966 //         |   monitor    |&lt;-   R26_monitor
 967 //         |--------------|
 968 //         |              |&lt;-   R15_esp
 969 //         | expression   |
 970 //         | stack        |
 971 //         |              |
 972 //         |--------------|
 973 //         |              |
 974 //         | abi_112      |&lt;-   R1_SP
 975 //         |==============|
 976 //
 977 // The top most frame needs an abi space of 112 bytes. This space is needed,
 978 // since we call to c. The c function may spill their arguments to the caller
 979 // frame. When we call to java, we don't need these spill slots. In order to save
 980 // space on the stack, we resize the caller. However, java locals reside in
 981 // the caller frame and the frame has to be increased. The frame_size for the
 982 // current frame was calculated based on max_stack as size for the expression
 983 // stack. At the call, just a part of the expression stack might be used.
 984 // We don't want to waste this space and cut the frame back accordingly.
 985 // The resulting amount for resizing is calculated as follows:
 986 // resize =   (number_of_locals - number_of_arguments) * slot_size
 987 //          + (R1_SP - R15_esp) + 48
 988 //
 989 // The size for the callee frame is calculated:
 990 // framesize = 112 + max_stack + monitor + state_size
 991 //
 992 // maxstack:   Max number of slots on the expression stack, loaded from the method.
 993 // monitor:    We statically reserve room for one monitor object.
 994 // state_size: We save the current state of the interpreter to this area.
 995 //
 996 void TemplateInterpreterGenerator::generate_fixed_frame(bool native_call, Register Rsize_of_parameters, Register Rsize_of_locals) {
 997   Register parent_frame_resize = R6_ARG4, // Frame will grow by this number of bytes.
 998            top_frame_size      = R7_ARG5,
 999            Rconst_method       = R8_ARG6;
1000 
1001   assert_different_registers(Rsize_of_parameters, Rsize_of_locals, parent_frame_resize, top_frame_size);
1002 
1003   __ ld(Rconst_method, method_(const));
1004   __ lhz(Rsize_of_parameters /* number of params */,
1005          in_bytes(ConstMethod::size_of_parameters_offset()), Rconst_method);
1006   if (native_call) {
1007     // If we're calling a native method, we reserve space for the worst-case signature
1008     // handler varargs vector, which is max(Argument::n_register_parameters, parameter_count+2).
1009     // We add two slots to the parameter_count, one for the jni
1010     // environment and one for a possible native mirror.
1011     Label skip_native_calculate_max_stack;
1012     __ addi(top_frame_size, Rsize_of_parameters, 2);
1013     __ cmpwi(CCR0, top_frame_size, Argument::n_register_parameters);
1014     __ bge(CCR0, skip_native_calculate_max_stack);
1015     __ li(top_frame_size, Argument::n_register_parameters);
1016     __ bind(skip_native_calculate_max_stack);
1017     __ sldi(Rsize_of_parameters, Rsize_of_parameters, Interpreter::logStackElementSize);
1018     __ sldi(top_frame_size, top_frame_size, Interpreter::logStackElementSize);
1019     __ sub(parent_frame_resize, R1_SP, R15_esp); // &lt;0, off by Interpreter::stackElementSize!
1020     assert(Rsize_of_locals == noreg, "Rsize_of_locals not initialized"); // Only relevant value is Rsize_of_parameters.
1021   } else {
1022     __ lhz(Rsize_of_locals /* number of params */, in_bytes(ConstMethod::size_of_locals_offset()), Rconst_method);
1023     __ sldi(Rsize_of_parameters, Rsize_of_parameters, Interpreter::logStackElementSize);
1024     __ sldi(Rsize_of_locals, Rsize_of_locals, Interpreter::logStackElementSize);
1025     __ lhz(top_frame_size, in_bytes(ConstMethod::max_stack_offset()), Rconst_method);
1026     __ sub(R11_scratch1, Rsize_of_locals, Rsize_of_parameters); // &gt;=0
1027     __ sub(parent_frame_resize, R1_SP, R15_esp); // &lt;0, off by Interpreter::stackElementSize!
1028     __ sldi(top_frame_size, top_frame_size, Interpreter::logStackElementSize);
1029     __ add(parent_frame_resize, parent_frame_resize, R11_scratch1);
1030   }
1031 
1032   // Compute top frame size.
1033   __ addi(top_frame_size, top_frame_size, frame::abi_reg_args_size + frame::ijava_state_size);
1034 
1035   // Cut back area between esp and max_stack.
1036   __ addi(parent_frame_resize, parent_frame_resize, frame::abi_minframe_size - Interpreter::stackElementSize);
1037 
1038   __ round_to(top_frame_size, frame::alignment_in_bytes);
1039   __ round_to(parent_frame_resize, frame::alignment_in_bytes);
1040   // parent_frame_resize = (locals-parameters) - (ESP-SP-ABI48) Rounded to frame alignment size.
1041   // Enlarge by locals-parameters (not in case of native_call), shrink by ESP-SP-ABI48.
1042 
1043   if (!native_call) {
1044     // Stack overflow check.
1045     // Native calls don't need the stack size check since they have no
1046     // expression stack and the arguments are already on the stack and
1047     // we only add a handful of words to the stack.
1048     __ add(R11_scratch1, parent_frame_resize, top_frame_size);
1049     generate_stack_overflow_check(R11_scratch1, R12_scratch2);
1050   }
1051 
1052   // Set up interpreter state registers.
1053 
1054   __ add(R18_locals, R15_esp, Rsize_of_parameters);
1055   __ ld(R27_constPoolCache, in_bytes(ConstMethod::constants_offset()), Rconst_method);
1056   __ ld(R27_constPoolCache, ConstantPool::cache_offset_in_bytes(), R27_constPoolCache);
1057 
1058   // Set method data pointer.
1059   if (ProfileInterpreter) {
1060     Label zero_continue;
1061     __ ld(R28_mdx, method_(method_data));
1062     __ cmpdi(CCR0, R28_mdx, 0);
1063     __ beq(CCR0, zero_continue);
1064     __ addi(R28_mdx, R28_mdx, in_bytes(MethodData::data_offset()));
1065     __ bind(zero_continue);
1066   }
1067 
1068   if (native_call) {
1069     __ li(R14_bcp, 0); // Must initialize.
1070   } else {
1071     __ add(R14_bcp, in_bytes(ConstMethod::codes_offset()), Rconst_method);
1072   }
1073 
1074   // Resize parent frame.
1075   __ mflr(R12_scratch2);
1076   __ neg(parent_frame_resize, parent_frame_resize);
1077   __ resize_frame(parent_frame_resize, R11_scratch1);
1078   __ std(R12_scratch2, _abi(lr), R1_SP);
1079 
1080   __ addi(R26_monitor, R1_SP, - frame::ijava_state_size);
1081   __ addi(R15_esp, R26_monitor, - Interpreter::stackElementSize);
1082 
1083   // Get mirror and store it in the frame as GC root for this Method*.
1084   __ load_mirror(R12_scratch2, R19_method);
1085 
1086   // Store values.
1087   // R15_esp, R14_bcp, R26_monitor, R28_mdx are saved at java calls
1088   // in InterpreterMacroAssembler::call_from_interpreter.
1089   __ std(R19_method, _ijava_state_neg(method), R1_SP);
1090   __ std(R12_scratch2, _ijava_state_neg(mirror), R1_SP);
1091   __ std(R21_sender_SP, _ijava_state_neg(sender_sp), R1_SP);
1092   __ std(R27_constPoolCache, _ijava_state_neg(cpoolCache), R1_SP);
1093   __ std(R18_locals, _ijava_state_neg(locals), R1_SP);
1094 
1095   // Note: esp, bcp, monitor, mdx live in registers. Hence, the correct version can only
1096   // be found in the frame after save_interpreter_state is done. This is always true
1097   // for non-top frames. But when a signal occurs, dumping the top frame can go wrong,
1098   // because e.g. frame::interpreter_frame_bcp() will not access the correct value
1099   // (Enhanced Stack Trace).
1100   // The signal handler does not save the interpreter state into the frame.
1101   __ li(R0, 0);
1102 #ifdef ASSERT
1103   // Fill remaining slots with constants.
1104   __ load_const_optimized(R11_scratch1, 0x5afe);
1105   __ load_const_optimized(R12_scratch2, 0xdead);
1106 #endif
1107   // We have to initialize some frame slots for native calls (accessed by GC).
1108   if (native_call) {
1109     __ std(R26_monitor, _ijava_state_neg(monitors), R1_SP);
1110     __ std(R14_bcp, _ijava_state_neg(bcp), R1_SP);
1111     if (ProfileInterpreter) { __ std(R28_mdx, _ijava_state_neg(mdx), R1_SP); }
1112   }
1113 #ifdef ASSERT
1114   else {
1115     __ std(R12_scratch2, _ijava_state_neg(monitors), R1_SP);
1116     __ std(R12_scratch2, _ijava_state_neg(bcp), R1_SP);
1117     __ std(R12_scratch2, _ijava_state_neg(mdx), R1_SP);
1118   }
1119   __ std(R11_scratch1, _ijava_state_neg(ijava_reserved), R1_SP);
1120   __ std(R12_scratch2, _ijava_state_neg(esp), R1_SP);
1121   __ std(R12_scratch2, _ijava_state_neg(lresult), R1_SP);
1122   __ std(R12_scratch2, _ijava_state_neg(fresult), R1_SP);
1123 #endif
1124   __ subf(R12_scratch2, top_frame_size, R1_SP);
1125   __ std(R0, _ijava_state_neg(oop_tmp), R1_SP);
1126   __ std(R12_scratch2, _ijava_state_neg(top_frame_sp), R1_SP);
1127 
1128   // Push top frame.
1129   __ push_frame(top_frame_size, R11_scratch1);
1130 }
1131 
1132 // End of helpers
1133 
1134 address TemplateInterpreterGenerator::generate_math_entry(AbstractInterpreter::MethodKind kind) {
1135   if (!Interpreter::math_entry_available(kind)) {
1136     NOT_PRODUCT(__ should_not_reach_here();)
1137     return NULL;
1138   }
1139 
1140   address entry = __ pc();
1141 
1142   __ lfd(F1_RET, Interpreter::stackElementSize, R15_esp);
1143 
1144   // Pop c2i arguments (if any) off when we return.
1145 #ifdef ASSERT
1146   __ ld(R9_ARG7, 0, R1_SP);
1147   __ ld(R10_ARG8, 0, R21_sender_SP);
1148   __ cmpd(CCR0, R9_ARG7, R10_ARG8);
1149   __ asm_assert_eq("backlink", 0x545);
1150 #endif // ASSERT
1151   __ mr(R1_SP, R21_sender_SP); // Cut the stack back to where the caller started.
1152 
1153   if (kind == Interpreter::java_lang_math_sqrt) {
1154     __ fsqrt(F1_RET, F1_RET);
1155   } else if (kind == Interpreter::java_lang_math_abs) {
1156     __ fabs(F1_RET, F1_RET);
1157   } else {
1158     ShouldNotReachHere();
1159   }
1160 
1161   // And we're done.
1162   __ blr();
1163 
1164   __ flush();
1165 
1166   return entry;
1167 }
1168 
1169 void TemplateInterpreterGenerator::bang_stack_shadow_pages(bool native_call) {
1170   // Quick &amp; dirty stack overflow checking: bang the stack &amp; handle trap.
1171   // Note that we do the banging after the frame is setup, since the exception
1172   // handling code expects to find a valid interpreter frame on the stack.
1173   // Doing the banging earlier fails if the caller frame is not an interpreter
1174   // frame.
1175   // (Also, the exception throwing code expects to unlock any synchronized
1176   // method receiever, so do the banging after locking the receiver.)
1177 
1178   // Bang each page in the shadow zone. We can't assume it's been done for
1179   // an interpreter frame with greater than a page of locals, so each page
1180   // needs to be checked.  Only true for non-native.
1181   if (UseStackBanging) {
1182     const int page_size = os::vm_page_size();
1183     const int n_shadow_pages = ((int)JavaThread::stack_shadow_zone_size()) / page_size;
1184     const int start_page = native_call ? n_shadow_pages : 1;
1185     BLOCK_COMMENT("bang_stack_shadow_pages:");
1186     for (int pages = start_page; pages &lt;= n_shadow_pages; pages++) {
1187       __ bang_stack_with_offset(pages*page_size);
1188     }
1189   }
1190 }
1191 
1192 // Interpreter stub for calling a native method. (asm interpreter)
1193 // This sets up a somewhat different looking stack for calling the
1194 // native method than the typical interpreter frame setup.
1195 //
1196 // On entry:
1197 //   R19_method    - method
1198 //   R16_thread    - JavaThread*
1199 //   R15_esp       - intptr_t* sender tos
1200 //
1201 //   abstract stack (grows up)
1202 //     [  IJava (caller of JNI callee)  ]  &lt;-- ASP
1203 //        ...
1204 address TemplateInterpreterGenerator::generate_native_entry(bool synchronized) {
1205 
1206   address entry = __ pc();
1207 
1208   const bool inc_counter = UseCompiler || CountCompiledCalls || LogTouchedMethods;
1209 
1210   // -----------------------------------------------------------------------------
1211   // Allocate a new frame that represents the native callee (i2n frame).
1212   // This is not a full-blown interpreter frame, but in particular, the
1213   // following registers are valid after this:
1214   // - R19_method
1215   // - R18_local (points to start of arguments to native function)
1216   //
1217   //   abstract stack (grows up)
1218   //     [  IJava (caller of JNI callee)  ]  &lt;-- ASP
1219   //        ...
1220 
1221   const Register signature_handler_fd = R11_scratch1;
1222   const Register pending_exception    = R0;
1223   const Register result_handler_addr  = R31;
1224   const Register native_method_fd     = R11_scratch1;
1225   const Register access_flags         = R22_tmp2;
1226   const Register active_handles       = R11_scratch1; // R26_monitor saved to state.
1227   const Register sync_state           = R12_scratch2;
1228   const Register sync_state_addr      = sync_state;   // Address is dead after use.
1229   const Register suspend_flags        = R11_scratch1;
1230 
1231   //=============================================================================
1232   // Allocate new frame and initialize interpreter state.
1233 
1234   Label exception_return;
1235   Label exception_return_sync_check;
1236   Label stack_overflow_return;
1237 
1238   // Generate new interpreter state and jump to stack_overflow_return in case of
1239   // a stack overflow.
1240   //generate_compute_interpreter_state(stack_overflow_return);
1241 
1242   Register size_of_parameters = R22_tmp2;
1243 
1244   generate_fixed_frame(true, size_of_parameters, noreg /* unused */);
1245 
1246   //=============================================================================
1247   // Increment invocation counter. On overflow, entry to JNI method
1248   // will be compiled.
1249   Label invocation_counter_overflow, continue_after_compile;
1250   if (inc_counter) {
1251     if (synchronized) {
1252       // Since at this point in the method invocation the exception handler
1253       // would try to exit the monitor of synchronized methods which hasn't
1254       // been entered yet, we set the thread local variable
1255       // _do_not_unlock_if_synchronized to true. If any exception was thrown by
1256       // runtime, exception handling i.e. unlock_if_synchronized_method will
1257       // check this thread local flag.
1258       // This flag has two effects, one is to force an unwind in the topmost
1259       // interpreter frame and not perform an unlock while doing so.
1260       __ li(R0, 1);
1261       __ stb(R0, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread);
1262     }
1263     generate_counter_incr(&amp;invocation_counter_overflow, NULL, NULL);
1264 
1265     BIND(continue_after_compile);
1266   }
1267 
1268   bang_stack_shadow_pages(true);
1269 
1270   if (inc_counter) {
1271     // Reset the _do_not_unlock_if_synchronized flag.
1272     if (synchronized) {
1273       __ li(R0, 0);
1274       __ stb(R0, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread);
1275     }
1276   }
1277 
1278   // access_flags = method-&gt;access_flags();
1279   // Load access flags.
1280   assert(access_flags-&gt;is_nonvolatile(),
1281          "access_flags must be in a non-volatile register");
1282   // Type check.
1283   assert(4 == sizeof(AccessFlags), "unexpected field size");
1284   __ lwz(access_flags, method_(access_flags));
1285 
1286   // We don't want to reload R19_method and access_flags after calls
1287   // to some helper functions.
1288   assert(R19_method-&gt;is_nonvolatile(),
1289          "R19_method must be a non-volatile register");
1290 
1291   // Check for synchronized methods. Must happen AFTER invocation counter
1292   // check, so method is not locked if counter overflows.
1293 
1294   if (synchronized) {
1295     lock_method(access_flags, R11_scratch1, R12_scratch2, true);
1296 
1297     // Update monitor in state.
1298     __ ld(R11_scratch1, 0, R1_SP);
1299     __ std(R26_monitor, _ijava_state_neg(monitors), R11_scratch1);
1300   }
1301 
1302   // jvmti/jvmpi support
1303   __ notify_method_entry();
1304 
1305   //=============================================================================
1306   // Get and call the signature handler.
1307 
1308   __ ld(signature_handler_fd, method_(signature_handler));
1309   Label call_signature_handler;
1310 
1311   __ cmpdi(CCR0, signature_handler_fd, 0);
1312   __ bne(CCR0, call_signature_handler);
1313 
1314   // Method has never been called. Either generate a specialized
1315   // handler or point to the slow one.
1316   //
1317   // Pass parameter 'false' to avoid exception check in call_VM.
1318   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::prepare_native_call), R19_method, false);
1319 
1320   // Check for an exception while looking up the target method. If we
1321   // incurred one, bail.
1322   __ ld(pending_exception, thread_(pending_exception));
1323   __ cmpdi(CCR0, pending_exception, 0);
1324   __ bne(CCR0, exception_return_sync_check); // Has pending exception.
1325 
1326   // Reload signature handler, it may have been created/assigned in the meanwhile.
1327   __ ld(signature_handler_fd, method_(signature_handler));
1328   __ twi_0(signature_handler_fd); // Order wrt. load of klass mirror and entry point (isync is below).
1329 
1330   BIND(call_signature_handler);
1331 
1332   // Before we call the signature handler we push a new frame to
1333   // protect the interpreter frame volatile registers when we return
1334   // from jni but before we can get back to Java.
1335 
1336   // First set the frame anchor while the SP/FP registers are
1337   // convenient and the slow signature handler can use this same frame
1338   // anchor.
1339 
1340   // We have a TOP_IJAVA_FRAME here, which belongs to us.
1341   __ set_top_ijava_frame_at_SP_as_last_Java_frame(R1_SP, R12_scratch2/*tmp*/);
1342 
1343   // Now the interpreter frame (and its call chain) have been
1344   // invalidated and flushed. We are now protected against eager
1345   // being enabled in native code. Even if it goes eager the
1346   // registers will be reloaded as clean and we will invalidate after
1347   // the call so no spurious flush should be possible.
1348 
1349   // Call signature handler and pass locals address.
1350   //
1351   // Our signature handlers copy required arguments to the C stack
1352   // (outgoing C args), R3_ARG1 to R10_ARG8, and FARG1 to FARG13.
1353   __ mr(R3_ARG1, R18_locals);
1354 #if !defined(ABI_ELFv2)
1355   __ ld(signature_handler_fd, 0, signature_handler_fd);
1356 #endif
1357 
1358   __ call_stub(signature_handler_fd);
1359 
1360   // Remove the register parameter varargs slots we allocated in
1361   // compute_interpreter_state. SP+16 ends up pointing to the ABI
1362   // outgoing argument area.
1363   //
1364   // Not needed on PPC64.
1365   //__ add(SP, SP, Argument::n_register_parameters*BytesPerWord);
1366 
1367   assert(result_handler_addr-&gt;is_nonvolatile(), "result_handler_addr must be in a non-volatile register");
1368   // Save across call to native method.
1369   __ mr(result_handler_addr, R3_RET);
1370 
1371   __ isync(); // Acquire signature handler before trying to fetch the native entry point and klass mirror.
1372 
1373   // Set up fixed parameters and call the native method.
1374   // If the method is static, get mirror into R4_ARG2.
1375   {
1376     Label method_is_not_static;
1377     // Access_flags is non-volatile and still, no need to restore it.
1378 
1379     // Restore access flags.
1380     __ testbitdi(CCR0, R0, access_flags, JVM_ACC_STATIC_BIT);
1381     __ bfalse(CCR0, method_is_not_static);
1382 
1383     __ load_mirror(R12_scratch2, R19_method);
1384     // state-&gt;_native_mirror = mirror;
1385 
1386     __ ld(R11_scratch1, 0, R1_SP);
1387     __ std(R12_scratch2/*mirror*/, _ijava_state_neg(oop_tmp), R11_scratch1);
1388     // R4_ARG2 = &amp;state-&gt;_oop_temp;
1389     __ addi(R4_ARG2, R11_scratch1, _ijava_state_neg(oop_tmp));
1390     BIND(method_is_not_static);
1391   }
1392 
1393   // At this point, arguments have been copied off the stack into
1394   // their JNI positions. Oops are boxed in-place on the stack, with
1395   // handles copied to arguments. The result handler address is in a
1396   // register.
1397 
1398   // Pass JNIEnv address as first parameter.
1399   __ addir(R3_ARG1, thread_(jni_environment));
1400 
1401   // Load the native_method entry before we change the thread state.
1402   __ ld(native_method_fd, method_(native_function));
1403 
1404   //=============================================================================
1405   // Transition from _thread_in_Java to _thread_in_native. As soon as
1406   // we make this change the safepoint code needs to be certain that
1407   // the last Java frame we established is good. The pc in that frame
1408   // just needs to be near here not an actual return address.
1409 
1410   // We use release_store_fence to update values like the thread state, where
1411   // we don't want the current thread to continue until all our prior memory
1412   // accesses (including the new thread state) are visible to other threads.
1413   __ li(R0, _thread_in_native);
1414   __ release();
1415 
1416   // TODO PPC port assert(4 == JavaThread::sz_thread_state(), "unexpected field size");
1417   __ stw(R0, thread_(thread_state));
1418 
1419   if (UseMembar) {
1420     __ fence();
1421   }
1422 
1423   //=============================================================================
1424   // Call the native method. Argument registers must not have been
1425   // overwritten since "__ call_stub(signature_handler);" (except for
1426   // ARG1 and ARG2 for static methods).
1427   __ call_c(native_method_fd);
1428 
1429   __ li(R0, 0);
1430   __ ld(R11_scratch1, 0, R1_SP);
1431   __ std(R3_RET, _ijava_state_neg(lresult), R11_scratch1);
1432   __ stfd(F1_RET, _ijava_state_neg(fresult), R11_scratch1);
1433   __ std(R0/*mirror*/, _ijava_state_neg(oop_tmp), R11_scratch1); // reset
1434 
1435   // Note: C++ interpreter needs the following here:
1436   // The frame_manager_lr field, which we use for setting the last
1437   // java frame, gets overwritten by the signature handler. Restore
1438   // it now.
1439   //__ get_PC_trash_LR(R11_scratch1);
1440   //__ std(R11_scratch1, _top_ijava_frame_abi(frame_manager_lr), R1_SP);
1441 
1442   // Because of GC R19_method may no longer be valid.
1443 
1444   // Block, if necessary, before resuming in _thread_in_Java state.
1445   // In order for GC to work, don't clear the last_Java_sp until after
1446   // blocking.
1447 
1448   //=============================================================================
1449   // Switch thread to "native transition" state before reading the
1450   // synchronization state. This additional state is necessary
1451   // because reading and testing the synchronization state is not
1452   // atomic w.r.t. GC, as this scenario demonstrates: Java thread A,
1453   // in _thread_in_native state, loads _not_synchronized and is
1454   // preempted. VM thread changes sync state to synchronizing and
1455   // suspends threads for GC. Thread A is resumed to finish this
1456   // native method, but doesn't block here since it didn't see any
1457   // synchronization in progress, and escapes.
1458 
1459   // We use release_store_fence to update values like the thread state, where
1460   // we don't want the current thread to continue until all our prior memory
1461   // accesses (including the new thread state) are visible to other threads.
1462   __ li(R0/*thread_state*/, _thread_in_native_trans);
1463   __ release();
1464   __ stw(R0/*thread_state*/, thread_(thread_state));
1465   if (UseMembar) {
1466     __ fence();
1467   }
1468   // Write serialization page so that the VM thread can do a pseudo remote
1469   // membar. We use the current thread pointer to calculate a thread
1470   // specific offset to write to within the page. This minimizes bus
1471   // traffic due to cache line collision.
1472   else {
1473     __ serialize_memory(R16_thread, R11_scratch1, R12_scratch2);
1474   }
1475 
1476   // Now before we return to java we must look for a current safepoint
1477   // (a new safepoint can not start since we entered native_trans).
1478   // We must check here because a current safepoint could be modifying
1479   // the callers registers right this moment.
1480 
1481   // Acquire isn't strictly necessary here because of the fence, but
1482   // sync_state is declared to be volatile, so we do it anyway
1483   // (cmp-br-isync on one path, release (same as acquire on PPC64) on the other path).
1484   int sync_state_offs = __ load_const_optimized(sync_state_addr, SafepointSynchronize::address_of_state(), /*temp*/R0, true);
1485 
1486   // TODO PPC port assert(4 == SafepointSynchronize::sz_state(), "unexpected field size");
1487   __ lwz(sync_state, sync_state_offs, sync_state_addr);
1488 
1489   // TODO PPC port assert(4 == Thread::sz_suspend_flags(), "unexpected field size");
1490   __ lwz(suspend_flags, thread_(suspend_flags));
1491 
1492   Label sync_check_done;
1493   Label do_safepoint;
1494   // No synchronization in progress nor yet synchronized.
1495   __ cmpwi(CCR0, sync_state, SafepointSynchronize::_not_synchronized);
1496   // Not suspended.
1497   __ cmpwi(CCR1, suspend_flags, 0);
1498 
1499   __ bne(CCR0, do_safepoint);
1500   __ beq(CCR1, sync_check_done);
1501   __ bind(do_safepoint);
1502   __ isync();
1503   // Block. We do the call directly and leave the current
1504   // last_Java_frame setup undisturbed. We must save any possible
1505   // native result across the call. No oop is present.
1506 
1507   __ mr(R3_ARG1, R16_thread);
1508 #if defined(ABI_ELFv2)
1509   __ call_c(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans),
1510             relocInfo::none);
1511 #else
1512   __ call_c(CAST_FROM_FN_PTR(FunctionDescriptor*, JavaThread::check_special_condition_for_native_trans),
1513             relocInfo::none);
1514 #endif
1515 
1516   __ bind(sync_check_done);
1517 
1518   //=============================================================================
1519   // &lt;&lt;&lt;&lt;&lt;&lt; Back in Interpreter Frame &gt;&gt;&gt;&gt;&gt;
1520 
1521   // We are in thread_in_native_trans here and back in the normal
1522   // interpreter frame. We don't have to do anything special about
1523   // safepoints and we can switch to Java mode anytime we are ready.
1524 
1525   // Note: frame::interpreter_frame_result has a dependency on how the
1526   // method result is saved across the call to post_method_exit. For
1527   // native methods it assumes that the non-FPU/non-void result is
1528   // saved in _native_lresult and a FPU result in _native_fresult. If
1529   // this changes then the interpreter_frame_result implementation
1530   // will need to be updated too.
1531 
1532   // On PPC64, we have stored the result directly after the native call.
1533 
1534   //=============================================================================
1535   // Back in Java
1536 
1537   // We use release_store_fence to update values like the thread state, where
1538   // we don't want the current thread to continue until all our prior memory
1539   // accesses (including the new thread state) are visible to other threads.
1540   __ li(R0/*thread_state*/, _thread_in_Java);
1541   __ release();
1542   __ stw(R0/*thread_state*/, thread_(thread_state));
1543   if (UseMembar) {
1544     __ fence();
1545   }
1546 
1547   __ reset_last_Java_frame();
1548 
1549   // Jvmdi/jvmpi support. Whether we've got an exception pending or
1550   // not, and whether unlocking throws an exception or not, we notify
1551   // on native method exit. If we do have an exception, we'll end up
1552   // in the caller's context to handle it, so if we don't do the
1553   // notify here, we'll drop it on the floor.
1554   __ notify_method_exit(true/*native method*/,
1555                         ilgl /*illegal state (not used for native methods)*/,
1556                         InterpreterMacroAssembler::NotifyJVMTI,
1557                         false /*check_exceptions*/);
1558 
1559   //=============================================================================
1560   // Handle exceptions
1561 
1562   if (synchronized) {
1563     // Don't check for exceptions since we're still in the i2n frame. Do that
1564     // manually afterwards.
1565     __ unlock_object(R26_monitor, false); // Can also unlock methods.
1566   }
1567 
1568   // Reset active handles after returning from native.
1569   // thread-&gt;active_handles()-&gt;clear();
1570   __ ld(active_handles, thread_(active_handles));
1571   // TODO PPC port assert(4 == JNIHandleBlock::top_size_in_bytes(), "unexpected field size");
1572   __ li(R0, 0);
1573   __ stw(R0, JNIHandleBlock::top_offset_in_bytes(), active_handles);
1574 
1575   Label exception_return_sync_check_already_unlocked;
1576   __ ld(R0/*pending_exception*/, thread_(pending_exception));
1577   __ cmpdi(CCR0, R0/*pending_exception*/, 0);
1578   __ bne(CCR0, exception_return_sync_check_already_unlocked);
1579 
1580   //-----------------------------------------------------------------------------
1581   // No exception pending.
1582 
1583   // Move native method result back into proper registers and return.
1584   // Invoke result handler (may unbox/promote).
1585   __ ld(R11_scratch1, 0, R1_SP);
1586   __ ld(R3_RET, _ijava_state_neg(lresult), R11_scratch1);
1587   __ lfd(F1_RET, _ijava_state_neg(fresult), R11_scratch1);
1588   __ call_stub(result_handler_addr);
1589 
1590   __ merge_frames(/*top_frame_sp*/ R21_sender_SP, /*return_pc*/ R0, R11_scratch1, R12_scratch2);
1591 
1592   // Must use the return pc which was loaded from the caller's frame
1593   // as the VM uses return-pc-patching for deoptimization.
1594   __ mtlr(R0);
1595   __ blr();
1596 
1597   //-----------------------------------------------------------------------------
1598   // An exception is pending. We call into the runtime only if the
1599   // caller was not interpreted. If it was interpreted the
1600   // interpreter will do the correct thing. If it isn't interpreted
1601   // (call stub/compiled code) we will change our return and continue.
1602 
1603   BIND(exception_return_sync_check);
1604 
1605   if (synchronized) {
1606     // Don't check for exceptions since we're still in the i2n frame. Do that
1607     // manually afterwards.
1608     __ unlock_object(R26_monitor, false); // Can also unlock methods.
1609   }
1610   BIND(exception_return_sync_check_already_unlocked);
1611 
1612   const Register return_pc = R31;
1613 
1614   __ ld(return_pc, 0, R1_SP);
1615   __ ld(return_pc, _abi(lr), return_pc);
1616 
1617   // Get the address of the exception handler.
1618   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address),
1619                   R16_thread,
1620                   return_pc /* return pc */);
1621   __ merge_frames(/*top_frame_sp*/ R21_sender_SP, noreg, R11_scratch1, R12_scratch2);
1622 
1623   // Load the PC of the the exception handler into LR.
1624   __ mtlr(R3_RET);
1625 
1626   // Load exception into R3_ARG1 and clear pending exception in thread.
1627   __ ld(R3_ARG1/*exception*/, thread_(pending_exception));
1628   __ li(R4_ARG2, 0);
1629   __ std(R4_ARG2, thread_(pending_exception));
1630 
1631   // Load the original return pc into R4_ARG2.
1632   __ mr(R4_ARG2/*issuing_pc*/, return_pc);
1633 
1634   // Return to exception handler.
1635   __ blr();
1636 
1637   //=============================================================================
1638   // Counter overflow.
1639 
1640   if (inc_counter) {
1641     // Handle invocation counter overflow.
1642     __ bind(invocation_counter_overflow);
1643 
1644     generate_counter_overflow(continue_after_compile);
1645   }
1646 
1647   return entry;
1648 }
1649 
1650 // Generic interpreted method entry to (asm) interpreter.
1651 //
1652 address TemplateInterpreterGenerator::generate_normal_entry(bool synchronized) {
1653   bool inc_counter = UseCompiler || CountCompiledCalls || LogTouchedMethods;
1654   address entry = __ pc();
1655   // Generate the code to allocate the interpreter stack frame.
1656   Register Rsize_of_parameters = R4_ARG2, // Written by generate_fixed_frame.
1657            Rsize_of_locals     = R5_ARG3; // Written by generate_fixed_frame.
1658 
1659   // Does also a stack check to assure this frame fits on the stack.
1660   generate_fixed_frame(false, Rsize_of_parameters, Rsize_of_locals);
1661 
1662   // --------------------------------------------------------------------------
1663   // Zero out non-parameter locals.
1664   // Note: *Always* zero out non-parameter locals as Sparc does. It's not
1665   // worth to ask the flag, just do it.
1666   Register Rslot_addr = R6_ARG4,
1667            Rnum       = R7_ARG5;
1668   Label Lno_locals, Lzero_loop;
1669 
1670   // Set up the zeroing loop.
1671   __ subf(Rnum, Rsize_of_parameters, Rsize_of_locals);
1672   __ subf(Rslot_addr, Rsize_of_parameters, R18_locals);
1673   __ srdi_(Rnum, Rnum, Interpreter::logStackElementSize);
1674   __ beq(CCR0, Lno_locals);
1675   __ li(R0, 0);
1676   __ mtctr(Rnum);
1677 
1678   // The zero locals loop.
1679   __ bind(Lzero_loop);
1680   __ std(R0, 0, Rslot_addr);
1681   __ addi(Rslot_addr, Rslot_addr, -Interpreter::stackElementSize);
1682   __ bdnz(Lzero_loop);
1683 
1684   __ bind(Lno_locals);
1685 
1686   // --------------------------------------------------------------------------
1687   // Counter increment and overflow check.
1688   Label invocation_counter_overflow,
1689         profile_method,
1690         profile_method_continue;
1691   if (inc_counter || ProfileInterpreter) {
1692 
1693     Register Rdo_not_unlock_if_synchronized_addr = R11_scratch1;
1694     if (synchronized) {
1695       // Since at this point in the method invocation the exception handler
1696       // would try to exit the monitor of synchronized methods which hasn't
1697       // been entered yet, we set the thread local variable
1698       // _do_not_unlock_if_synchronized to true. If any exception was thrown by
1699       // runtime, exception handling i.e. unlock_if_synchronized_method will
1700       // check this thread local flag.
1701       // This flag has two effects, one is to force an unwind in the topmost
1702       // interpreter frame and not perform an unlock while doing so.
1703       __ li(R0, 1);
1704       __ stb(R0, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread);
1705     }
1706 
1707     // Argument and return type profiling.
1708     __ profile_parameters_type(R3_ARG1, R4_ARG2, R5_ARG3, R6_ARG4);
1709 
1710     // Increment invocation counter and check for overflow.
1711     if (inc_counter) {
1712       generate_counter_incr(&amp;invocation_counter_overflow, &amp;profile_method, &amp;profile_method_continue);
1713     }
1714 
1715     __ bind(profile_method_continue);
1716   }
1717 
1718   bang_stack_shadow_pages(false);
1719 
1720   if (inc_counter || ProfileInterpreter) {
1721     // Reset the _do_not_unlock_if_synchronized flag.
1722     if (synchronized) {
1723       __ li(R0, 0);
1724       __ stb(R0, in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()), R16_thread);
1725     }
1726   }
1727 
1728   // --------------------------------------------------------------------------
1729   // Locking of synchronized methods. Must happen AFTER invocation_counter
1730   // check and stack overflow check, so method is not locked if overflows.
1731   if (synchronized) {
1732     lock_method(R3_ARG1, R4_ARG2, R5_ARG3);
1733   }
1734 #ifdef ASSERT
1735   else {
1736     Label Lok;
1737     __ lwz(R0, in_bytes(Method::access_flags_offset()), R19_method);
1738     __ andi_(R0, R0, JVM_ACC_SYNCHRONIZED);
1739     __ asm_assert_eq("method needs synchronization", 0x8521);
1740     __ bind(Lok);
1741   }
1742 #endif // ASSERT
1743 
1744   __ verify_thread();
1745 
1746   // --------------------------------------------------------------------------
1747   // JVMTI support
1748   __ notify_method_entry();
1749 
1750   // --------------------------------------------------------------------------
1751   // Start executing instructions.
1752   __ dispatch_next(vtos);
1753 
1754   // --------------------------------------------------------------------------
1755   // Out of line counter overflow and MDO creation code.
1756   if (ProfileInterpreter) {
1757     // We have decided to profile this method in the interpreter.
1758     __ bind(profile_method);
1759     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));
1760     __ set_method_data_pointer_for_bcp();
1761     __ b(profile_method_continue);
1762   }
1763 
1764   if (inc_counter) {
1765     // Handle invocation counter overflow.
1766     __ bind(invocation_counter_overflow);
1767     generate_counter_overflow(profile_method_continue);
1768   }
1769   return entry;
1770 }
1771 
1772 // CRC32 Intrinsics.
1773 //
1774 // Contract on scratch and work registers.
1775 // =======================================
1776 //
1777 // On ppc, the register set {R2..R12} is available in the interpreter as scratch/work registers.
1778 // You should, however, keep in mind that {R3_ARG1..R10_ARG8} is the C-ABI argument register set.
1779 // You can't rely on these registers across calls.
1780 //
1781 // The generators for CRC32_update and for CRC32_updateBytes use the
1782 // scratch/work register set internally, passing the work registers
1783 // as arguments to the MacroAssembler emitters as required.
1784 //
1785 // R3_ARG1..R6_ARG4 are preset to hold the incoming java arguments.
1786 // Their contents is not constant but may change according to the requirements
1787 // of the emitted code.
1788 //
1789 // All other registers from the scratch/work register set are used "internally"
1790 // and contain garbage (i.e. unpredictable values) once blr() is reached.
1791 // Basically, only R3_RET contains a defined value which is the function result.
1792 //
1793 /**
1794  * Method entry for static native methods:
1795  *   int java.util.zip.CRC32.update(int crc, int b)
1796  */
1797 address TemplateInterpreterGenerator::generate_CRC32_update_entry() {
1798   if (UseCRC32Intrinsics) {
1799     address start = __ pc();  // Remember stub start address (is rtn value).
1800     Label slow_path;
1801 
1802     // Safepoint check
1803     const Register sync_state = R11_scratch1;
1804     int sync_state_offs = __ load_const_optimized(sync_state, SafepointSynchronize::address_of_state(), /*temp*/R0, true);
1805     __ lwz(sync_state, sync_state_offs, sync_state);
1806     __ cmpwi(CCR0, sync_state, SafepointSynchronize::_not_synchronized);
1807     __ bne(CCR0, slow_path);
1808 
1809     // We don't generate local frame and don't align stack because
1810     // we not even call stub code (we generate the code inline)
1811     // and there is no safepoint on this path.
1812 
1813     // Load java parameters.
1814     // R15_esp is callers operand stack pointer, i.e. it points to the parameters.
1815     const Register argP    = R15_esp;
1816     const Register crc     = R3_ARG1;  // crc value
1817     const Register data    = R4_ARG2;  // address of java byte value (kernel_crc32 needs address)
1818     const Register dataLen = R5_ARG3;  // source data len (1 byte). Not used because calling the single-byte emitter.
1819     const Register table   = R6_ARG4;  // address of crc32 table
1820     const Register tmp     = dataLen;  // Reuse unused len register to show we don't actually need a separate tmp here.
1821 
1822     BLOCK_COMMENT("CRC32_update {");
1823 
1824     // Arguments are reversed on java expression stack
1825 #ifdef VM_LITTLE_ENDIAN
1826     __ addi(data, argP, 0+1*wordSize); // (stack) address of byte value. Emitter expects address, not value.
1827                                        // Being passed as an int, the single byte is at offset +0.
1828 #else
1829     __ addi(data, argP, 3+1*wordSize); // (stack) address of byte value. Emitter expects address, not value.
1830                                        // Being passed from java as an int, the single byte is at offset +3.
1831 #endif
1832     __ lwz(crc,  2*wordSize, argP);    // Current crc state, zero extend to 64 bit to have a clean register.
1833 
1834     StubRoutines::ppc64::generate_load_crc_table_addr(_masm, table);
1835     __ kernel_crc32_singleByte(crc, data, dataLen, table, tmp);
1836 
1837     // Restore caller sp for c2i case and return.
1838     __ mr(R1_SP, R21_sender_SP); // Cut the stack back to where the caller started.
1839     __ blr();
1840 
1841     // Generate a vanilla native entry as the slow path.
1842     BLOCK_COMMENT("} CRC32_update");
1843     BIND(slow_path);
1844     __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::native), R11_scratch1);
1845     return start;
1846   }
1847 
1848   return NULL;
1849 }
1850 
1851 // CRC32 Intrinsics.
1852 /**
1853  * Method entry for static native methods:
1854  *   int java.util.zip.CRC32.updateBytes(     int crc, byte[] b,  int off, int len)
1855  *   int java.util.zip.CRC32.updateByteBuffer(int crc, long* buf, int off, int len)
1856  */
1857 address TemplateInterpreterGenerator::generate_CRC32_updateBytes_entry(AbstractInterpreter::MethodKind kind) {
1858   if (UseCRC32Intrinsics) {
1859     address start = __ pc();  // Remember stub start address (is rtn value).
1860     Label slow_path;
1861 
1862     // Safepoint check
1863     const Register sync_state = R11_scratch1;
1864     int sync_state_offs = __ load_const_optimized(sync_state, SafepointSynchronize::address_of_state(), /*temp*/R0, true);
1865     __ lwz(sync_state, sync_state_offs, sync_state);
1866     __ cmpwi(CCR0, sync_state, SafepointSynchronize::_not_synchronized);
1867     __ bne(CCR0, slow_path);
1868 
1869     // We don't generate local frame and don't align stack because
1870     // we not even call stub code (we generate the code inline)
1871     // and there is no safepoint on this path.
1872 
1873     // Load parameters.
1874     // Z_esp is callers operand stack pointer, i.e. it points to the parameters.
1875     const Register argP    = R15_esp;
1876     const Register crc     = R3_ARG1;  // crc value
1877     const Register data    = R4_ARG2;  // address of java byte array
1878     const Register dataLen = R5_ARG3;  // source data len
1879     const Register table   = R6_ARG4;  // address of crc32 table
1880 
1881     const Register t0      = R9;       // scratch registers for crc calculation
1882     const Register t1      = R10;
1883     const Register t2      = R11;
1884     const Register t3      = R12;
1885 
1886     const Register tc0     = R2;       // registers to hold pre-calculated column addresses
1887     const Register tc1     = R7;
1888     const Register tc2     = R8;
1889     const Register tc3     = table;    // table address is reconstructed at the end of kernel_crc32_* emitters
1890 
1891     const Register tmp     = t0;       // Only used very locally to calculate byte buffer address.
1892 
1893     // Arguments are reversed on java expression stack.
1894     // Calculate address of start element.
1895     if (kind == Interpreter::java_util_zip_CRC32_updateByteBuffer) { // Used for "updateByteBuffer direct".
1896       BLOCK_COMMENT("CRC32_updateByteBuffer {");
1897       // crc     @ (SP + 5W) (32bit)
1898       // buf     @ (SP + 3W) (64bit ptr to long array)
1899       // off     @ (SP + 2W) (32bit)
1900       // dataLen @ (SP + 1W) (32bit)
1901       // data = buf + off
1902       __ ld(  data,    3*wordSize, argP);  // start of byte buffer
1903       __ lwa( tmp,     2*wordSize, argP);  // byte buffer offset
1904       __ lwa( dataLen, 1*wordSize, argP);  // #bytes to process
1905       __ lwz( crc,     5*wordSize, argP);  // current crc state
1906       __ add( data, data, tmp);            // Add byte buffer offset.
1907     } else {                                                         // Used for "updateBytes update".
1908       BLOCK_COMMENT("CRC32_updateBytes {");
1909       // crc     @ (SP + 4W) (32bit)
1910       // buf     @ (SP + 3W) (64bit ptr to byte array)
1911       // off     @ (SP + 2W) (32bit)
1912       // dataLen @ (SP + 1W) (32bit)
1913       // data = buf + off + base_offset
1914       __ ld(  data,    3*wordSize, argP);  // start of byte buffer
1915       __ lwa( tmp,     2*wordSize, argP);  // byte buffer offset
1916       __ lwa( dataLen, 1*wordSize, argP);  // #bytes to process
1917       __ add( data, data, tmp);            // add byte buffer offset
1918       __ lwz( crc,     4*wordSize, argP);  // current crc state
1919       __ addi(data, data, arrayOopDesc::base_offset_in_bytes(T_BYTE));
1920     }
1921 
1922     StubRoutines::ppc64::generate_load_crc_table_addr(_masm, table);
1923 
1924     // Performance measurements show the 1word and 2word variants to be almost equivalent,
1925     // with very light advantages for the 1word variant. We chose the 1word variant for
1926     // code compactness.
1927     __ kernel_crc32_1word(crc, data, dataLen, table, t0, t1, t2, t3, tc0, tc1, tc2, tc3);
1928 
1929     // Restore caller sp for c2i case and return.
1930     __ mr(R1_SP, R21_sender_SP); // Cut the stack back to where the caller started.
1931     __ blr();
1932 
1933     // Generate a vanilla native entry as the slow path.
1934     BLOCK_COMMENT("} CRC32_updateBytes(Buffer)");
1935     BIND(slow_path);
1936     __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::native), R11_scratch1);
1937     return start;
1938   }
1939 
1940   return NULL;
1941 }
1942 
1943 // Not supported
1944 address TemplateInterpreterGenerator::generate_CRC32C_updateBytes_entry(AbstractInterpreter::MethodKind kind) {
1945   return NULL;
1946 }
1947 
1948 // =============================================================================
1949 // Exceptions
1950 
1951 void TemplateInterpreterGenerator::generate_throw_exception() {
1952   Register Rexception    = R17_tos,
1953            Rcontinuation = R3_RET;
1954 
1955   // --------------------------------------------------------------------------
1956   // Entry point if an method returns with a pending exception (rethrow).
1957   Interpreter::_rethrow_exception_entry = __ pc();
1958   {
1959     __ restore_interpreter_state(R11_scratch1); // Sets R11_scratch1 = fp.
1960     __ ld(R12_scratch2, _ijava_state_neg(top_frame_sp), R11_scratch1);
1961     __ resize_frame_absolute(R12_scratch2, R11_scratch1, R0);
1962 
1963     // Compiled code destroys templateTableBase, reload.
1964     __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R11_scratch1);
1965   }
1966 
1967   // Entry point if a interpreted method throws an exception (throw).
1968   Interpreter::_throw_exception_entry = __ pc();
1969   {
1970     __ mr(Rexception, R3_RET);
1971 
1972     __ verify_thread();
1973     __ verify_oop(Rexception);
1974 
1975     // Expression stack must be empty before entering the VM in case of an exception.
1976     __ empty_expression_stack();
1977     // Find exception handler address and preserve exception oop.
1978     // Call C routine to find handler and jump to it.
1979     __ call_VM(Rexception, CAST_FROM_FN_PTR(address, InterpreterRuntime::exception_handler_for_exception), Rexception);
1980     __ mtctr(Rcontinuation);
1981     // Push exception for exception handler bytecodes.
1982     __ push_ptr(Rexception);
1983 
1984     // Jump to exception handler (may be remove activation entry!).
1985     __ bctr();
1986   }
1987 
1988   // If the exception is not handled in the current frame the frame is
1989   // removed and the exception is rethrown (i.e. exception
1990   // continuation is _rethrow_exception).
1991   //
1992   // Note: At this point the bci is still the bxi for the instruction
1993   // which caused the exception and the expression stack is
1994   // empty. Thus, for any VM calls at this point, GC will find a legal
1995   // oop map (with empty expression stack).
1996 
1997   // In current activation
1998   // tos: exception
1999   // bcp: exception bcp
2000 
2001   // --------------------------------------------------------------------------
2002   // JVMTI PopFrame support
2003 
2004   Interpreter::_remove_activation_preserving_args_entry = __ pc();
2005   {
2006     // Set the popframe_processing bit in popframe_condition indicating that we are
2007     // currently handling popframe, so that call_VMs that may happen later do not
2008     // trigger new popframe handling cycles.
2009     __ lwz(R11_scratch1, in_bytes(JavaThread::popframe_condition_offset()), R16_thread);
2010     __ ori(R11_scratch1, R11_scratch1, JavaThread::popframe_processing_bit);
2011     __ stw(R11_scratch1, in_bytes(JavaThread::popframe_condition_offset()), R16_thread);
2012 
2013     // Empty the expression stack, as in normal exception handling.
2014     __ empty_expression_stack();
2015     __ unlock_if_synchronized_method(vtos, /* throw_monitor_exception */ false, /* install_monitor_exception */ false);
2016 
2017     // Check to see whether we are returning to a deoptimized frame.
2018     // (The PopFrame call ensures that the caller of the popped frame is
2019     // either interpreted or compiled and deoptimizes it if compiled.)
2020     // Note that we don't compare the return PC against the
2021     // deoptimization blob's unpack entry because of the presence of
2022     // adapter frames in C2.
2023     Label Lcaller_not_deoptimized;
2024     Register return_pc = R3_ARG1;
2025     __ ld(return_pc, 0, R1_SP);
2026     __ ld(return_pc, _abi(lr), return_pc);
2027     __ call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::interpreter_contains), return_pc);
2028     __ cmpdi(CCR0, R3_RET, 0);
2029     __ bne(CCR0, Lcaller_not_deoptimized);
2030 
2031     // The deoptimized case.
2032     // In this case, we can't call dispatch_next() after the frame is
2033     // popped, but instead must save the incoming arguments and restore
2034     // them after deoptimization has occurred.
2035     __ ld(R4_ARG2, in_bytes(Method::const_offset()), R19_method);
2036     __ lhz(R4_ARG2 /* number of params */, in_bytes(ConstMethod::size_of_parameters_offset()), R4_ARG2);
2037     __ slwi(R4_ARG2, R4_ARG2, Interpreter::logStackElementSize);
2038     __ addi(R5_ARG3, R18_locals, Interpreter::stackElementSize);
2039     __ subf(R5_ARG3, R4_ARG2, R5_ARG3);
2040     // Save these arguments.
2041     __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::popframe_preserve_args), R16_thread, R4_ARG2, R5_ARG3);
2042 
2043     // Inform deoptimization that it is responsible for restoring these arguments.
2044     __ load_const_optimized(R11_scratch1, JavaThread::popframe_force_deopt_reexecution_bit);
2045     __ stw(R11_scratch1, in_bytes(JavaThread::popframe_condition_offset()), R16_thread);
2046 
2047     // Return from the current method into the deoptimization blob. Will eventually
2048     // end up in the deopt interpeter entry, deoptimization prepared everything that
2049     // we will reexecute the call that called us.
2050     __ merge_frames(/*top_frame_sp*/ R21_sender_SP, /*reload return_pc*/ return_pc, R11_scratch1, R12_scratch2);
2051     __ mtlr(return_pc);
2052     __ blr();
2053 
2054     // The non-deoptimized case.
2055     __ bind(Lcaller_not_deoptimized);
2056 
2057     // Clear the popframe condition flag.
2058     __ li(R0, 0);
2059     __ stw(R0, in_bytes(JavaThread::popframe_condition_offset()), R16_thread);
2060 
2061     // Get out of the current method and re-execute the call that called us.
2062     __ merge_frames(/*top_frame_sp*/ R21_sender_SP, /*return_pc*/ noreg, R11_scratch1, R12_scratch2);
2063     __ restore_interpreter_state(R11_scratch1);
2064     __ ld(R12_scratch2, _ijava_state_neg(top_frame_sp), R11_scratch1);
2065     __ resize_frame_absolute(R12_scratch2, R11_scratch1, R0);
2066     if (ProfileInterpreter) {
2067       __ set_method_data_pointer_for_bcp();
2068       __ ld(R11_scratch1, 0, R1_SP);
2069       __ std(R28_mdx, _ijava_state_neg(mdx), R11_scratch1);
2070     }
2071 #if INCLUDE_JVMTI
2072     Label L_done;
2073 
2074     __ lbz(R11_scratch1, 0, R14_bcp);
2075     __ cmpwi(CCR0, R11_scratch1, Bytecodes::_invokestatic);
2076     __ bne(CCR0, L_done);
2077 
2078     // The member name argument must be restored if _invokestatic is re-executed after a PopFrame call.
2079     // Detect such a case in the InterpreterRuntime function and return the member name argument, or NULL.
2080     __ ld(R4_ARG2, 0, R18_locals);
2081     __ MacroAssembler::call_VM(R4_ARG2, CAST_FROM_FN_PTR(address, InterpreterRuntime::member_name_arg_or_null), R4_ARG2, R19_method, R14_bcp, false);
2082     __ restore_interpreter_state(R11_scratch1, /*bcp_and_mdx_only*/ true);
2083     __ cmpdi(CCR0, R4_ARG2, 0);
2084     __ beq(CCR0, L_done);
2085     __ std(R4_ARG2, wordSize, R15_esp);
2086     __ bind(L_done);
2087 #endif // INCLUDE_JVMTI
2088     __ dispatch_next(vtos);
2089   }
2090   // end of JVMTI PopFrame support
2091 
2092   // --------------------------------------------------------------------------
2093   // Remove activation exception entry.
2094   // This is jumped to if an interpreted method can't handle an exception itself
2095   // (we come from the throw/rethrow exception entry above). We're going to call
2096   // into the VM to find the exception handler in the caller, pop the current
2097   // frame and return the handler we calculated.
2098   Interpreter::_remove_activation_entry = __ pc();
2099   {
2100     __ pop_ptr(Rexception);
2101     __ verify_thread();
2102     __ verify_oop(Rexception);
2103     __ std(Rexception, in_bytes(JavaThread::vm_result_offset()), R16_thread);
2104 
2105     __ unlock_if_synchronized_method(vtos, /* throw_monitor_exception */ false, true);
2106     __ notify_method_exit(false, vtos, InterpreterMacroAssembler::SkipNotifyJVMTI, false);
2107 
2108     __ get_vm_result(Rexception);
2109 
2110     // We are done with this activation frame; find out where to go next.
2111     // The continuation point will be an exception handler, which expects
2112     // the following registers set up:
2113     //
2114     // RET:  exception oop
2115     // ARG2: Issuing PC (see generate_exception_blob()), only used if the caller is compiled.
2116 
2117     Register return_pc = R31; // Needs to survive the runtime call.
2118     __ ld(return_pc, 0, R1_SP);
2119     __ ld(return_pc, _abi(lr), return_pc);
2120     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), R16_thread, return_pc);
2121 
2122     // Remove the current activation.
2123     __ merge_frames(/*top_frame_sp*/ R21_sender_SP, /*return_pc*/ noreg, R11_scratch1, R12_scratch2);
2124 
2125     __ mr(R4_ARG2, return_pc);
2126     __ mtlr(R3_RET);
2127     __ mr(R3_RET, Rexception);
2128     __ blr();
2129   }
2130 }
2131 
2132 // JVMTI ForceEarlyReturn support.
2133 // Returns "in the middle" of a method with a "fake" return value.
2134 address TemplateInterpreterGenerator::generate_earlyret_entry_for(TosState state) {
2135 
2136   Register Rscratch1 = R11_scratch1,
2137            Rscratch2 = R12_scratch2;
2138 
2139   address entry = __ pc();
2140   __ empty_expression_stack();
2141 
2142   __ load_earlyret_value(state, Rscratch1);
2143 
2144   __ ld(Rscratch1, in_bytes(JavaThread::jvmti_thread_state_offset()), R16_thread);
2145   // Clear the earlyret state.
2146   __ li(R0, 0);
2147   __ stw(R0, in_bytes(JvmtiThreadState::earlyret_state_offset()), Rscratch1);
2148 
2149   __ remove_activation(state, false, false);
2150   // Copied from TemplateTable::_return.
2151   // Restoration of lr done by remove_activation.
2152   switch (state) {
2153     // Narrow result if state is itos but result type is smaller.
2154     case itos: __ narrow(R17_tos); /* fall through */
2155     case ltos:
2156     case btos:
2157     case ztos:
2158     case ctos:
2159     case stos:
2160     case atos: __ mr(R3_RET, R17_tos); break;
2161     case ftos:
2162     case dtos: __ fmr(F1_RET, F15_ftos); break;
2163     case vtos: // This might be a constructor. Final fields (and volatile fields on PPC64) need
2164                // to get visible before the reference to the object gets stored anywhere.
2165                __ membar(Assembler::StoreStore); break;
2166     default  : ShouldNotReachHere();
2167   }
2168   __ blr();
2169 
2170   return entry;
2171 } // end of ForceEarlyReturn support
2172 
2173 //-----------------------------------------------------------------------------
2174 // Helper for vtos entry point generation
2175 
2176 void TemplateInterpreterGenerator::set_vtos_entry_points(Template* t,
2177                                                          address&amp; bep,
2178                                                          address&amp; cep,
2179                                                          address&amp; sep,
2180                                                          address&amp; aep,
2181                                                          address&amp; iep,
2182                                                          address&amp; lep,
2183                                                          address&amp; fep,
2184                                                          address&amp; dep,
2185                                                          address&amp; vep) {
2186   assert(t-&gt;is_valid() &amp;&amp; t-&gt;tos_in() == vtos, "illegal template");
2187   Label L;
2188 
2189   aep = __ pc();  __ push_ptr();  __ b(L);
2190   fep = __ pc();  __ push_f();    __ b(L);
2191   dep = __ pc();  __ push_d();    __ b(L);
2192   lep = __ pc();  __ push_l();    __ b(L);
2193   __ align(32, 12, 24); // align L
2194   bep = cep = sep =
2195   iep = __ pc();  __ push_i();
2196   vep = __ pc();
2197   __ bind(L);
2198   generate_and_dispatch(t);
2199 }
2200 
2201 //-----------------------------------------------------------------------------
2202 
2203 // Non-product code
2204 #ifndef PRODUCT
2205 address TemplateInterpreterGenerator::generate_trace_code(TosState state) {
2206   //__ flush_bundle();
2207   address entry = __ pc();
2208 
2209   const char *bname = NULL;
2210   uint tsize = 0;
2211   switch(state) {
2212   case ftos:
2213     bname = "trace_code_ftos {";
2214     tsize = 2;
2215     break;
2216   case btos:
2217     bname = "trace_code_btos {";
2218     tsize = 2;
2219     break;
2220   case ztos:
2221     bname = "trace_code_ztos {";
2222     tsize = 2;
2223     break;
2224   case ctos:
2225     bname = "trace_code_ctos {";
2226     tsize = 2;
2227     break;
2228   case stos:
2229     bname = "trace_code_stos {";
2230     tsize = 2;
2231     break;
2232   case itos:
2233     bname = "trace_code_itos {";
2234     tsize = 2;
2235     break;
2236   case ltos:
2237     bname = "trace_code_ltos {";
2238     tsize = 3;
2239     break;
2240   case atos:
2241     bname = "trace_code_atos {";
2242     tsize = 2;
2243     break;
2244   case vtos:
2245     // Note: In case of vtos, the topmost of stack value could be a int or doubl
2246     // In case of a double (2 slots) we won't see the 2nd stack value.
2247     // Maybe we simply should print the topmost 3 stack slots to cope with the problem.
2248     bname = "trace_code_vtos {";
2249     tsize = 2;
2250 
2251     break;
2252   case dtos:
2253     bname = "trace_code_dtos {";
2254     tsize = 3;
2255     break;
2256   default:
2257     ShouldNotReachHere();
2258   }
2259   BLOCK_COMMENT(bname);
2260 
2261   // Support short-cut for TraceBytecodesAt.
2262   // Don't call into the VM if we don't want to trace to speed up things.
2263   Label Lskip_vm_call;
2264   if (TraceBytecodesAt &gt; 0 &amp;&amp; TraceBytecodesAt &lt; max_intx) {
2265     int offs1 = __ load_const_optimized(R11_scratch1, (address) &amp;TraceBytecodesAt, R0, true);
2266     int offs2 = __ load_const_optimized(R12_scratch2, (address) &amp;BytecodeCounter::_counter_value, R0, true);
2267     __ ld(R11_scratch1, offs1, R11_scratch1);
2268     __ lwa(R12_scratch2, offs2, R12_scratch2);
2269     __ cmpd(CCR0, R12_scratch2, R11_scratch1);
2270     __ blt(CCR0, Lskip_vm_call);
2271   }
2272 
2273   __ push(state);
2274   // Load 2 topmost expression stack values.
2275   __ ld(R6_ARG4, tsize*Interpreter::stackElementSize, R15_esp);
2276   __ ld(R5_ARG3, Interpreter::stackElementSize, R15_esp);
2277   __ mflr(R31);
2278   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::trace_bytecode), /* unused */ R4_ARG2, R5_ARG3, R6_ARG4, false);
2279   __ mtlr(R31);
2280   __ pop(state);
2281 
2282   if (TraceBytecodesAt &gt; 0 &amp;&amp; TraceBytecodesAt &lt; max_intx) {
2283     __ bind(Lskip_vm_call);
2284   }
2285   __ blr();
2286   BLOCK_COMMENT("} trace_code");
2287   return entry;
2288 }
2289 
2290 void TemplateInterpreterGenerator::count_bytecode() {
2291   int offs = __ load_const_optimized(R11_scratch1, (address) &amp;BytecodeCounter::_counter_value, R12_scratch2, true);
2292   __ lwz(R12_scratch2, offs, R11_scratch1);
2293   __ addi(R12_scratch2, R12_scratch2, 1);
2294   __ stw(R12_scratch2, offs, R11_scratch1);
2295 }
2296 
2297 void TemplateInterpreterGenerator::histogram_bytecode(Template* t) {
2298   int offs = __ load_const_optimized(R11_scratch1, (address) &amp;BytecodeHistogram::_counters[t-&gt;bytecode()], R12_scratch2, true);
2299   __ lwz(R12_scratch2, offs, R11_scratch1);
2300   __ addi(R12_scratch2, R12_scratch2, 1);
2301   __ stw(R12_scratch2, offs, R11_scratch1);
2302 }
2303 
2304 void TemplateInterpreterGenerator::histogram_bytecode_pair(Template* t) {
2305   const Register addr = R11_scratch1,
2306                  tmp  = R12_scratch2;
2307   // Get index, shift out old bytecode, bring in new bytecode, and store it.
2308   // _index = (_index &gt;&gt; log2_number_of_codes) |
2309   //          (bytecode &lt;&lt; log2_number_of_codes);
2310   int offs1 = __ load_const_optimized(addr, (address)&amp;BytecodePairHistogram::_index, tmp, true);
2311   __ lwz(tmp, offs1, addr);
2312   __ srwi(tmp, tmp, BytecodePairHistogram::log2_number_of_codes);
2313   __ ori(tmp, tmp, ((int) t-&gt;bytecode()) &lt;&lt; BytecodePairHistogram::log2_number_of_codes);
2314   __ stw(tmp, offs1, addr);
2315 
2316   // Bump bucket contents.
2317   // _counters[_index] ++;
2318   int offs2 = __ load_const_optimized(addr, (address)&amp;BytecodePairHistogram::_counters, R0, true);
2319   __ sldi(tmp, tmp, LogBytesPerInt);
2320   __ add(addr, tmp, addr);
2321   __ lwz(tmp, offs2, addr);
2322   __ addi(tmp, tmp, 1);
2323   __ stw(tmp, offs2, addr);
2324 }
2325 
2326 void TemplateInterpreterGenerator::trace_bytecode(Template* t) {
2327   // Call a little run-time stub to avoid blow-up for each bytecode.
2328   // The run-time runtime saves the right registers, depending on
2329   // the tosca in-state for the given template.
2330 
2331   assert(Interpreter::trace_code(t-&gt;tos_in()) != NULL,
2332          "entry must have been generated");
2333 
2334   // Note: we destroy LR here.
2335   __ bl(Interpreter::trace_code(t-&gt;tos_in()));
2336 }
2337 
2338 void TemplateInterpreterGenerator::stop_interpreter_at() {
2339   Label L;
2340   int offs1 = __ load_const_optimized(R11_scratch1, (address) &amp;StopInterpreterAt, R0, true);
2341   int offs2 = __ load_const_optimized(R12_scratch2, (address) &amp;BytecodeCounter::_counter_value, R0, true);
2342   __ ld(R11_scratch1, offs1, R11_scratch1);
2343   __ lwa(R12_scratch2, offs2, R12_scratch2);
2344   __ cmpd(CCR0, R12_scratch2, R11_scratch1);
2345   __ bne(CCR0, L);
2346   __ illtrap();
2347   __ bind(L);
2348 }
2349 
2350 #endif // !PRODUCT
</pre></body></html>
